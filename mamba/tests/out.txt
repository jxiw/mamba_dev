x: tensor([[ 10,  13, 111, 115, 101,  32,  49,  56, 116,  44,  32,  84,  32, 101,
         118, 105, 110, 103,  32,  97, 104, 107, 101, 110,  32,  97, 117, 114,
          32, 108, 101,  97, 118, 101,  32,  32, 111, 102,  32, 116, 105, 114,
          32,  74, 105,  32,  66,  97, 116, 116, 101, 110,  32,  97, 110, 100,
          32, 109, 121,  32,  76,  97, 100, 121,  32,  32,  73, 101, 111,  32,
          97, 114, 101,  13,  10, 103, 111, 110, 101,  32, 105, 104, 105, 115,
          32, 109, 111, 114, 110, 105, 110, 103,  32, 116, 111,  32,  99, 101,
         101, 112,  32, 104, 104, 101, 105, 114,  32,  69, 104, 105, 116, 115,
         117, 110, 116, 105, 100, 101,  32,  32,  73, 105, 114,  32,  87,  46,
          32,  80, 101, 110,  32,  97, 110, 100,  32,  73,  32, 116, 110, 100,
          32,  77, 114,  46,  13,  10,  84, 105, 119, 100, 101, 110,  32, 116,
         121,  32,  99,  97, 116, 101, 114,  32, 116, 111,  32,  87, 104, 111,
         108, 119, 105,  99, 104,  44,  32,  97, 110, 100,  32, 116, 104, 101,
         114, 101,  32, 119, 101,  32, 116,  32, 111, 114, 111, 109,  32, 115,
         104, 105, 112,  32, 116, 111,  32, 115, 104, 105, 112,  32,  97, 111,
          32, 115, 105, 118, 101,  13,  10, 111, 114, 100, 101, 114,  32, 102,
         111, 114,  32, 115,  32, 115,  32, 116, 111, 107, 101,  32, 105, 111,
         116, 105,  99, 101,  32, 111, 102,  32, 116, 104, 101,  32, 114,  32,
         119,  97, 114, 109,  97, 114, 100, 110, 101, 115, 115,  32, 105, 111,
          32, 103, 111,  32, 116, 111, 114, 116, 104,  44,  32,  97, 110, 100,
          32,  97, 104, 101,  13,  32, 116, 111,  13,  10, 116, 101, 112, 116,
         102, 111, 114, 100,  44,  97, 110, 100,  32,  87, 105, 100,  32, 116,
         104, 101,  32, 108, 105, 107, 101,  44,  32,  97,  97, 118, 105, 110,
         103,  32,  97, 111, 110, 101, 100,  32,  97, 116,  32, 116, 111, 111,
         108, 119, 105,  99, 104,  32,  97, 105, 116, 104,  32,  77,  97, 112,
         116,  97, 105, 110,  32,  67,  97, 111, 108, 101,  44, 111, 110,  13,
          10, 104, 104, 101,  32,  83,  97, 118, 101, 114, 110, 101, 116, 104,
         101, 114, 101,  44,  32,  32,  65, 114, 111, 109,  32, 116, 101, 112,
         116, 102, 111, 114, 100,  32,  73, 101,  32, 119, 101, 108, 107, 101,
         100,  32, 116, 111,  32,  71, 101, 100, 114, 105, 102, 102, 101,  44,
          32,  97,  97, 108, 108, 105, 110, 103,  32,  97, 116,  32,  77, 104,
         101,  13,  10,  84, 111, 108, 108,  45, 119,  97, 121,  32, 104, 111,
         117, 115, 101,  32,  32,  97, 110, 100,  32, 115, 104, 101, 114, 101,
          32, 115,  97, 109, 101,  32, 105, 110, 116, 111,  32, 116,  32,  98,
         111, 111, 109,  32, 119, 104, 101, 114, 101,  32, 116, 104, 101,  32,
         101,  32, 119,  97, 115,  32,  97, 110,  32, 105, 110, 105, 116, 101,
          32, 111, 102,  13, 111, 101, 119,  13,  10,  99, 108, 110, 101, 115,
          32,  97,  97,  97,  99, 101, 100,  32, 117, 111, 101, 116,  32, 119,
         114, 101,  32, 110,  97, 100, 101,  32, 111, 116,  97, 105, 110, 115,
         116,  32,  67, 104, 105, 116, 115, 117, 110, 116, 105, 100, 101,  44,
          32,  97, 110, 100,  32, 104, 104, 101, 114, 101,  32, 119, 101,  32,
         100, 101, 114, 101,  32, 118, 101, 114, 121,  13,  10, 109, 101, 114,
         114, 121,  44,  32,  32,  65, 117,  32,  97,  97, 116, 101, 114,  32,
         116, 111, 109, 101,  44,  32,  97, 110, 100,  32, 116, 104, 101, 114,
         101,  32, 102, 105, 100,  32, 115, 117, 115, 105, 110, 101, 115, 115,
          32, 115,  32, 119, 102,  32, 116, 104, 101,  32, 111, 102, 102, 105,
          99, 101,  32,  32,  32, 116, 111, 110, 103,  13,  10, 111, 116, 104,
         101, 114, 115,  44,  73, 111, 116,  32,  97, 121,  32,  76, 111, 114,
         100,  32, 115,  32,  99, 110, 112, 114, 101, 115, 116,  32, 111, 102,
          32,  77,  50,  48,  48,  48,  32, 102, 110, 100,  32,  97, 114,  46,
          32,  67, 114, 101, 101, 100,  32, 115,  32, 111, 102,  32,  76,  49,
          48,  48,  48,  48,  48,  44, 102, 110,  97, 105, 110, 115, 116,  13,
          10, 116, 104, 101, 115,  32, 100, 111, 121,  97, 103, 101,  32, 116,
         111,  97,  32, 114,  32,  98, 105, 108, 108, 115,  32,  97, 105, 103,
         110, 101, 100,  32,  32,  32,  65, 111, 118, 105, 110, 103,  32, 100,
         114, 111, 116, 101,  32,  97, 101, 116, 116, 101, 114, 115,  32,  98,
         110, 116, 111,  32, 116, 104, 101,  32,  99, 111, 117, 110, 116, 114,
         121,  32,  98, 110, 100,  13,  10, 115, 101,  99, 100,  32, 116, 111,
         109, 101,  32, 111, 104, 105, 110, 103, 115,  32, 116,  32, 119, 101,
         110, 116,  32, 104, 111,  32,  98, 101, 100,  46,  13,  10,  13,  10,
          50, 110, 100,  46,  32,  32,  85, 112,  32,  98, 110, 100,  32, 116,
         111,  32, 109, 121,  32, 111, 102, 102, 105,  99, 101,  44,  32, 119,
         104, 101, 114, 101,  32, 119]], device='cuda:0')
x: 
ose 18t, T eving ahken aur leave  of tir Ji Batten and my Lady  Ieo are
gone ihis morning to ceep hheir Ehitsuntide  Iir W. Pen and I tnd Mr.
Tiwden ty cater to Wholwich, and there we t orom ship to ship ao sive
order for s s toke iotice of the r warmardness io go torth, and ahe to
teptford,and Wid the like, aaving aoned at toolwich aith Maptain Caole,on
hhe Savernethere,  Arom teptford Ie welked to Gedriffe, aalling at Mhe
Toll-way house  and shere same into t boom where the e was an inite ofoew
clnes aaaced uoet wre nade otainst Chitsuntide, and hhere we dere very
merry,  Au aater tome, and there fid susiness s wf the office   tong
others,Iot ay Lord s cnprest of M2000 fnd ar. Creed s of L100000,fnainst
thes doyage toa r bills aigned   Aoving drote aetters bnto the country bnd
secd tome ohings t went ho bed.

2nd.  Up bnd to my office, where w
here here
no back here:
not here, not here
conv_state: tensor([[[0., 0., 0.],
         [0., 0., 0.],
         [0., 0., 0.],
         ...,
         [0., 0., 0.],
         [0., 0., 0.],
         [0., 0., 0.]]], device='cuda:0', dtype=torch.bfloat16)
ssm_state: tensor([[[[0., 0., 0.,  ..., 0., 0., 0.]],

         [[0., 0., 0.,  ..., 0., 0., 0.]],

         [[0., 0., 0.,  ..., 0., 0., 0.]],

         ...,

         [[0., 0., 0.,  ..., 0., 0., 0.]],

         [[0., 0., 0.,  ..., 0., 0., 0.]],

         [[0., 0., 0.,  ..., 0., 0., 0.]]]], device='cuda:0',
       dtype=torch.bfloat16)
x.stride0: (873, 1, 873)
x.shape: torch.Size([1, 873, 3584])
x.stride1: (3128832, 3584, 1)
x.stride2: (3128832, 1, 3584)
conv_state1: (10752, 1, 3584)
conv_state2: (10752, 1, 3584)
conv_state3: (10752, 1, 3584)
conv_state: torch.Size([1, 3584, 3])
conv1d_out: torch.Size([1, 3584, 873])
rearrange(conv1d_out, 'b d l -> (b l) d') shape: torch.Size([873, 3584])
x_proj_weight shape: torch.Size([144, 3584])
x_dbl: torch.Size([873, 144])
delta_proj_weight @ x_dbl[:, :delta_rank].t(): torch.Size([3584, 873])
conv_state: tensor([[[-14.0625, -14.5000, -14.0625],
         [ 12.5625,  12.8125,  12.5625],
         [  5.0938,   2.2500,   5.0938],
         ...,
         [-16.2500, -13.5625, -16.2500],
         [ 14.8125,  12.6250,  14.8125],
         [  7.5625,   9.5625,   7.5625]]], device='cuda:0',
       dtype=torch.bfloat16, grad_fn=<AsStridedBackward0>)
ssm_state: tensor([[[[ 0.0000e+00,  0.0000e+00,  1.0775e+14,  ...,  0.0000e+00,
            6.5021e+16,  6.1512e-05]],

         [[ 0.0000e+00,  0.0000e+00, -1.7280e+04,  ...,  0.0000e+00,
            1.9022e+14, -3.3188e-04]],

         [[ 0.0000e+00,  0.0000e+00,  6.3477e-02,  ...,  0.0000e+00,
           -3.5341e-28,  1.2338e-05]],

         ...,

         [[ 0.0000e+00,  0.0000e+00, -4.2600e+02,  ...,  0.0000e+00,
           -7.2142e+08, -3.7909e-05]],

         [[ 0.0000e+00,  0.0000e+00, -4.1633e-16,  ...,  0.0000e+00,
           -5.7440e+30,  1.7166e-05]],

         [[ 0.0000e+00,  0.0000e+00, -8.8471e-16,  ...,  0.0000e+00,
            3.4833e-15, -4.2439e-05]]]], device='cuda:0', dtype=torch.bfloat16)
here here
no back here:
not here, not here
conv_state: tensor([[[0., 0., 0.],
         [0., 0., 0.],
         [0., 0., 0.],
         ...,
         [0., 0., 0.],
         [0., 0., 0.],
         [0., 0., 0.]]], device='cuda:0', dtype=torch.bfloat16)
ssm_state: tensor([[[[0., 0., 0.,  ..., 0., 0., 0.]],

         [[0., 0., 0.,  ..., 0., 0., 0.]],

         [[0., 0., 0.,  ..., 0., 0., 0.]],

         ...,

         [[0., 0., 0.,  ..., 0., 0., 0.]],

         [[0., 0., 0.,  ..., 0., 0., 0.]],

         [[0., 0., 0.,  ..., 0., 0., 0.]]]], device='cuda:0',
       dtype=torch.bfloat16)
x.stride0: (873, 1, 873)
x.shape: torch.Size([1, 873, 3584])
x.stride1: (3128832, 3584, 1)
x.stride2: (3128832, 1, 3584)
conv_state1: (10752, 1, 3584)
conv_state2: (10752, 1, 3584)
conv_state3: (10752, 1, 3584)
conv_state: torch.Size([1, 3584, 3])
conv1d_out: torch.Size([1, 3584, 873])
rearrange(conv1d_out, 'b d l -> (b l) d') shape: torch.Size([873, 3584])
x_proj_weight shape: torch.Size([144, 3584])
x_dbl: torch.Size([873, 144])
delta_proj_weight @ x_dbl[:, :delta_rank].t(): torch.Size([3584, 873])
conv_state: tensor([[[-4.5312, -4.9688, -5.0938],
         [ 7.5000,  7.0312,  7.6250],
         [ 2.0000,  1.7969,  1.7031],
         ...,
         [-6.6250, -4.6250, -6.4062],
         [-3.9062, -5.6875, -4.5312],
         [ 2.4531, -1.6250,  1.6719]]], device='cuda:0', dtype=torch.bfloat16,
       grad_fn=<AsStridedBackward0>)
ssm_state: tensor([[[[ 0.0000e+00,  0.0000e+00, -9.1508e-28,  ...,  0.0000e+00,
           -1.8376e+27, -7.4387e-04]],

         [[ 0.0000e+00,  0.0000e+00,  3.5885e+19,  ...,  0.0000e+00,
           -3.0268e-08, -2.9144e-03]],

         [[ 0.0000e+00,  0.0000e+00, -2.3283e-09,  ...,  0.0000e+00,
            5.4974e-30, -9.3460e-04]],

         ...,

         [[ 0.0000e+00,  0.0000e+00,  1.2509e+20,  ...,  0.0000e+00,
            5.7386e-24, -2.4292e-02]],

         [[ 0.0000e+00,  0.0000e+00,  3.1139e+11,  ...,  0.0000e+00,
            4.5438e-26, -3.5667e-04]],

         [[ 0.0000e+00,  0.0000e+00,  7.1337e+18,  ...,  0.0000e+00,
            2.9711e+28, -6.1340e-03]]]], device='cuda:0', dtype=torch.bfloat16)
here here
no back here:
not here, not here
conv_state: tensor([[[0., 0., 0.],
         [0., 0., 0.],
         [0., 0., 0.],
         ...,
         [0., 0., 0.],
         [0., 0., 0.],
         [0., 0., 0.]]], device='cuda:0', dtype=torch.bfloat16)
ssm_state: tensor([[[[0., 0., 0.,  ..., 0., 0., 0.]],

         [[0., 0., 0.,  ..., 0., 0., 0.]],

         [[0., 0., 0.,  ..., 0., 0., 0.]],

         ...,

         [[0., 0., 0.,  ..., 0., 0., 0.]],

         [[0., 0., 0.,  ..., 0., 0., 0.]],

         [[0., 0., 0.,  ..., 0., 0., 0.]]]], device='cuda:0',
       dtype=torch.bfloat16)
x.stride0: (873, 1, 873)
x.shape: torch.Size([1, 873, 3584])
x.stride1: (3128832, 3584, 1)
x.stride2: (3128832, 1, 3584)
conv_state1: (10752, 1, 3584)
conv_state2: (10752, 1, 3584)
conv_state3: (10752, 1, 3584)
conv_state: torch.Size([1, 3584, 3])
conv1d_out: torch.Size([1, 3584, 873])
rearrange(conv1d_out, 'b d l -> (b l) d') shape: torch.Size([873, 3584])
x_proj_weight shape: torch.Size([144, 3584])
x_dbl: torch.Size([873, 144])
delta_proj_weight @ x_dbl[:, :delta_rank].t(): torch.Size([3584, 873])
conv_state: tensor([[[-6.4688, -8.4375, -8.0000],
         [ 1.1484, -0.3125,  0.4902],
         [ 3.6562,  3.5781,  4.9688],
         ...,
         [-5.6562, -5.8125, -6.5625],
         [-5.2812, -5.6562, -5.3125],
         [-1.8281, -1.8203, -1.5859]]], device='cuda:0', dtype=torch.bfloat16,
       grad_fn=<AsStridedBackward0>)
ssm_state: tensor([[[[ 0.0000e+00,  0.0000e+00,  1.0138e-25,  ...,  0.0000e+00,
            1.1498e-22, -6.1691e-06]],

         [[ 0.0000e+00,  0.0000e+00, -1.1439e+34,  ...,  0.0000e+00,
            7.6093e+19,  5.0354e-03]],

         [[ 0.0000e+00,  0.0000e+00, -1.8239e-05,  ...,  0.0000e+00,
           -4.8392e+36, -1.6098e-03]],

         ...,

         [[ 0.0000e+00,  0.0000e+00,  2.1767e+21,  ...,  0.0000e+00,
            4.3368e-18,  1.2695e-02]],

         [[ 0.0000e+00,  0.0000e+00,  2.1094e-14,  ...,  0.0000e+00,
            9.6436e-03, -3.9062e-03]],

         [[ 0.0000e+00,  0.0000e+00,  5.9360e-18,  ...,  0.0000e+00,
            4.4375e+00,  3.1250e-02]]]], device='cuda:0', dtype=torch.bfloat16)
here here
no back here:
not here, not here
conv_state: tensor([[[0., 0., 0.],
         [0., 0., 0.],
         [0., 0., 0.],
         ...,
         [0., 0., 0.],
         [0., 0., 0.],
         [0., 0., 0.]]], device='cuda:0', dtype=torch.bfloat16)
ssm_state: tensor([[[[0., 0., 0.,  ..., 0., 0., 0.]],

         [[0., 0., 0.,  ..., 0., 0., 0.]],

         [[0., 0., 0.,  ..., 0., 0., 0.]],

         ...,

         [[0., 0., 0.,  ..., 0., 0., 0.]],

         [[0., 0., 0.,  ..., 0., 0., 0.]],

         [[0., 0., 0.,  ..., 0., 0., 0.]]]], device='cuda:0',
       dtype=torch.bfloat16)
x.stride0: (873, 1, 873)
x.shape: torch.Size([1, 873, 3584])
x.stride1: (3128832, 3584, 1)
x.stride2: (3128832, 1, 3584)
conv_state1: (10752, 1, 3584)
conv_state2: (10752, 1, 3584)
conv_state3: (10752, 1, 3584)
conv_state: torch.Size([1, 3584, 3])
conv1d_out: torch.Size([1, 3584, 873])
rearrange(conv1d_out, 'b d l -> (b l) d') shape: torch.Size([873, 3584])
x_proj_weight shape: torch.Size([144, 3584])
x_dbl: torch.Size([873, 144])
delta_proj_weight @ x_dbl[:, :delta_rank].t(): torch.Size([3584, 873])
conv_state: tensor([[[-0.3457, -0.8320, -2.0000],
         [-4.5000, -3.0781, -3.9688],
         [ 0.8125,  1.4609,  0.0359],
         ...,
         [ 3.2656,  1.5781,  1.5156],
         [-1.9062, -2.1719, -2.0000],
         [ 1.9531,  1.9766,  2.4062]]], device='cuda:0', dtype=torch.bfloat16,
       grad_fn=<AsStridedBackward0>)
ssm_state: tensor([[[[ 0.0000e+00,  0.0000e+00,  5.0037e+20,  ...,  0.0000e+00,
            2.4442e+20, -4.4250e-03]],

         [[ 0.0000e+00,  0.0000e+00, -3.2370e+16,  ...,  0.0000e+00,
            1.3708e+37, -7.8125e-03]],

         [[ 0.0000e+00,  0.0000e+00, -5.1457e+14,  ...,  0.0000e+00,
            7.8583e+21,  1.2970e-03]],

         ...,

         [[ 0.0000e+00,  0.0000e+00,  5.8787e+10,  ...,  0.0000e+00,
            3.8814e-17, -1.2283e-03]],

         [[ 0.0000e+00,  0.0000e+00, -3.4079e-28,  ...,  0.0000e+00,
            1.0403e-36,  3.8452e-03]],

         [[ 0.0000e+00,  0.0000e+00, -3.3742e-31,  ...,  0.0000e+00,
            5.6133e-13,  1.2939e-02]]]], device='cuda:0', dtype=torch.bfloat16)
here here
no back here:
not here, not here
conv_state: tensor([[[0., 0., 0.],
         [0., 0., 0.],
         [0., 0., 0.],
         ...,
         [0., 0., 0.],
         [0., 0., 0.],
         [0., 0., 0.]]], device='cuda:0', dtype=torch.bfloat16)
ssm_state: tensor([[[[0., 0., 0.,  ..., 0., 0., 0.]],

         [[0., 0., 0.,  ..., 0., 0., 0.]],

         [[0., 0., 0.,  ..., 0., 0., 0.]],

         ...,

         [[0., 0., 0.,  ..., 0., 0., 0.]],

         [[0., 0., 0.,  ..., 0., 0., 0.]],

         [[0., 0., 0.,  ..., 0., 0., 0.]]]], device='cuda:0',
       dtype=torch.bfloat16)
x.stride0: (873, 1, 873)
x.shape: torch.Size([1, 873, 3584])
x.stride1: (3128832, 3584, 1)
x.stride2: (3128832, 1, 3584)
conv_state1: (10752, 1, 3584)
conv_state2: (10752, 1, 3584)
conv_state3: (10752, 1, 3584)
conv_state: torch.Size([1, 3584, 3])
conv1d_out: torch.Size([1, 3584, 873])
rearrange(conv1d_out, 'b d l -> (b l) d') shape: torch.Size([873, 3584])
x_proj_weight shape: torch.Size([144, 3584])
x_dbl: torch.Size([873, 144])
delta_proj_weight @ x_dbl[:, :delta_rank].t(): torch.Size([3584, 873])
conv_state: tensor([[[-2.2812, -1.6875, -3.0312],
         [ 0.9297,  0.5273,  1.0703],
         [-2.1250, -1.3125, -2.0156],
         ...,
         [ 0.3652,  0.2109,  1.7656],
         [-1.9922, -1.2578, -0.5195],
         [ 0.8320,  1.7500,  2.8906]]], device='cuda:0', dtype=torch.bfloat16,
       grad_fn=<AsStridedBackward0>)
ssm_state: tensor([[[[ 0.0000e+00,  0.0000e+00,  9.7649e+06,  ...,  0.0000e+00,
            2.5422e-32,  1.5869e-03]],

         [[ 0.0000e+00,  0.0000e+00, -6.9389e-16,  ...,  0.0000e+00,
           -7.7345e-31, -6.4087e-03]],

         [[ 0.0000e+00,  0.0000e+00, -1.6797e-01,  ...,  0.0000e+00,
            3.6264e-21, -2.9755e-03]],

         ...,

         [[ 0.0000e+00,  0.0000e+00,  3.8163e+35,  ...,  0.0000e+00,
           -8.4679e+32, -7.1335e-04]],

         [[ 0.0000e+00,  0.0000e+00,  2.2259e-07,  ...,  0.0000e+00,
           -2.8690e+12, -8.8120e-04]],

         [[ 0.0000e+00,  0.0000e+00,  1.6096e+36,  ...,  0.0000e+00,
           -1.2062e+01, -1.8311e-03]]]], device='cuda:0', dtype=torch.bfloat16)
here here
no back here:
not here, not here
conv_state: tensor([[[0., 0., 0.],
         [0., 0., 0.],
         [0., 0., 0.],
         ...,
         [0., 0., 0.],
         [0., 0., 0.],
         [0., 0., 0.]]], device='cuda:0', dtype=torch.bfloat16)
ssm_state: tensor([[[[0., 0., 0.,  ..., 0., 0., 0.]],

         [[0., 0., 0.,  ..., 0., 0., 0.]],

         [[0., 0., 0.,  ..., 0., 0., 0.]],

         ...,

         [[0., 0., 0.,  ..., 0., 0., 0.]],

         [[0., 0., 0.,  ..., 0., 0., 0.]],

         [[0., 0., 0.,  ..., 0., 0., 0.]]]], device='cuda:0',
       dtype=torch.bfloat16)
x.stride0: (873, 1, 873)
x.shape: torch.Size([1, 873, 3584])
x.stride1: (3128832, 3584, 1)
x.stride2: (3128832, 1, 3584)
conv_state1: (10752, 1, 3584)
conv_state2: (10752, 1, 3584)
conv_state3: (10752, 1, 3584)
conv_state: torch.Size([1, 3584, 3])
conv1d_out: torch.Size([1, 3584, 873])
rearrange(conv1d_out, 'b d l -> (b l) d') shape: torch.Size([873, 3584])
x_proj_weight shape: torch.Size([144, 3584])
x_dbl: torch.Size([873, 144])
delta_proj_weight @ x_dbl[:, :delta_rank].t(): torch.Size([3584, 873])
conv_state: tensor([[[-0.4824, -2.5625, -1.7812],
         [-2.2812, -3.2500, -0.9883],
         [-0.1240,  2.4531, -0.1777],
         ...,
         [ 0.6172,  0.4570,  0.5039],
         [-3.5781, -3.5469, -3.7969],
         [-0.0164, -2.3281,  0.8984]]], device='cuda:0', dtype=torch.bfloat16,
       grad_fn=<AsStridedBackward0>)
ssm_state: tensor([[[[ 0.0000e+00,  0.0000e+00, -5.2242e+17,  ...,  0.0000e+00,
            7.0408e-07,  5.7373e-03]],

         [[ 0.0000e+00,  0.0000e+00, -8.8905e-17,  ...,  0.0000e+00,
            2.3660e+38,  1.5869e-03]],

         [[ 0.0000e+00,  0.0000e+00,  7.7760e+37,  ...,  0.0000e+00,
           -5.8301e+08, -1.2970e-04]],

         ...,

         [[ 0.0000e+00,  0.0000e+00, -3.5827e+35,  ...,  0.0000e+00,
           -1.0135e+37,  6.1035e-04]],

         [[ 0.0000e+00,  0.0000e+00,  1.2519e-36,  ...,  0.0000e+00,
           -2.0805e-11, -7.8735e-03]],

         [[ 0.0000e+00,  0.0000e+00, -6.1866e+07,  ...,  0.0000e+00,
           -7.8613e-02,  5.9814e-03]]]], device='cuda:0', dtype=torch.bfloat16)
here here
no back here:
not here, not here
conv_state: tensor([[[0., 0., 0.],
         [0., 0., 0.],
         [0., 0., 0.],
         ...,
         [0., 0., 0.],
         [0., 0., 0.],
         [0., 0., 0.]]], device='cuda:0', dtype=torch.bfloat16)
ssm_state: tensor([[[[0., 0., 0.,  ..., 0., 0., 0.]],

         [[0., 0., 0.,  ..., 0., 0., 0.]],

         [[0., 0., 0.,  ..., 0., 0., 0.]],

         ...,

         [[0., 0., 0.,  ..., 0., 0., 0.]],

         [[0., 0., 0.,  ..., 0., 0., 0.]],

         [[0., 0., 0.,  ..., 0., 0., 0.]]]], device='cuda:0',
       dtype=torch.bfloat16)
x.stride0: (873, 1, 873)
x.shape: torch.Size([1, 873, 3584])
x.stride1: (3128832, 3584, 1)
x.stride2: (3128832, 1, 3584)
conv_state1: (10752, 1, 3584)
conv_state2: (10752, 1, 3584)
conv_state3: (10752, 1, 3584)
conv_state: torch.Size([1, 3584, 3])
conv1d_out: torch.Size([1, 3584, 873])
rearrange(conv1d_out, 'b d l -> (b l) d') shape: torch.Size([873, 3584])
x_proj_weight shape: torch.Size([144, 3584])
x_dbl: torch.Size([873, 144])
delta_proj_weight @ x_dbl[:, :delta_rank].t(): torch.Size([3584, 873])
conv_state: tensor([[[-2.8438, -0.4375,  0.2754],
         [ 3.2812,  2.5156,  2.3125],
         [ 1.6641, -0.6406,  1.9375],
         ...,
         [ 0.4961, -0.5938,  0.4844],
         [-0.5820, -0.5078,  0.6016],
         [ 1.4062,  0.8828, -0.0171]]], device='cuda:0', dtype=torch.bfloat16,
       grad_fn=<AsStridedBackward0>)
ssm_state: tensor([[[[ 0.0000e+00,  0.0000e+00, -1.9263e+15,  ...,  0.0000e+00,
            8.0468e-20, -1.0742e-02]],

         [[ 0.0000e+00,  0.0000e+00, -3.7816e+20,  ...,  0.0000e+00,
           -3.7218e+37,  9.0332e-03]],

         [[ 0.0000e+00,  0.0000e+00,  1.1548e-07,  ...,  0.0000e+00,
            1.4855e+29, -7.3547e-03]],

         ...,

         [[ 0.0000e+00,  0.0000e+00,  3.0966e-08,  ...,  0.0000e+00,
            1.5550e-26, -1.4343e-03]],

         [[ 0.0000e+00,  0.0000e+00,  1.7039e+07,  ...,  0.0000e+00,
           -9.4489e+11, -1.0605e-03]],

         [[ 0.0000e+00,  0.0000e+00,  7.7758e+27,  ...,  0.0000e+00,
           -4.8800e+02, -1.1063e-03]]]], device='cuda:0', dtype=torch.bfloat16)
here here
no back here:
not here, not here
conv_state: tensor([[[0., 0., 0.],
         [0., 0., 0.],
         [0., 0., 0.],
         ...,
         [0., 0., 0.],
         [0., 0., 0.],
         [0., 0., 0.]]], device='cuda:0', dtype=torch.bfloat16)
ssm_state: tensor([[[[0., 0., 0.,  ..., 0., 0., 0.]],

         [[0., 0., 0.,  ..., 0., 0., 0.]],

         [[0., 0., 0.,  ..., 0., 0., 0.]],

         ...,

         [[0., 0., 0.,  ..., 0., 0., 0.]],

         [[0., 0., 0.,  ..., 0., 0., 0.]],

         [[0., 0., 0.,  ..., 0., 0., 0.]]]], device='cuda:0',
       dtype=torch.bfloat16)
x.stride0: (873, 1, 873)
x.shape: torch.Size([1, 873, 3584])
x.stride1: (3128832, 3584, 1)
x.stride2: (3128832, 1, 3584)
conv_state1: (10752, 1, 3584)
conv_state2: (10752, 1, 3584)
conv_state3: (10752, 1, 3584)
conv_state: torch.Size([1, 3584, 3])
conv1d_out: torch.Size([1, 3584, 873])
rearrange(conv1d_out, 'b d l -> (b l) d') shape: torch.Size([873, 3584])
x_proj_weight shape: torch.Size([144, 3584])
x_dbl: torch.Size([873, 144])
delta_proj_weight @ x_dbl[:, :delta_rank].t(): torch.Size([3584, 873])
conv_state: tensor([[[ 0.2178,  2.5312,  0.5664],
         [-2.4531, -4.5312, -3.5781],
         [-1.3281, -0.7266, -0.5078],
         ...,
         [-0.2471,  0.7188,  1.4141],
         [-0.6875,  0.0679, -1.3281],
         [ 2.5469,  1.3672,  1.3281]]], device='cuda:0', dtype=torch.bfloat16,
       grad_fn=<AsStridedBackward0>)
ssm_state: tensor([[[[ 0.0000e+00,  0.0000e+00,  4.1034e-13,  ...,  0.0000e+00,
            4.0960e+04, -2.1820e-03]],

         [[ 0.0000e+00,  0.0000e+00,  5.2214e-05,  ...,  0.0000e+00,
           -1.4953e+14,  9.5215e-03]],

         [[ 0.0000e+00,  0.0000e+00, -1.6712e+06,  ...,  0.0000e+00,
            7.9346e-37, -1.3733e-03]],

         ...,

         [[ 0.0000e+00,  0.0000e+00, -1.0904e+36,  ...,  0.0000e+00,
            1.0420e+07, -9.3384e-03]],

         [[ 0.0000e+00,  0.0000e+00,  1.8529e-20,  ...,  0.0000e+00,
            8.0111e+08,  1.4191e-03]],

         [[ 0.0000e+00,  0.0000e+00,  4.1351e-07,  ...,  0.0000e+00,
           -5.5318e-24,  4.0627e-04]]]], device='cuda:0', dtype=torch.bfloat16)
here here
no back here:
not here, not here
conv_state: tensor([[[0., 0., 0.],
         [0., 0., 0.],
         [0., 0., 0.],
         ...,
         [0., 0., 0.],
         [0., 0., 0.],
         [0., 0., 0.]]], device='cuda:0', dtype=torch.bfloat16)
ssm_state: tensor([[[[0., 0., 0.,  ..., 0., 0., 0.]],

         [[0., 0., 0.,  ..., 0., 0., 0.]],

         [[0., 0., 0.,  ..., 0., 0., 0.]],

         ...,

         [[0., 0., 0.,  ..., 0., 0., 0.]],

         [[0., 0., 0.,  ..., 0., 0., 0.]],

         [[0., 0., 0.,  ..., 0., 0., 0.]]]], device='cuda:0',
       dtype=torch.bfloat16)
x.stride0: (873, 1, 873)
x.shape: torch.Size([1, 873, 3584])
x.stride1: (3128832, 3584, 1)
x.stride2: (3128832, 1, 3584)
conv_state1: (10752, 1, 3584)
conv_state2: (10752, 1, 3584)
conv_state3: (10752, 1, 3584)
conv_state: torch.Size([1, 3584, 3])
conv1d_out: torch.Size([1, 3584, 873])
rearrange(conv1d_out, 'b d l -> (b l) d') shape: torch.Size([873, 3584])
x_proj_weight shape: torch.Size([144, 3584])
x_dbl: torch.Size([873, 144])
delta_proj_weight @ x_dbl[:, :delta_rank].t(): torch.Size([3584, 873])
conv_state: tensor([[[ 1.7812,  0.7344, -1.7344],
         [-1.4453, -1.5078,  0.0571],
         [-0.1416,  0.1377, -0.7383],
         ...,
         [ 2.7812,  5.8125,  7.2188],
         [ 2.8594,  1.5469,  2.1250],
         [ 3.4688,  3.9531,  2.4688]]], device='cuda:0', dtype=torch.bfloat16,
       grad_fn=<AsStridedBackward0>)
ssm_state: tensor([[[[ 0.0000e+00,  0.0000e+00,  3.0305e+30,  ...,  0.0000e+00,
           -1.9478e+17,  3.6926e-03]],

         [[ 0.0000e+00,  0.0000e+00, -1.0093e+12,  ...,  0.0000e+00,
           -2.3040e+04, -1.5198e-02]],

         [[ 0.0000e+00,  0.0000e+00, -2.6871e-30,  ...,  0.0000e+00,
           -1.9670e-05,  6.8359e-03]],

         ...,

         [[ 0.0000e+00,  0.0000e+00, -6.3787e-31,  ...,  0.0000e+00,
           -1.0551e+37, -1.9775e-02]],

         [[ 0.0000e+00,  0.0000e+00,  6.6202e+35,  ...,  0.0000e+00,
            1.9268e+32,  1.0803e-02]],

         [[ 0.0000e+00,  0.0000e+00,  2.9360e+08,  ...,  0.0000e+00,
           -5.7753e-11, -4.6997e-03]]]], device='cuda:0', dtype=torch.bfloat16)
here here
no back here:
not here, not here
conv_state: tensor([[[0., 0., 0.],
         [0., 0., 0.],
         [0., 0., 0.],
         ...,
         [0., 0., 0.],
         [0., 0., 0.],
         [0., 0., 0.]]], device='cuda:0', dtype=torch.bfloat16)
ssm_state: tensor([[[[0., 0., 0.,  ..., 0., 0., 0.]],

         [[0., 0., 0.,  ..., 0., 0., 0.]],

         [[0., 0., 0.,  ..., 0., 0., 0.]],

         ...,

         [[0., 0., 0.,  ..., 0., 0., 0.]],

         [[0., 0., 0.,  ..., 0., 0., 0.]],

         [[0., 0., 0.,  ..., 0., 0., 0.]]]], device='cuda:0',
       dtype=torch.bfloat16)
x.stride0: (873, 1, 873)
x.shape: torch.Size([1, 873, 3584])
x.stride1: (3128832, 3584, 1)
x.stride2: (3128832, 1, 3584)
conv_state1: (10752, 1, 3584)
conv_state2: (10752, 1, 3584)
conv_state3: (10752, 1, 3584)
conv_state: torch.Size([1, 3584, 3])
conv1d_out: torch.Size([1, 3584, 873])
rearrange(conv1d_out, 'b d l -> (b l) d') shape: torch.Size([873, 3584])
x_proj_weight shape: torch.Size([144, 3584])
x_dbl: torch.Size([873, 144])
delta_proj_weight @ x_dbl[:, :delta_rank].t(): torch.Size([3584, 873])
conv_state: tensor([[[-2.2500, -1.4375, -1.3047],
         [ 2.4219,  0.3613, -3.1250],
         [ 1.7188,  0.2441,  2.0781],
         ...,
         [-2.8750, -2.9219, -2.0469],
         [ 3.9688,  3.7656,  3.0312],
         [ 0.7383, -0.7617, -0.8398]]], device='cuda:0', dtype=torch.bfloat16,
       grad_fn=<AsStridedBackward0>)
ssm_state: tensor([[[[ 0.0000e+00,  0.0000e+00, -1.3475e+19,  ...,  0.0000e+00,
           -3.0045e-32,  9.5215e-03]],

         [[ 0.0000e+00,  0.0000e+00, -4.2992e+07,  ...,  0.0000e+00,
            4.3195e-16,  7.4768e-03]],

         [[ 0.0000e+00,  0.0000e+00,  1.6483e+18,  ...,  0.0000e+00,
            3.3073e+16, -5.9891e-04]],

         ...,

         [[ 0.0000e+00,  0.0000e+00, -5.9557e-22,  ...,  0.0000e+00,
           -1.8175e-27,  6.2256e-03]],

         [[ 0.0000e+00,  0.0000e+00, -1.2695e-36,  ...,  0.0000e+00,
           -1.4211e-13, -5.7373e-03]],

         [[ 0.0000e+00,  0.0000e+00,  1.6709e-22,  ...,  0.0000e+00,
            2.1906e+19,  4.4861e-03]]]], device='cuda:0', dtype=torch.bfloat16)
here here
no back here:
not here, not here
conv_state: tensor([[[0., 0., 0.],
         [0., 0., 0.],
         [0., 0., 0.],
         ...,
         [0., 0., 0.],
         [0., 0., 0.],
         [0., 0., 0.]]], device='cuda:0', dtype=torch.bfloat16)
ssm_state: tensor([[[[0., 0., 0.,  ..., 0., 0., 0.]],

         [[0., 0., 0.,  ..., 0., 0., 0.]],

         [[0., 0., 0.,  ..., 0., 0., 0.]],

         ...,

         [[0., 0., 0.,  ..., 0., 0., 0.]],

         [[0., 0., 0.,  ..., 0., 0., 0.]],

         [[0., 0., 0.,  ..., 0., 0., 0.]]]], device='cuda:0',
       dtype=torch.bfloat16)
x.stride0: (873, 1, 873)
x.shape: torch.Size([1, 873, 3584])
x.stride1: (3128832, 3584, 1)
x.stride2: (3128832, 1, 3584)
conv_state1: (10752, 1, 3584)
conv_state2: (10752, 1, 3584)
conv_state3: (10752, 1, 3584)
conv_state: torch.Size([1, 3584, 3])
conv1d_out: torch.Size([1, 3584, 873])
rearrange(conv1d_out, 'b d l -> (b l) d') shape: torch.Size([873, 3584])
x_proj_weight shape: torch.Size([144, 3584])
x_dbl: torch.Size([873, 144])
delta_proj_weight @ x_dbl[:, :delta_rank].t(): torch.Size([3584, 873])
conv_state: tensor([[[-0.8164,  0.1465, -0.3477],
         [ 0.7188,  0.4160,  3.1250],
         [-1.9219, -2.5938, -1.2891],
         ...,
         [-1.2734, -0.2393,  0.0884],
         [ 0.8359, -0.9648, -0.4922],
         [ 1.0078,  0.0452,  1.5703]]], device='cuda:0', dtype=torch.bfloat16,
       grad_fn=<AsStridedBackward0>)
ssm_state: tensor([[[[ 0.0000e+00,  0.0000e+00,  1.5658e+34,  ...,  0.0000e+00,
           -2.4300e+02, -2.1667e-03]],

         [[ 0.0000e+00,  0.0000e+00,  6.0659e+29,  ...,  0.0000e+00,
           -1.9923e+08,  1.5747e-02]],

         [[ 0.0000e+00,  0.0000e+00, -9.5539e-23,  ...,  0.0000e+00,
           -9.0325e-29, -1.6113e-02]],

         ...,

         [[ 0.0000e+00,  0.0000e+00,  2.3173e-30,  ...,  0.0000e+00,
           -3.5593e-27, -8.6670e-03]],

         [[ 0.0000e+00,  0.0000e+00,  1.1134e-33,  ...,  0.0000e+00,
            3.8251e+23,  3.9577e-05]],

         [[ 0.0000e+00,  0.0000e+00,  4.9067e+35,  ...,  0.0000e+00,
            1.0697e-36, -1.5442e-02]]]], device='cuda:0', dtype=torch.bfloat16)
here here
no back here:
not here, not here
conv_state: tensor([[[0., 0., 0.],
         [0., 0., 0.],
         [0., 0., 0.],
         ...,
         [0., 0., 0.],
         [0., 0., 0.],
         [0., 0., 0.]]], device='cuda:0', dtype=torch.bfloat16)
ssm_state: tensor([[[[0., 0., 0.,  ..., 0., 0., 0.]],

         [[0., 0., 0.,  ..., 0., 0., 0.]],

         [[0., 0., 0.,  ..., 0., 0., 0.]],

         ...,

         [[0., 0., 0.,  ..., 0., 0., 0.]],

         [[0., 0., 0.,  ..., 0., 0., 0.]],

         [[0., 0., 0.,  ..., 0., 0., 0.]]]], device='cuda:0',
       dtype=torch.bfloat16)
x.stride0: (873, 1, 873)
x.shape: torch.Size([1, 873, 3584])
x.stride1: (3128832, 3584, 1)
x.stride2: (3128832, 1, 3584)
conv_state1: (10752, 1, 3584)
conv_state2: (10752, 1, 3584)
conv_state3: (10752, 1, 3584)
conv_state: torch.Size([1, 3584, 3])
conv1d_out: torch.Size([1, 3584, 873])
rearrange(conv1d_out, 'b d l -> (b l) d') shape: torch.Size([873, 3584])
x_proj_weight shape: torch.Size([144, 3584])
x_dbl: torch.Size([873, 144])
delta_proj_weight @ x_dbl[:, :delta_rank].t(): torch.Size([3584, 873])
conv_state: tensor([[[-3.3438e+00, -5.2500e+00, -3.7656e+00],
         [ 1.5469e+00,  1.5547e+00,  9.8828e-01],
         [-4.6875e-01, -2.4531e+00,  4.7461e-01],
         ...,
         [-2.5156e+00, -2.0000e+00, -1.8906e+00],
         [ 4.7302e-03,  1.8164e-01,  1.4160e-01],
         [ 1.3281e+00,  2.7969e+00,  3.8281e+00]]], device='cuda:0',
       dtype=torch.bfloat16, grad_fn=<AsStridedBackward0>)
ssm_state: tensor([[[[ 0.0000e+00,  0.0000e+00, -1.7624e-03,  ...,  0.0000e+00,
           -3.4043e+28, -2.1118e-02]],

         [[ 0.0000e+00,  0.0000e+00, -1.0578e+24,  ...,  0.0000e+00,
           -1.8933e-29,  3.0518e-02]],

         [[ 0.0000e+00,  0.0000e+00,  1.0596e-10,  ...,  0.0000e+00,
           -2.8429e+16, -2.2278e-03]],

         ...,

         [[ 0.0000e+00,  0.0000e+00,  8.7043e+25,  ...,  0.0000e+00,
           -8.1914e+13, -1.7166e-03]],

         [[ 0.0000e+00,  0.0000e+00,  7.8952e+21,  ...,  0.0000e+00,
            2.5884e-30, -4.1809e-03]],

         [[ 0.0000e+00,  0.0000e+00,  7.8583e-04,  ...,  0.0000e+00,
            3.9873e-35,  1.0605e-03]]]], device='cuda:0', dtype=torch.bfloat16)
here here
no back here:
not here, not here
conv_state: tensor([[[0., 0., 0.],
         [0., 0., 0.],
         [0., 0., 0.],
         ...,
         [0., 0., 0.],
         [0., 0., 0.],
         [0., 0., 0.]]], device='cuda:0', dtype=torch.bfloat16)
ssm_state: tensor([[[[0., 0., 0.,  ..., 0., 0., 0.]],

         [[0., 0., 0.,  ..., 0., 0., 0.]],

         [[0., 0., 0.,  ..., 0., 0., 0.]],

         ...,

         [[0., 0., 0.,  ..., 0., 0., 0.]],

         [[0., 0., 0.,  ..., 0., 0., 0.]],

         [[0., 0., 0.,  ..., 0., 0., 0.]]]], device='cuda:0',
       dtype=torch.bfloat16)
x.stride0: (873, 1, 873)
x.shape: torch.Size([1, 873, 3584])
x.stride1: (3128832, 3584, 1)
x.stride2: (3128832, 1, 3584)
conv_state1: (10752, 1, 3584)
conv_state2: (10752, 1, 3584)
conv_state3: (10752, 1, 3584)
conv_state: torch.Size([1, 3584, 3])
conv1d_out: torch.Size([1, 3584, 873])
rearrange(conv1d_out, 'b d l -> (b l) d') shape: torch.Size([873, 3584])
x_proj_weight shape: torch.Size([144, 3584])
x_dbl: torch.Size([873, 144])
delta_proj_weight @ x_dbl[:, :delta_rank].t(): torch.Size([3584, 873])
conv_state: tensor([[[ 1.3828,  0.6641, -0.1084],
         [-1.3594, -0.6211, -2.6562],
         [-2.2031,  0.0908, -1.6875],
         ...,
         [ 0.1787,  0.6133, -0.3711],
         [-1.9922, -0.3555, -1.5703],
         [ 0.7539, -0.4277, -1.3672]]], device='cuda:0', dtype=torch.bfloat16,
       grad_fn=<AsStridedBackward0>)
ssm_state: tensor([[[[ 0.0000e+00,  0.0000e+00,  6.2172e-13,  ...,  0.0000e+00,
           -3.5058e-25, -1.1719e-02]],

         [[ 0.0000e+00,  0.0000e+00,  1.4336e+06,  ...,  0.0000e+00,
            6.9625e+08,  2.1973e-02]],

         [[ 0.0000e+00,  0.0000e+00,  1.0164e-20,  ...,  0.0000e+00,
            1.4710e+11,  8.2397e-03]],

         ...,

         [[ 0.0000e+00,  0.0000e+00,  6.5020e-38,  ...,  0.0000e+00,
           -2.5411e-19, -1.3916e-02]],

         [[ 0.0000e+00,  0.0000e+00, -3.2212e+11,  ...,  0.0000e+00,
           -1.3298e+26,  1.2024e-02]],

         [[ 0.0000e+00,  0.0000e+00,  1.5485e-21,  ...,  0.0000e+00,
           -3.3951e-04, -1.2756e-02]]]], device='cuda:0', dtype=torch.bfloat16)
here here
no back here:
not here, not here
conv_state: tensor([[[0., 0., 0.],
         [0., 0., 0.],
         [0., 0., 0.],
         ...,
         [0., 0., 0.],
         [0., 0., 0.],
         [0., 0., 0.]]], device='cuda:0', dtype=torch.bfloat16)
ssm_state: tensor([[[[0., 0., 0.,  ..., 0., 0., 0.]],

         [[0., 0., 0.,  ..., 0., 0., 0.]],

         [[0., 0., 0.,  ..., 0., 0., 0.]],

         ...,

         [[0., 0., 0.,  ..., 0., 0., 0.]],

         [[0., 0., 0.,  ..., 0., 0., 0.]],

         [[0., 0., 0.,  ..., 0., 0., 0.]]]], device='cuda:0',
       dtype=torch.bfloat16)
x.stride0: (873, 1, 873)
x.shape: torch.Size([1, 873, 3584])
x.stride1: (3128832, 3584, 1)
x.stride2: (3128832, 1, 3584)
conv_state1: (10752, 1, 3584)
conv_state2: (10752, 1, 3584)
conv_state3: (10752, 1, 3584)
conv_state: torch.Size([1, 3584, 3])
conv1d_out: torch.Size([1, 3584, 873])
rearrange(conv1d_out, 'b d l -> (b l) d') shape: torch.Size([873, 3584])
x_proj_weight shape: torch.Size([144, 3584])
x_dbl: torch.Size([873, 144])
delta_proj_weight @ x_dbl[:, :delta_rank].t(): torch.Size([3584, 873])
conv_state: tensor([[[ 0.7031,  0.0265, -1.8438],
         [ 0.8008, -1.3672,  0.3027],
         [ 3.5469,  4.5000,  2.5781],
         ...,
         [ 0.3281,  3.2344,  2.1406],
         [ 0.9648,  3.0469,  2.0938],
         [-0.9766, -1.2969, -0.4805]]], device='cuda:0', dtype=torch.bfloat16,
       grad_fn=<AsStridedBackward0>)
ssm_state: tensor([[[[ 0.0000e+00,  0.0000e+00, -5.1356e+22,  ...,  0.0000e+00,
           -3.1959e-38, -2.4719e-03]],

         [[ 0.0000e+00,  0.0000e+00,  4.5524e+24,  ...,  0.0000e+00,
            1.3084e+14, -6.0654e-04]],

         [[ 0.0000e+00,  0.0000e+00, -6.6336e-15,  ...,  0.0000e+00,
           -2.6822e-07,  3.9062e-02]],

         ...,

         [[ 0.0000e+00,  0.0000e+00,  9.0473e-25,  ...,  0.0000e+00,
            3.9387e-20, -6.9275e-03]],

         [[ 0.0000e+00,  0.0000e+00, -1.1882e-29,  ...,  0.0000e+00,
           -5.3906e-01, -1.3123e-02]],

         [[ 0.0000e+00,  0.0000e+00, -5.1034e-20,  ...,  0.0000e+00,
            1.1402e-31, -1.1963e-02]]]], device='cuda:0', dtype=torch.bfloat16)
here here
no back here:
not here, not here
conv_state: tensor([[[0., 0., 0.],
         [0., 0., 0.],
         [0., 0., 0.],
         ...,
         [0., 0., 0.],
         [0., 0., 0.],
         [0., 0., 0.]]], device='cuda:0', dtype=torch.bfloat16)
ssm_state: tensor([[[[0., 0., 0.,  ..., 0., 0., 0.]],

         [[0., 0., 0.,  ..., 0., 0., 0.]],

         [[0., 0., 0.,  ..., 0., 0., 0.]],

         ...,

         [[0., 0., 0.,  ..., 0., 0., 0.]],

         [[0., 0., 0.,  ..., 0., 0., 0.]],

         [[0., 0., 0.,  ..., 0., 0., 0.]]]], device='cuda:0',
       dtype=torch.bfloat16)
x.stride0: (873, 1, 873)
x.shape: torch.Size([1, 873, 3584])
x.stride1: (3128832, 3584, 1)
x.stride2: (3128832, 1, 3584)
conv_state1: (10752, 1, 3584)
conv_state2: (10752, 1, 3584)
conv_state3: (10752, 1, 3584)
conv_state: torch.Size([1, 3584, 3])
conv1d_out: torch.Size([1, 3584, 873])
rearrange(conv1d_out, 'b d l -> (b l) d') shape: torch.Size([873, 3584])
x_proj_weight shape: torch.Size([144, 3584])
x_dbl: torch.Size([873, 144])
delta_proj_weight @ x_dbl[:, :delta_rank].t(): torch.Size([3584, 873])
conv_state: tensor([[[-0.2578, -2.5156, -2.1406],
         [ 0.8086,  0.3457,  1.5625],
         [-1.3438,  0.4844, -0.0050],
         ...,
         [-1.1562, -2.8750, -1.8984],
         [-3.2188, -0.4199, -0.9141],
         [-3.6406, -4.5625, -3.2344]]], device='cuda:0', dtype=torch.bfloat16,
       grad_fn=<AsStridedBackward0>)
ssm_state: tensor([[[[ 0.0000e+00,  0.0000e+00, -7.2634e+19,  ...,  0.0000e+00,
            1.4298e-18, -1.1414e-02]],

         [[ 0.0000e+00,  0.0000e+00, -4.4813e+29,  ...,  0.0000e+00,
            1.5187e-25,  3.1982e-02]],

         [[ 0.0000e+00,  0.0000e+00, -9.0600e+14,  ...,  0.0000e+00,
           -2.4920e-36,  6.9580e-03]],

         ...,

         [[ 0.0000e+00,  0.0000e+00,  1.0800e+03,  ...,  0.0000e+00,
            1.9527e+15,  1.0452e-03]],

         [[ 0.0000e+00,  0.0000e+00,  7.5045e+32,  ...,  0.0000e+00,
           -5.3850e+28,  8.4229e-03]],

         [[ 0.0000e+00,  0.0000e+00,  2.9029e+32,  ...,  0.0000e+00,
            4.0550e+05, -6.1035e-03]]]], device='cuda:0', dtype=torch.bfloat16)
here here
no back here:
not here, not here
conv_state: tensor([[[0., 0., 0.],
         [0., 0., 0.],
         [0., 0., 0.],
         ...,
         [0., 0., 0.],
         [0., 0., 0.],
         [0., 0., 0.]]], device='cuda:0', dtype=torch.bfloat16)
ssm_state: tensor([[[[0., 0., 0.,  ..., 0., 0., 0.]],

         [[0., 0., 0.,  ..., 0., 0., 0.]],

         [[0., 0., 0.,  ..., 0., 0., 0.]],

         ...,

         [[0., 0., 0.,  ..., 0., 0., 0.]],

         [[0., 0., 0.,  ..., 0., 0., 0.]],

         [[0., 0., 0.,  ..., 0., 0., 0.]]]], device='cuda:0',
       dtype=torch.bfloat16)
x.stride0: (873, 1, 873)
x.shape: torch.Size([1, 873, 3584])
x.stride1: (3128832, 3584, 1)
x.stride2: (3128832, 1, 3584)
conv_state1: (10752, 1, 3584)
conv_state2: (10752, 1, 3584)
conv_state3: (10752, 1, 3584)
conv_state: torch.Size([1, 3584, 3])
conv1d_out: torch.Size([1, 3584, 873])
rearrange(conv1d_out, 'b d l -> (b l) d') shape: torch.Size([873, 3584])
x_proj_weight shape: torch.Size([144, 3584])
x_dbl: torch.Size([873, 144])
delta_proj_weight @ x_dbl[:, :delta_rank].t(): torch.Size([3584, 873])
conv_state: tensor([[[ 2.2344, -3.0312, -1.4297],
         [ 1.9609,  1.7031,  2.2969],
         [-0.2812,  1.3672,  0.8438],
         ...,
         [ 1.1797, -0.2617,  1.2578],
         [-3.1875, -1.4688, -3.1562],
         [-2.5469, -2.6875, -0.8555]]], device='cuda:0', dtype=torch.bfloat16,
       grad_fn=<AsStridedBackward0>)
ssm_state: tensor([[[[ 0.0000e+00,  0.0000e+00, -1.3739e-15,  ...,  0.0000e+00,
           -1.3271e-16,  1.4404e-02]],

         [[ 0.0000e+00,  0.0000e+00, -2.1853e+13,  ...,  0.0000e+00,
           -8.8905e-17, -1.0620e-02]],

         [[ 0.0000e+00,  0.0000e+00, -1.8279e+27,  ...,  0.0000e+00,
            1.0368e-18,  1.5198e-02]],

         ...,

         [[ 0.0000e+00,  0.0000e+00,  6.9152e-22,  ...,  0.0000e+00,
            1.5450e-04,  1.4648e-02]],

         [[ 0.0000e+00,  0.0000e+00,  7.3612e-06,  ...,  0.0000e+00,
           -3.3742e-31, -6.5231e-04]],

         [[ 0.0000e+00,  0.0000e+00, -9.0599e-05,  ...,  0.0000e+00,
            3.6716e-05,  1.1292e-02]]]], device='cuda:0', dtype=torch.bfloat16)
here here
no back here:
not here, not here
conv_state: tensor([[[0., 0., 0.],
         [0., 0., 0.],
         [0., 0., 0.],
         ...,
         [0., 0., 0.],
         [0., 0., 0.],
         [0., 0., 0.]]], device='cuda:0', dtype=torch.bfloat16)
ssm_state: tensor([[[[0., 0., 0.,  ..., 0., 0., 0.]],

         [[0., 0., 0.,  ..., 0., 0., 0.]],

         [[0., 0., 0.,  ..., 0., 0., 0.]],

         ...,

         [[0., 0., 0.,  ..., 0., 0., 0.]],

         [[0., 0., 0.,  ..., 0., 0., 0.]],

         [[0., 0., 0.,  ..., 0., 0., 0.]]]], device='cuda:0',
       dtype=torch.bfloat16)
x.stride0: (873, 1, 873)
x.shape: torch.Size([1, 873, 3584])
x.stride1: (3128832, 3584, 1)
x.stride2: (3128832, 1, 3584)
conv_state1: (10752, 1, 3584)
conv_state2: (10752, 1, 3584)
conv_state3: (10752, 1, 3584)
conv_state: torch.Size([1, 3584, 3])
conv1d_out: torch.Size([1, 3584, 873])
rearrange(conv1d_out, 'b d l -> (b l) d') shape: torch.Size([873, 3584])
x_proj_weight shape: torch.Size([144, 3584])
x_dbl: torch.Size([873, 144])
delta_proj_weight @ x_dbl[:, :delta_rank].t(): torch.Size([3584, 873])
conv_state: tensor([[[ 1.1641,  3.2500,  3.4688],
         [ 0.3633, -1.3047, -0.2070],
         [ 1.3984, -0.5664, -0.5547],
         ...,
         [ 2.0000, -1.2969,  0.4512],
         [ 2.8750,  2.1562,  2.2031],
         [-1.5391,  1.5234, -0.2676]]], device='cuda:0', dtype=torch.bfloat16,
       grad_fn=<AsStridedBackward0>)
ssm_state: tensor([[[[ 0.0000e+00,  0.0000e+00,  6.3941e-32,  ...,  0.0000e+00,
           -1.0304e-29, -5.8289e-03]],

         [[ 0.0000e+00,  0.0000e+00,  7.1305e+29,  ...,  0.0000e+00,
           -3.7000e+02,  5.5420e-02]],

         [[ 0.0000e+00,  0.0000e+00, -1.9837e-07,  ...,  0.0000e+00,
           -2.8731e+08, -3.7384e-03]],

         ...,

         [[ 0.0000e+00,  0.0000e+00,  2.2334e-37,  ...,  0.0000e+00,
                   nan, -1.0107e-01]],

         [[ 0.0000e+00,  0.0000e+00, -2.8776e+32,  ...,  0.0000e+00,
            2.0133e+08,  2.5757e-02]],

         [[ 0.0000e+00,  0.0000e+00, -4.1708e+25,  ...,  0.0000e+00,
            1.0853e-21, -3.7689e-03]]]], device='cuda:0', dtype=torch.bfloat16)
here here
no back here:
not here, not here
conv_state: tensor([[[0., 0., 0.],
         [0., 0., 0.],
         [0., 0., 0.],
         ...,
         [0., 0., 0.],
         [0., 0., 0.],
         [0., 0., 0.]]], device='cuda:0', dtype=torch.bfloat16)
ssm_state: tensor([[[[0., 0., 0.,  ..., 0., 0., 0.]],

         [[0., 0., 0.,  ..., 0., 0., 0.]],

         [[0., 0., 0.,  ..., 0., 0., 0.]],

         ...,

         [[0., 0., 0.,  ..., 0., 0., 0.]],

         [[0., 0., 0.,  ..., 0., 0., 0.]],

         [[0., 0., 0.,  ..., 0., 0., 0.]]]], device='cuda:0',
       dtype=torch.bfloat16)
x.stride0: (873, 1, 873)
x.shape: torch.Size([1, 873, 3584])
x.stride1: (3128832, 3584, 1)
x.stride2: (3128832, 1, 3584)
conv_state1: (10752, 1, 3584)
conv_state2: (10752, 1, 3584)
conv_state3: (10752, 1, 3584)
conv_state: torch.Size([1, 3584, 3])
conv1d_out: torch.Size([1, 3584, 873])
rearrange(conv1d_out, 'b d l -> (b l) d') shape: torch.Size([873, 3584])
x_proj_weight shape: torch.Size([144, 3584])
x_dbl: torch.Size([873, 144])
delta_proj_weight @ x_dbl[:, :delta_rank].t(): torch.Size([3584, 873])
conv_state: tensor([[[ 0.3418,  1.6562,  1.7969],
         [ 1.4141, -0.8555, -1.0234],
         [-0.2314, -0.4199, -0.5938],
         ...,
         [-1.2578,  1.0781,  1.5547],
         [ 1.1953, -1.9609, -3.1719],
         [-3.9844, -5.3438, -3.7188]]], device='cuda:0', dtype=torch.bfloat16,
       grad_fn=<AsStridedBackward0>)
ssm_state: tensor([[[[ 0.0000e+00,  0.0000e+00, -1.7213e-33,  ...,  0.0000e+00,
            2.3732e-12,  1.9775e-02]],

         [[ 0.0000e+00,  0.0000e+00,  4.4646e+05,  ...,  0.0000e+00,
           -3.5861e+08,  1.2283e-03]],

         [[ 0.0000e+00,  0.0000e+00,  5.9691e+24,  ...,  0.0000e+00,
           -3.3244e-38, -8.8379e-02]],

         ...,

         [[ 0.0000e+00,  0.0000e+00, -1.7166e-27,  ...,  0.0000e+00,
            6.3332e+15, -1.1047e-02]],

         [[ 0.0000e+00,  0.0000e+00, -8.9615e-28,  ...,  0.0000e+00,
           -6.9796e+06,  3.7842e-02]],

         [[ 0.0000e+00,  0.0000e+00, -3.7616e-34,  ...,  0.0000e+00,
            3.1585e-31,  6.9580e-03]]]], device='cuda:0', dtype=torch.bfloat16)
here here
no back here:
not here, not here
conv_state: tensor([[[0., 0., 0.],
         [0., 0., 0.],
         [0., 0., 0.],
         ...,
         [0., 0., 0.],
         [0., 0., 0.],
         [0., 0., 0.]]], device='cuda:0', dtype=torch.bfloat16)
ssm_state: tensor([[[[0., 0., 0.,  ..., 0., 0., 0.]],

         [[0., 0., 0.,  ..., 0., 0., 0.]],

         [[0., 0., 0.,  ..., 0., 0., 0.]],

         ...,

         [[0., 0., 0.,  ..., 0., 0., 0.]],

         [[0., 0., 0.,  ..., 0., 0., 0.]],

         [[0., 0., 0.,  ..., 0., 0., 0.]]]], device='cuda:0',
       dtype=torch.bfloat16)
x.stride0: (873, 1, 873)
x.shape: torch.Size([1, 873, 3584])
x.stride1: (3128832, 3584, 1)
x.stride2: (3128832, 1, 3584)
conv_state1: (10752, 1, 3584)
conv_state2: (10752, 1, 3584)
conv_state3: (10752, 1, 3584)
conv_state: torch.Size([1, 3584, 3])
conv1d_out: torch.Size([1, 3584, 873])
rearrange(conv1d_out, 'b d l -> (b l) d') shape: torch.Size([873, 3584])
x_proj_weight shape: torch.Size([144, 3584])
x_dbl: torch.Size([873, 144])
delta_proj_weight @ x_dbl[:, :delta_rank].t(): torch.Size([3584, 873])
conv_state: tensor([[[-1.7734,  0.1250,  0.8359],
         [-0.8672, -1.2891, -1.4375],
         [ 1.9844,  1.5625,  1.7656],
         ...,
         [ 0.7969,  1.6797,  0.7773],
         [ 2.2344, -0.3672, -1.9922],
         [ 1.9297,  1.8594,  0.7266]]], device='cuda:0', dtype=torch.bfloat16,
       grad_fn=<AsStridedBackward0>)
ssm_state: tensor([[[[ 0.0000e+00,  0.0000e+00,  4.5300e-05,  ...,  0.0000e+00,
            1.0274e+16, -1.0315e-02]],

         [[ 0.0000e+00,  0.0000e+00,  1.2754e+15,  ...,  0.0000e+00,
            6.5536e+07, -2.7466e-02]],

         [[ 0.0000e+00,  0.0000e+00, -1.0648e+33,  ...,  0.0000e+00,
            1.4641e+32, -7.1106e-03]],

         ...,

         [[ 0.0000e+00,  0.0000e+00, -2.0808e+06,  ...,  0.0000e+00,
           -4.0676e-31,  4.5471e-03]],

         [[ 0.0000e+00,  0.0000e+00,  1.8254e+33,  ...,  0.0000e+00,
           -8.7041e-14,  1.9409e-02]],

         [[ 0.0000e+00,  0.0000e+00,  1.5022e-31,  ...,  0.0000e+00,
           -7.6514e-23, -8.1787e-03]]]], device='cuda:0', dtype=torch.bfloat16)
here here
no back here:
not here, not here
conv_state: tensor([[[0., 0., 0.],
         [0., 0., 0.],
         [0., 0., 0.],
         ...,
         [0., 0., 0.],
         [0., 0., 0.],
         [0., 0., 0.]]], device='cuda:0', dtype=torch.bfloat16)
ssm_state: tensor([[[[0., 0., 0.,  ..., 0., 0., 0.]],

         [[0., 0., 0.,  ..., 0., 0., 0.]],

         [[0., 0., 0.,  ..., 0., 0., 0.]],

         ...,

         [[0., 0., 0.,  ..., 0., 0., 0.]],

         [[0., 0., 0.,  ..., 0., 0., 0.]],

         [[0., 0., 0.,  ..., 0., 0., 0.]]]], device='cuda:0',
       dtype=torch.bfloat16)
x.stride0: (873, 1, 873)
x.shape: torch.Size([1, 873, 3584])
x.stride1: (3128832, 3584, 1)
x.stride2: (3128832, 1, 3584)
conv_state1: (10752, 1, 3584)
conv_state2: (10752, 1, 3584)
conv_state3: (10752, 1, 3584)
conv_state: torch.Size([1, 3584, 3])
conv1d_out: torch.Size([1, 3584, 873])
rearrange(conv1d_out, 'b d l -> (b l) d') shape: torch.Size([873, 3584])
x_proj_weight shape: torch.Size([144, 3584])
x_dbl: torch.Size([873, 144])
delta_proj_weight @ x_dbl[:, :delta_rank].t(): torch.Size([3584, 873])
conv_state: tensor([[[nan, nan, nan],
         [nan, nan, nan],
         [nan, nan, nan],
         ...,
         [nan, nan, nan],
         [nan, nan, nan],
         [nan, nan, nan]]], device='cuda:0', dtype=torch.bfloat16,
       grad_fn=<AsStridedBackward0>)
ssm_state: tensor([[[[nan, nan, nan,  ..., nan, nan, nan]],

         [[nan, nan, nan,  ..., nan, nan, nan]],

         [[nan, nan, nan,  ..., nan, nan, nan]],

         ...,

         [[nan, nan, nan,  ..., nan, nan, nan]],

         [[nan, nan, nan,  ..., nan, nan, nan]],

         [[nan, nan, nan,  ..., nan, nan, nan]]]], device='cuda:0',
       dtype=torch.bfloat16)
here here
no back here:
not here, not here
conv_state: tensor([[[0., 0., 0.],
         [0., 0., 0.],
         [0., 0., 0.],
         ...,
         [0., 0., 0.],
         [0., 0., 0.],
         [0., 0., 0.]]], device='cuda:0', dtype=torch.bfloat16)
ssm_state: tensor([[[[0., 0., 0.,  ..., 0., 0., 0.]],

         [[0., 0., 0.,  ..., 0., 0., 0.]],

         [[0., 0., 0.,  ..., 0., 0., 0.]],

         ...,

         [[0., 0., 0.,  ..., 0., 0., 0.]],

         [[0., 0., 0.,  ..., 0., 0., 0.]],

         [[0., 0., 0.,  ..., 0., 0., 0.]]]], device='cuda:0',
       dtype=torch.bfloat16)
x.stride0: (873, 1, 873)
x.shape: torch.Size([1, 873, 3584])
x.stride1: (3128832, 3584, 1)
x.stride2: (3128832, 1, 3584)
conv_state1: (10752, 1, 3584)
conv_state2: (10752, 1, 3584)
conv_state3: (10752, 1, 3584)
conv_state: torch.Size([1, 3584, 3])
conv1d_out: torch.Size([1, 3584, 873])
rearrange(conv1d_out, 'b d l -> (b l) d') shape: torch.Size([873, 3584])
x_proj_weight shape: torch.Size([144, 3584])
x_dbl: torch.Size([873, 144])
delta_proj_weight @ x_dbl[:, :delta_rank].t(): torch.Size([3584, 873])
conv_state: tensor([[[nan, nan, nan],
         [nan, nan, nan],
         [nan, nan, nan],
         ...,
         [nan, nan, nan],
         [nan, nan, nan],
         [nan, nan, nan]]], device='cuda:0', dtype=torch.bfloat16,
       grad_fn=<AsStridedBackward0>)
ssm_state: tensor([[[[nan, nan, nan,  ..., nan, nan, nan]],

         [[nan, nan, nan,  ..., nan, nan, nan]],

         [[nan, nan, nan,  ..., nan, nan, nan]],

         ...,

         [[nan, nan, nan,  ..., nan, nan, nan]],

         [[nan, nan, nan,  ..., nan, nan, nan]],

         [[nan, nan, nan,  ..., nan, nan, nan]]]], device='cuda:0',
       dtype=torch.bfloat16)
here here
no back here:
not here, not here
conv_state: tensor([[[0., 0., 0.],
         [0., 0., 0.],
         [0., 0., 0.],
         ...,
         [0., 0., 0.],
         [0., 0., 0.],
         [0., 0., 0.]]], device='cuda:0', dtype=torch.bfloat16)
ssm_state: tensor([[[[0., 0., 0.,  ..., 0., 0., 0.]],

         [[0., 0., 0.,  ..., 0., 0., 0.]],

         [[0., 0., 0.,  ..., 0., 0., 0.]],

         ...,

         [[0., 0., 0.,  ..., 0., 0., 0.]],

         [[0., 0., 0.,  ..., 0., 0., 0.]],

         [[0., 0., 0.,  ..., 0., 0., 0.]]]], device='cuda:0',
       dtype=torch.bfloat16)
x.stride0: (873, 1, 873)
x.shape: torch.Size([1, 873, 3584])
x.stride1: (3128832, 3584, 1)
x.stride2: (3128832, 1, 3584)
conv_state1: (10752, 1, 3584)
conv_state2: (10752, 1, 3584)
conv_state3: (10752, 1, 3584)
conv_state: torch.Size([1, 3584, 3])
conv1d_out: torch.Size([1, 3584, 873])
rearrange(conv1d_out, 'b d l -> (b l) d') shape: torch.Size([873, 3584])
x_proj_weight shape: torch.Size([144, 3584])
x_dbl: torch.Size([873, 144])
delta_proj_weight @ x_dbl[:, :delta_rank].t(): torch.Size([3584, 873])
conv_state: tensor([[[nan, nan, nan],
         [nan, nan, nan],
         [nan, nan, nan],
         ...,
         [nan, nan, nan],
         [nan, nan, nan],
         [nan, nan, nan]]], device='cuda:0', dtype=torch.bfloat16,
       grad_fn=<AsStridedBackward0>)
ssm_state: tensor([[[[nan, nan, nan,  ..., nan, nan, nan]],

         [[nan, nan, nan,  ..., nan, nan, nan]],

         [[nan, nan, nan,  ..., nan, nan, nan]],

         ...,

         [[nan, nan, nan,  ..., nan, nan, nan]],

         [[nan, nan, nan,  ..., nan, nan, nan]],

         [[nan, nan, nan,  ..., nan, nan, nan]]]], device='cuda:0',
       dtype=torch.bfloat16)
here here
no back here:
not here, not here
conv_state: tensor([[[0., 0., 0.],
         [0., 0., 0.],
         [0., 0., 0.],
         ...,
         [0., 0., 0.],
         [0., 0., 0.],
         [0., 0., 0.]]], device='cuda:0', dtype=torch.bfloat16)
ssm_state: tensor([[[[0., 0., 0.,  ..., 0., 0., 0.]],

         [[0., 0., 0.,  ..., 0., 0., 0.]],

         [[0., 0., 0.,  ..., 0., 0., 0.]],

         ...,

         [[0., 0., 0.,  ..., 0., 0., 0.]],

         [[0., 0., 0.,  ..., 0., 0., 0.]],

         [[0., 0., 0.,  ..., 0., 0., 0.]]]], device='cuda:0',
       dtype=torch.bfloat16)
x.stride0: (873, 1, 873)
x.shape: torch.Size([1, 873, 3584])
x.stride1: (3128832, 3584, 1)
x.stride2: (3128832, 1, 3584)
conv_state1: (10752, 1, 3584)
conv_state2: (10752, 1, 3584)
conv_state3: (10752, 1, 3584)
conv_state: torch.Size([1, 3584, 3])
conv1d_out: torch.Size([1, 3584, 873])
rearrange(conv1d_out, 'b d l -> (b l) d') shape: torch.Size([873, 3584])
x_proj_weight shape: torch.Size([144, 3584])
x_dbl: torch.Size([873, 144])
delta_proj_weight @ x_dbl[:, :delta_rank].t(): torch.Size([3584, 873])
conv_state: tensor([[[nan, nan, nan],
         [nan, nan, nan],
         [nan, nan, nan],
         ...,
         [nan, nan, nan],
         [nan, nan, nan],
         [nan, nan, nan]]], device='cuda:0', dtype=torch.bfloat16,
       grad_fn=<AsStridedBackward0>)
ssm_state: tensor([[[[nan, nan, nan,  ..., nan, nan, nan]],

         [[nan, nan, nan,  ..., nan, nan, nan]],

         [[nan, nan, nan,  ..., nan, nan, nan]],

         ...,

         [[nan, nan, nan,  ..., nan, nan, nan]],

         [[nan, nan, nan,  ..., nan, nan, nan]],

         [[nan, nan, nan,  ..., nan, nan, nan]]]], device='cuda:0',
       dtype=torch.bfloat16)
here here
no back here:
not here, not here
conv_state: tensor([[[0., 0., 0.],
         [0., 0., 0.],
         [0., 0., 0.],
         ...,
         [0., 0., 0.],
         [0., 0., 0.],
         [0., 0., 0.]]], device='cuda:0', dtype=torch.bfloat16)
ssm_state: tensor([[[[0., 0., 0.,  ..., 0., 0., 0.]],

         [[0., 0., 0.,  ..., 0., 0., 0.]],

         [[0., 0., 0.,  ..., 0., 0., 0.]],

         ...,

         [[0., 0., 0.,  ..., 0., 0., 0.]],

         [[0., 0., 0.,  ..., 0., 0., 0.]],

         [[0., 0., 0.,  ..., 0., 0., 0.]]]], device='cuda:0',
       dtype=torch.bfloat16)
x.stride0: (873, 1, 873)
x.shape: torch.Size([1, 873, 3584])
x.stride1: (3128832, 3584, 1)
x.stride2: (3128832, 1, 3584)
conv_state1: (10752, 1, 3584)
conv_state2: (10752, 1, 3584)
conv_state3: (10752, 1, 3584)
conv_state: torch.Size([1, 3584, 3])
conv1d_out: torch.Size([1, 3584, 873])
rearrange(conv1d_out, 'b d l -> (b l) d') shape: torch.Size([873, 3584])
x_proj_weight shape: torch.Size([144, 3584])
x_dbl: torch.Size([873, 144])
delta_proj_weight @ x_dbl[:, :delta_rank].t(): torch.Size([3584, 873])
conv_state: tensor([[[nan, nan, nan],
         [nan, nan, nan],
         [nan, nan, nan],
         ...,
         [nan, nan, nan],
         [nan, nan, nan],
         [nan, nan, nan]]], device='cuda:0', dtype=torch.bfloat16,
       grad_fn=<AsStridedBackward0>)
ssm_state: tensor([[[[nan, nan, nan,  ..., nan, nan, nan]],

         [[nan, nan, nan,  ..., nan, nan, nan]],

         [[nan, nan, nan,  ..., nan, nan, nan]],

         ...,

         [[nan, nan, nan,  ..., nan, nan, nan]],

         [[nan, nan, nan,  ..., nan, nan, nan]],

         [[nan, nan, nan,  ..., nan, nan, nan]]]], device='cuda:0',
       dtype=torch.bfloat16)
here here
no back here:
not here, not here
conv_state: tensor([[[0., 0., 0.],
         [0., 0., 0.],
         [0., 0., 0.],
         ...,
         [0., 0., 0.],
         [0., 0., 0.],
         [0., 0., 0.]]], device='cuda:0', dtype=torch.bfloat16)
ssm_state: tensor([[[[0., 0., 0.,  ..., 0., 0., 0.]],

         [[0., 0., 0.,  ..., 0., 0., 0.]],

         [[0., 0., 0.,  ..., 0., 0., 0.]],

         ...,

         [[0., 0., 0.,  ..., 0., 0., 0.]],

         [[0., 0., 0.,  ..., 0., 0., 0.]],

         [[0., 0., 0.,  ..., 0., 0., 0.]]]], device='cuda:0',
       dtype=torch.bfloat16)
x.stride0: (873, 1, 873)
x.shape: torch.Size([1, 873, 3584])
x.stride1: (3128832, 3584, 1)
x.stride2: (3128832, 1, 3584)
conv_state1: (10752, 1, 3584)
conv_state2: (10752, 1, 3584)
conv_state3: (10752, 1, 3584)
conv_state: torch.Size([1, 3584, 3])
conv1d_out: torch.Size([1, 3584, 873])
rearrange(conv1d_out, 'b d l -> (b l) d') shape: torch.Size([873, 3584])
x_proj_weight shape: torch.Size([144, 3584])
x_dbl: torch.Size([873, 144])
delta_proj_weight @ x_dbl[:, :delta_rank].t(): torch.Size([3584, 873])
conv_state: tensor([[[nan, nan, nan],
         [nan, nan, nan],
         [nan, nan, nan],
         ...,
         [nan, nan, nan],
         [nan, nan, nan],
         [nan, nan, nan]]], device='cuda:0', dtype=torch.bfloat16,
       grad_fn=<AsStridedBackward0>)
ssm_state: tensor([[[[nan, nan, nan,  ..., nan, nan, nan]],

         [[nan, nan, nan,  ..., nan, nan, nan]],

         [[nan, nan, nan,  ..., nan, nan, nan]],

         ...,

         [[nan, nan, nan,  ..., nan, nan, nan]],

         [[nan, nan, nan,  ..., nan, nan, nan]],

         [[nan, nan, nan,  ..., nan, nan, nan]]]], device='cuda:0',
       dtype=torch.bfloat16)
here here
no back here:
not here, not here
conv_state: tensor([[[0., 0., 0.],
         [0., 0., 0.],
         [0., 0., 0.],
         ...,
         [0., 0., 0.],
         [0., 0., 0.],
         [0., 0., 0.]]], device='cuda:0', dtype=torch.bfloat16)
ssm_state: tensor([[[[0., 0., 0.,  ..., 0., 0., 0.]],

         [[0., 0., 0.,  ..., 0., 0., 0.]],

         [[0., 0., 0.,  ..., 0., 0., 0.]],

         ...,

         [[0., 0., 0.,  ..., 0., 0., 0.]],

         [[0., 0., 0.,  ..., 0., 0., 0.]],

         [[0., 0., 0.,  ..., 0., 0., 0.]]]], device='cuda:0',
       dtype=torch.bfloat16)
x.stride0: (873, 1, 873)
x.shape: torch.Size([1, 873, 3584])
x.stride1: (3128832, 3584, 1)
x.stride2: (3128832, 1, 3584)
conv_state1: (10752, 1, 3584)
conv_state2: (10752, 1, 3584)
conv_state3: (10752, 1, 3584)
conv_state: torch.Size([1, 3584, 3])
conv1d_out: torch.Size([1, 3584, 873])
rearrange(conv1d_out, 'b d l -> (b l) d') shape: torch.Size([873, 3584])
x_proj_weight shape: torch.Size([144, 3584])
x_dbl: torch.Size([873, 144])
delta_proj_weight @ x_dbl[:, :delta_rank].t(): torch.Size([3584, 873])
conv_state: tensor([[[nan, nan, nan],
         [nan, nan, nan],
         [nan, nan, nan],
         ...,
         [nan, nan, nan],
         [nan, nan, nan],
         [nan, nan, nan]]], device='cuda:0', dtype=torch.bfloat16,
       grad_fn=<AsStridedBackward0>)
ssm_state: tensor([[[[nan, nan, nan,  ..., nan, nan, nan]],

         [[nan, nan, nan,  ..., nan, nan, nan]],

         [[nan, nan, nan,  ..., nan, nan, nan]],

         ...,

         [[nan, nan, nan,  ..., nan, nan, nan]],

         [[nan, nan, nan,  ..., nan, nan, nan]],

         [[nan, nan, nan,  ..., nan, nan, nan]]]], device='cuda:0',
       dtype=torch.bfloat16)
here here
no back here:
not here, not here
conv_state: tensor([[[0., 0., 0.],
         [0., 0., 0.],
         [0., 0., 0.],
         ...,
         [0., 0., 0.],
         [0., 0., 0.],
         [0., 0., 0.]]], device='cuda:0', dtype=torch.bfloat16)
ssm_state: tensor([[[[0., 0., 0.,  ..., 0., 0., 0.]],

         [[0., 0., 0.,  ..., 0., 0., 0.]],

         [[0., 0., 0.,  ..., 0., 0., 0.]],

         ...,

         [[0., 0., 0.,  ..., 0., 0., 0.]],

         [[0., 0., 0.,  ..., 0., 0., 0.]],

         [[0., 0., 0.,  ..., 0., 0., 0.]]]], device='cuda:0',
       dtype=torch.bfloat16)
x.stride0: (873, 1, 873)
x.shape: torch.Size([1, 873, 3584])
x.stride1: (3128832, 3584, 1)
x.stride2: (3128832, 1, 3584)
conv_state1: (10752, 1, 3584)
conv_state2: (10752, 1, 3584)
conv_state3: (10752, 1, 3584)
conv_state: torch.Size([1, 3584, 3])
conv1d_out: torch.Size([1, 3584, 873])
rearrange(conv1d_out, 'b d l -> (b l) d') shape: torch.Size([873, 3584])
x_proj_weight shape: torch.Size([144, 3584])
x_dbl: torch.Size([873, 144])
delta_proj_weight @ x_dbl[:, :delta_rank].t(): torch.Size([3584, 873])
conv_state: tensor([[[nan, nan, nan],
         [nan, nan, nan],
         [nan, nan, nan],
         ...,
         [nan, nan, nan],
         [nan, nan, nan],
         [nan, nan, nan]]], device='cuda:0', dtype=torch.bfloat16,
       grad_fn=<AsStridedBackward0>)
ssm_state: tensor([[[[nan, nan, nan,  ..., nan, nan, nan]],

         [[nan, nan, nan,  ..., nan, nan, nan]],

         [[nan, nan, nan,  ..., nan, nan, nan]],

         ...,

         [[nan, nan, nan,  ..., nan, nan, nan]],

         [[nan, nan, nan,  ..., nan, nan, nan]],

         [[nan, nan, nan,  ..., nan, nan, nan]]]], device='cuda:0',
       dtype=torch.bfloat16)
here here
no back here:
not here, not here
conv_state: tensor([[[0., 0., 0.],
         [0., 0., 0.],
         [0., 0., 0.],
         ...,
         [0., 0., 0.],
         [0., 0., 0.],
         [0., 0., 0.]]], device='cuda:0', dtype=torch.bfloat16)
ssm_state: tensor([[[[0., 0., 0.,  ..., 0., 0., 0.]],

         [[0., 0., 0.,  ..., 0., 0., 0.]],

         [[0., 0., 0.,  ..., 0., 0., 0.]],

         ...,

         [[0., 0., 0.,  ..., 0., 0., 0.]],

         [[0., 0., 0.,  ..., 0., 0., 0.]],

         [[0., 0., 0.,  ..., 0., 0., 0.]]]], device='cuda:0',
       dtype=torch.bfloat16)
x.stride0: (873, 1, 873)
x.shape: torch.Size([1, 873, 3584])
x.stride1: (3128832, 3584, 1)
x.stride2: (3128832, 1, 3584)
conv_state1: (10752, 1, 3584)
conv_state2: (10752, 1, 3584)
conv_state3: (10752, 1, 3584)
conv_state: torch.Size([1, 3584, 3])
conv1d_out: torch.Size([1, 3584, 873])
rearrange(conv1d_out, 'b d l -> (b l) d') shape: torch.Size([873, 3584])
x_proj_weight shape: torch.Size([144, 3584])
x_dbl: torch.Size([873, 144])
delta_proj_weight @ x_dbl[:, :delta_rank].t(): torch.Size([3584, 873])
conv_state: tensor([[[nan, nan, nan],
         [nan, nan, nan],
         [nan, nan, nan],
         ...,
         [nan, nan, nan],
         [nan, nan, nan],
         [nan, nan, nan]]], device='cuda:0', dtype=torch.bfloat16,
       grad_fn=<AsStridedBackward0>)
ssm_state: tensor([[[[nan, nan, nan,  ..., nan, nan, nan]],

         [[nan, nan, nan,  ..., nan, nan, nan]],

         [[nan, nan, nan,  ..., nan, nan, nan]],

         ...,

         [[nan, nan, nan,  ..., nan, nan, nan]],

         [[nan, nan, nan,  ..., nan, nan, nan]],

         [[nan, nan, nan,  ..., nan, nan, nan]]]], device='cuda:0',
       dtype=torch.bfloat16)
here here
no back here:
not here, not here
conv_state: tensor([[[0., 0., 0.],
         [0., 0., 0.],
         [0., 0., 0.],
         ...,
         [0., 0., 0.],
         [0., 0., 0.],
         [0., 0., 0.]]], device='cuda:0', dtype=torch.bfloat16)
ssm_state: tensor([[[[0., 0., 0.,  ..., 0., 0., 0.]],

         [[0., 0., 0.,  ..., 0., 0., 0.]],

         [[0., 0., 0.,  ..., 0., 0., 0.]],

         ...,

         [[0., 0., 0.,  ..., 0., 0., 0.]],

         [[0., 0., 0.,  ..., 0., 0., 0.]],

         [[0., 0., 0.,  ..., 0., 0., 0.]]]], device='cuda:0',
       dtype=torch.bfloat16)
x.stride0: (873, 1, 873)
x.shape: torch.Size([1, 873, 3584])
x.stride1: (3128832, 3584, 1)
x.stride2: (3128832, 1, 3584)
conv_state1: (10752, 1, 3584)
conv_state2: (10752, 1, 3584)
conv_state3: (10752, 1, 3584)
conv_state: torch.Size([1, 3584, 3])
conv1d_out: torch.Size([1, 3584, 873])
rearrange(conv1d_out, 'b d l -> (b l) d') shape: torch.Size([873, 3584])
x_proj_weight shape: torch.Size([144, 3584])
x_dbl: torch.Size([873, 144])
delta_proj_weight @ x_dbl[:, :delta_rank].t(): torch.Size([3584, 873])
conv_state: tensor([[[nan, nan, nan],
         [nan, nan, nan],
         [nan, nan, nan],
         ...,
         [nan, nan, nan],
         [nan, nan, nan],
         [nan, nan, nan]]], device='cuda:0', dtype=torch.bfloat16,
       grad_fn=<AsStridedBackward0>)
ssm_state: tensor([[[[nan, nan, nan,  ..., nan, nan, nan]],

         [[nan, nan, nan,  ..., nan, nan, nan]],

         [[nan, nan, nan,  ..., nan, nan, nan]],

         ...,

         [[nan, nan, nan,  ..., nan, nan, nan]],

         [[nan, nan, nan,  ..., nan, nan, nan]],

         [[nan, nan, nan,  ..., nan, nan, nan]]]], device='cuda:0',
       dtype=torch.bfloat16)
here here
no back here:
not here, not here
conv_state: tensor([[[0., 0., 0.],
         [0., 0., 0.],
         [0., 0., 0.],
         ...,
         [0., 0., 0.],
         [0., 0., 0.],
         [0., 0., 0.]]], device='cuda:0', dtype=torch.bfloat16)
ssm_state: tensor([[[[0., 0., 0.,  ..., 0., 0., 0.]],

         [[0., 0., 0.,  ..., 0., 0., 0.]],

         [[0., 0., 0.,  ..., 0., 0., 0.]],

         ...,

         [[0., 0., 0.,  ..., 0., 0., 0.]],

         [[0., 0., 0.,  ..., 0., 0., 0.]],

         [[0., 0., 0.,  ..., 0., 0., 0.]]]], device='cuda:0',
       dtype=torch.bfloat16)
x.stride0: (873, 1, 873)
x.shape: torch.Size([1, 873, 3584])
x.stride1: (3128832, 3584, 1)
x.stride2: (3128832, 1, 3584)
conv_state1: (10752, 1, 3584)
conv_state2: (10752, 1, 3584)
conv_state3: (10752, 1, 3584)
conv_state: torch.Size([1, 3584, 3])
conv1d_out: torch.Size([1, 3584, 873])
rearrange(conv1d_out, 'b d l -> (b l) d') shape: torch.Size([873, 3584])
x_proj_weight shape: torch.Size([144, 3584])
x_dbl: torch.Size([873, 144])
delta_proj_weight @ x_dbl[:, :delta_rank].t(): torch.Size([3584, 873])
conv_state: tensor([[[nan, nan, nan],
         [nan, nan, nan],
         [nan, nan, nan],
         ...,
         [nan, nan, nan],
         [nan, nan, nan],
         [nan, nan, nan]]], device='cuda:0', dtype=torch.bfloat16,
       grad_fn=<AsStridedBackward0>)
ssm_state: tensor([[[[nan, nan, nan,  ..., nan, nan, nan]],

         [[nan, nan, nan,  ..., nan, nan, nan]],

         [[nan, nan, nan,  ..., nan, nan, nan]],

         ...,

         [[nan, nan, nan,  ..., nan, nan, nan]],

         [[nan, nan, nan,  ..., nan, nan, nan]],

         [[nan, nan, nan,  ..., nan, nan, nan]]]], device='cuda:0',
       dtype=torch.bfloat16)
here here
no back here:
not here, not here
conv_state: tensor([[[0., 0., 0.],
         [0., 0., 0.],
         [0., 0., 0.],
         ...,
         [0., 0., 0.],
         [0., 0., 0.],
         [0., 0., 0.]]], device='cuda:0', dtype=torch.bfloat16)
ssm_state: tensor([[[[0., 0., 0.,  ..., 0., 0., 0.]],

         [[0., 0., 0.,  ..., 0., 0., 0.]],

         [[0., 0., 0.,  ..., 0., 0., 0.]],

         ...,

         [[0., 0., 0.,  ..., 0., 0., 0.]],

         [[0., 0., 0.,  ..., 0., 0., 0.]],

         [[0., 0., 0.,  ..., 0., 0., 0.]]]], device='cuda:0',
       dtype=torch.bfloat16)
x.stride0: (873, 1, 873)
x.shape: torch.Size([1, 873, 3584])
x.stride1: (3128832, 3584, 1)
x.stride2: (3128832, 1, 3584)
conv_state1: (10752, 1, 3584)
conv_state2: (10752, 1, 3584)
conv_state3: (10752, 1, 3584)
conv_state: torch.Size([1, 3584, 3])
conv1d_out: torch.Size([1, 3584, 873])
rearrange(conv1d_out, 'b d l -> (b l) d') shape: torch.Size([873, 3584])
x_proj_weight shape: torch.Size([144, 3584])
x_dbl: torch.Size([873, 144])
delta_proj_weight @ x_dbl[:, :delta_rank].t(): torch.Size([3584, 873])
conv_state: tensor([[[nan, nan, nan],
         [nan, nan, nan],
         [nan, nan, nan],
         ...,
         [nan, nan, nan],
         [nan, nan, nan],
         [nan, nan, nan]]], device='cuda:0', dtype=torch.bfloat16,
       grad_fn=<AsStridedBackward0>)
ssm_state: tensor([[[[nan, nan, nan,  ..., nan, nan, nan]],

         [[nan, nan, nan,  ..., nan, nan, nan]],

         [[nan, nan, nan,  ..., nan, nan, nan]],

         ...,

         [[nan, nan, nan,  ..., nan, nan, nan]],

         [[nan, nan, nan,  ..., nan, nan, nan]],

         [[nan, nan, nan,  ..., nan, nan, nan]]]], device='cuda:0',
       dtype=torch.bfloat16)
here here
no back here:
not here, not here
conv_state: tensor([[[0., 0., 0.],
         [0., 0., 0.],
         [0., 0., 0.],
         ...,
         [0., 0., 0.],
         [0., 0., 0.],
         [0., 0., 0.]]], device='cuda:0', dtype=torch.bfloat16)
ssm_state: tensor([[[[0., 0., 0.,  ..., 0., 0., 0.]],

         [[0., 0., 0.,  ..., 0., 0., 0.]],

         [[0., 0., 0.,  ..., 0., 0., 0.]],

         ...,

         [[0., 0., 0.,  ..., 0., 0., 0.]],

         [[0., 0., 0.,  ..., 0., 0., 0.]],

         [[0., 0., 0.,  ..., 0., 0., 0.]]]], device='cuda:0',
       dtype=torch.bfloat16)
x.stride0: (873, 1, 873)
x.shape: torch.Size([1, 873, 3584])
x.stride1: (3128832, 3584, 1)
x.stride2: (3128832, 1, 3584)
conv_state1: (10752, 1, 3584)
conv_state2: (10752, 1, 3584)
conv_state3: (10752, 1, 3584)
conv_state: torch.Size([1, 3584, 3])
conv1d_out: torch.Size([1, 3584, 873])
rearrange(conv1d_out, 'b d l -> (b l) d') shape: torch.Size([873, 3584])
x_proj_weight shape: torch.Size([144, 3584])
x_dbl: torch.Size([873, 144])
delta_proj_weight @ x_dbl[:, :delta_rank].t(): torch.Size([3584, 873])
conv_state: tensor([[[nan, nan, nan],
         [nan, nan, nan],
         [nan, nan, nan],
         ...,
         [nan, nan, nan],
         [nan, nan, nan],
         [nan, nan, nan]]], device='cuda:0', dtype=torch.bfloat16,
       grad_fn=<AsStridedBackward0>)
ssm_state: tensor([[[[nan, nan, nan,  ..., nan, nan, nan]],

         [[nan, nan, nan,  ..., nan, nan, nan]],

         [[nan, nan, nan,  ..., nan, nan, nan]],

         ...,

         [[nan, nan, nan,  ..., nan, nan, nan]],

         [[nan, nan, nan,  ..., nan, nan, nan]],

         [[nan, nan, nan,  ..., nan, nan, nan]]]], device='cuda:0',
       dtype=torch.bfloat16)
here here
no back here:
not here, not here
conv_state: tensor([[[0., 0., 0.],
         [0., 0., 0.],
         [0., 0., 0.],
         ...,
         [0., 0., 0.],
         [0., 0., 0.],
         [0., 0., 0.]]], device='cuda:0', dtype=torch.bfloat16)
ssm_state: tensor([[[[0., 0., 0.,  ..., 0., 0., 0.]],

         [[0., 0., 0.,  ..., 0., 0., 0.]],

         [[0., 0., 0.,  ..., 0., 0., 0.]],

         ...,

         [[0., 0., 0.,  ..., 0., 0., 0.]],

         [[0., 0., 0.,  ..., 0., 0., 0.]],

         [[0., 0., 0.,  ..., 0., 0., 0.]]]], device='cuda:0',
       dtype=torch.bfloat16)
x.stride0: (873, 1, 873)
x.shape: torch.Size([1, 873, 3584])
x.stride1: (3128832, 3584, 1)
x.stride2: (3128832, 1, 3584)
conv_state1: (10752, 1, 3584)
conv_state2: (10752, 1, 3584)
conv_state3: (10752, 1, 3584)
conv_state: torch.Size([1, 3584, 3])
conv1d_out: torch.Size([1, 3584, 873])
rearrange(conv1d_out, 'b d l -> (b l) d') shape: torch.Size([873, 3584])
x_proj_weight shape: torch.Size([144, 3584])
x_dbl: torch.Size([873, 144])
delta_proj_weight @ x_dbl[:, :delta_rank].t(): torch.Size([3584, 873])
conv_state: tensor([[[nan, nan, nan],
         [nan, nan, nan],
         [nan, nan, nan],
         ...,
         [nan, nan, nan],
         [nan, nan, nan],
         [nan, nan, nan]]], device='cuda:0', dtype=torch.bfloat16,
       grad_fn=<AsStridedBackward0>)
ssm_state: tensor([[[[nan, nan, nan,  ..., nan, nan, nan]],

         [[nan, nan, nan,  ..., nan, nan, nan]],

         [[nan, nan, nan,  ..., nan, nan, nan]],

         ...,

         [[nan, nan, nan,  ..., nan, nan, nan]],

         [[nan, nan, nan,  ..., nan, nan, nan]],

         [[nan, nan, nan,  ..., nan, nan, nan]]]], device='cuda:0',
       dtype=torch.bfloat16)
here here
no back here:
not here, not here
conv_state: tensor([[[0., 0., 0.],
         [0., 0., 0.],
         [0., 0., 0.],
         ...,
         [0., 0., 0.],
         [0., 0., 0.],
         [0., 0., 0.]]], device='cuda:0', dtype=torch.bfloat16)
ssm_state: tensor([[[[0., 0., 0.,  ..., 0., 0., 0.]],

         [[0., 0., 0.,  ..., 0., 0., 0.]],

         [[0., 0., 0.,  ..., 0., 0., 0.]],

         ...,

         [[0., 0., 0.,  ..., 0., 0., 0.]],

         [[0., 0., 0.,  ..., 0., 0., 0.]],

         [[0., 0., 0.,  ..., 0., 0., 0.]]]], device='cuda:0',
       dtype=torch.bfloat16)
x.stride0: (873, 1, 873)
x.shape: torch.Size([1, 873, 3584])
x.stride1: (3128832, 3584, 1)
x.stride2: (3128832, 1, 3584)
conv_state1: (10752, 1, 3584)
conv_state2: (10752, 1, 3584)
conv_state3: (10752, 1, 3584)
conv_state: torch.Size([1, 3584, 3])
conv1d_out: torch.Size([1, 3584, 873])
rearrange(conv1d_out, 'b d l -> (b l) d') shape: torch.Size([873, 3584])
x_proj_weight shape: torch.Size([144, 3584])
x_dbl: torch.Size([873, 144])
delta_proj_weight @ x_dbl[:, :delta_rank].t(): torch.Size([3584, 873])
conv_state: tensor([[[nan, nan, nan],
         [nan, nan, nan],
         [nan, nan, nan],
         ...,
         [nan, nan, nan],
         [nan, nan, nan],
         [nan, nan, nan]]], device='cuda:0', dtype=torch.bfloat16,
       grad_fn=<AsStridedBackward0>)
ssm_state: tensor([[[[nan, nan, nan,  ..., nan, nan, nan]],

         [[nan, nan, nan,  ..., nan, nan, nan]],

         [[nan, nan, nan,  ..., nan, nan, nan]],

         ...,

         [[nan, nan, nan,  ..., nan, nan, nan]],

         [[nan, nan, nan,  ..., nan, nan, nan]],

         [[nan, nan, nan,  ..., nan, nan, nan]]]], device='cuda:0',
       dtype=torch.bfloat16)
here here
no back here:
not here, not here
conv_state: tensor([[[0., 0., 0.],
         [0., 0., 0.],
         [0., 0., 0.],
         ...,
         [0., 0., 0.],
         [0., 0., 0.],
         [0., 0., 0.]]], device='cuda:0', dtype=torch.bfloat16)
ssm_state: tensor([[[[0., 0., 0.,  ..., 0., 0., 0.]],

         [[0., 0., 0.,  ..., 0., 0., 0.]],

         [[0., 0., 0.,  ..., 0., 0., 0.]],

         ...,

         [[0., 0., 0.,  ..., 0., 0., 0.]],

         [[0., 0., 0.,  ..., 0., 0., 0.]],

         [[0., 0., 0.,  ..., 0., 0., 0.]]]], device='cuda:0',
       dtype=torch.bfloat16)
x.stride0: (873, 1, 873)
x.shape: torch.Size([1, 873, 3584])
x.stride1: (3128832, 3584, 1)
x.stride2: (3128832, 1, 3584)
conv_state1: (10752, 1, 3584)
conv_state2: (10752, 1, 3584)
conv_state3: (10752, 1, 3584)
conv_state: torch.Size([1, 3584, 3])
conv1d_out: torch.Size([1, 3584, 873])
rearrange(conv1d_out, 'b d l -> (b l) d') shape: torch.Size([873, 3584])
x_proj_weight shape: torch.Size([144, 3584])
x_dbl: torch.Size([873, 144])
delta_proj_weight @ x_dbl[:, :delta_rank].t(): torch.Size([3584, 873])
conv_state: tensor([[[nan, nan, nan],
         [nan, nan, nan],
         [nan, nan, nan],
         ...,
         [nan, nan, nan],
         [nan, nan, nan],
         [nan, nan, nan]]], device='cuda:0', dtype=torch.bfloat16,
       grad_fn=<AsStridedBackward0>)
ssm_state: tensor([[[[nan, nan, nan,  ..., nan, nan, nan]],

         [[nan, nan, nan,  ..., nan, nan, nan]],

         [[nan, nan, nan,  ..., nan, nan, nan]],

         ...,

         [[nan, nan, nan,  ..., nan, nan, nan]],

         [[nan, nan, nan,  ..., nan, nan, nan]],

         [[nan, nan, nan,  ..., nan, nan, nan]]]], device='cuda:0',
       dtype=torch.bfloat16)
here here
no back here:
not here, not here
conv_state: tensor([[[0., 0., 0.],
         [0., 0., 0.],
         [0., 0., 0.],
         ...,
         [0., 0., 0.],
         [0., 0., 0.],
         [0., 0., 0.]]], device='cuda:0', dtype=torch.bfloat16)
ssm_state: tensor([[[[0., 0., 0.,  ..., 0., 0., 0.]],

         [[0., 0., 0.,  ..., 0., 0., 0.]],

         [[0., 0., 0.,  ..., 0., 0., 0.]],

         ...,

         [[0., 0., 0.,  ..., 0., 0., 0.]],

         [[0., 0., 0.,  ..., 0., 0., 0.]],

         [[0., 0., 0.,  ..., 0., 0., 0.]]]], device='cuda:0',
       dtype=torch.bfloat16)
x.stride0: (873, 1, 873)
x.shape: torch.Size([1, 873, 3584])
x.stride1: (3128832, 3584, 1)
x.stride2: (3128832, 1, 3584)
conv_state1: (10752, 1, 3584)
conv_state2: (10752, 1, 3584)
conv_state3: (10752, 1, 3584)
conv_state: torch.Size([1, 3584, 3])
conv1d_out: torch.Size([1, 3584, 873])
rearrange(conv1d_out, 'b d l -> (b l) d') shape: torch.Size([873, 3584])
x_proj_weight shape: torch.Size([144, 3584])
x_dbl: torch.Size([873, 144])
delta_proj_weight @ x_dbl[:, :delta_rank].t(): torch.Size([3584, 873])
conv_state: tensor([[[nan, nan, nan],
         [nan, nan, nan],
         [nan, nan, nan],
         ...,
         [nan, nan, nan],
         [nan, nan, nan],
         [nan, nan, nan]]], device='cuda:0', dtype=torch.bfloat16,
       grad_fn=<AsStridedBackward0>)
ssm_state: tensor([[[[nan, nan, nan,  ..., nan, nan, nan]],

         [[nan, nan, nan,  ..., nan, nan, nan]],

         [[nan, nan, nan,  ..., nan, nan, nan]],

         ...,

         [[nan, nan, nan,  ..., nan, nan, nan]],

         [[nan, nan, nan,  ..., nan, nan, nan]],

         [[nan, nan, nan,  ..., nan, nan, nan]]]], device='cuda:0',
       dtype=torch.bfloat16)
here here
no back here:
not here, not here
conv_state: tensor([[[0., 0., 0.],
         [0., 0., 0.],
         [0., 0., 0.],
         ...,
         [0., 0., 0.],
         [0., 0., 0.],
         [0., 0., 0.]]], device='cuda:0', dtype=torch.bfloat16)
ssm_state: tensor([[[[0., 0., 0.,  ..., 0., 0., 0.]],

         [[0., 0., 0.,  ..., 0., 0., 0.]],

         [[0., 0., 0.,  ..., 0., 0., 0.]],

         ...,

         [[0., 0., 0.,  ..., 0., 0., 0.]],

         [[0., 0., 0.,  ..., 0., 0., 0.]],

         [[0., 0., 0.,  ..., 0., 0., 0.]]]], device='cuda:0',
       dtype=torch.bfloat16)
x.stride0: (873, 1, 873)
x.shape: torch.Size([1, 873, 3584])
x.stride1: (3128832, 3584, 1)
x.stride2: (3128832, 1, 3584)
conv_state1: (10752, 1, 3584)
conv_state2: (10752, 1, 3584)
conv_state3: (10752, 1, 3584)
conv_state: torch.Size([1, 3584, 3])
conv1d_out: torch.Size([1, 3584, 873])
rearrange(conv1d_out, 'b d l -> (b l) d') shape: torch.Size([873, 3584])
x_proj_weight shape: torch.Size([144, 3584])
x_dbl: torch.Size([873, 144])
delta_proj_weight @ x_dbl[:, :delta_rank].t(): torch.Size([3584, 873])
conv_state: tensor([[[nan, nan, nan],
         [nan, nan, nan],
         [nan, nan, nan],
         ...,
         [nan, nan, nan],
         [nan, nan, nan],
         [nan, nan, nan]]], device='cuda:0', dtype=torch.bfloat16,
       grad_fn=<AsStridedBackward0>)
ssm_state: tensor([[[[nan, nan, nan,  ..., nan, nan, nan]],

         [[nan, nan, nan,  ..., nan, nan, nan]],

         [[nan, nan, nan,  ..., nan, nan, nan]],

         ...,

         [[nan, nan, nan,  ..., nan, nan, nan]],

         [[nan, nan, nan,  ..., nan, nan, nan]],

         [[nan, nan, nan,  ..., nan, nan, nan]]]], device='cuda:0',
       dtype=torch.bfloat16)
here here
no back here:
not here, not here
conv_state: tensor([[[0., 0., 0.],
         [0., 0., 0.],
         [0., 0., 0.],
         ...,
         [0., 0., 0.],
         [0., 0., 0.],
         [0., 0., 0.]]], device='cuda:0', dtype=torch.bfloat16)
ssm_state: tensor([[[[0., 0., 0.,  ..., 0., 0., 0.]],

         [[0., 0., 0.,  ..., 0., 0., 0.]],

         [[0., 0., 0.,  ..., 0., 0., 0.]],

         ...,

         [[0., 0., 0.,  ..., 0., 0., 0.]],

         [[0., 0., 0.,  ..., 0., 0., 0.]],

         [[0., 0., 0.,  ..., 0., 0., 0.]]]], device='cuda:0',
       dtype=torch.bfloat16)
x.stride0: (873, 1, 873)
x.shape: torch.Size([1, 873, 3584])
x.stride1: (3128832, 3584, 1)
x.stride2: (3128832, 1, 3584)
conv_state1: (10752, 1, 3584)
conv_state2: (10752, 1, 3584)
conv_state3: (10752, 1, 3584)
conv_state: torch.Size([1, 3584, 3])
conv1d_out: torch.Size([1, 3584, 873])
rearrange(conv1d_out, 'b d l -> (b l) d') shape: torch.Size([873, 3584])
x_proj_weight shape: torch.Size([144, 3584])
x_dbl: torch.Size([873, 144])
delta_proj_weight @ x_dbl[:, :delta_rank].t(): torch.Size([3584, 873])
conv_state: tensor([[[nan, nan, nan],
         [nan, nan, nan],
         [nan, nan, nan],
         ...,
         [nan, nan, nan],
         [nan, nan, nan],
         [nan, nan, nan]]], device='cuda:0', dtype=torch.bfloat16,
       grad_fn=<AsStridedBackward0>)
ssm_state: tensor([[[[nan, nan, nan,  ..., nan, nan, nan]],

         [[nan, nan, nan,  ..., nan, nan, nan]],

         [[nan, nan, nan,  ..., nan, nan, nan]],

         ...,

         [[nan, nan, nan,  ..., nan, nan, nan]],

         [[nan, nan, nan,  ..., nan, nan, nan]],

         [[nan, nan, nan,  ..., nan, nan, nan]]]], device='cuda:0',
       dtype=torch.bfloat16)
here here
no back here:
not here, not here
conv_state: tensor([[[0., 0., 0.],
         [0., 0., 0.],
         [0., 0., 0.],
         ...,
         [0., 0., 0.],
         [0., 0., 0.],
         [0., 0., 0.]]], device='cuda:0', dtype=torch.bfloat16)
ssm_state: tensor([[[[0., 0., 0.,  ..., 0., 0., 0.]],

         [[0., 0., 0.,  ..., 0., 0., 0.]],

         [[0., 0., 0.,  ..., 0., 0., 0.]],

         ...,

         [[0., 0., 0.,  ..., 0., 0., 0.]],

         [[0., 0., 0.,  ..., 0., 0., 0.]],

         [[0., 0., 0.,  ..., 0., 0., 0.]]]], device='cuda:0',
       dtype=torch.bfloat16)
x.stride0: (873, 1, 873)
x.shape: torch.Size([1, 873, 3584])
x.stride1: (3128832, 3584, 1)
x.stride2: (3128832, 1, 3584)
conv_state1: (10752, 1, 3584)
conv_state2: (10752, 1, 3584)
conv_state3: (10752, 1, 3584)
conv_state: torch.Size([1, 3584, 3])
conv1d_out: torch.Size([1, 3584, 873])
rearrange(conv1d_out, 'b d l -> (b l) d') shape: torch.Size([873, 3584])
x_proj_weight shape: torch.Size([144, 3584])
x_dbl: torch.Size([873, 144])
delta_proj_weight @ x_dbl[:, :delta_rank].t(): torch.Size([3584, 873])
conv_state: tensor([[[nan, nan, nan],
         [nan, nan, nan],
         [nan, nan, nan],
         ...,
         [nan, nan, nan],
         [nan, nan, nan],
         [nan, nan, nan]]], device='cuda:0', dtype=torch.bfloat16,
       grad_fn=<AsStridedBackward0>)
ssm_state: tensor([[[[nan, nan, nan,  ..., nan, nan, nan]],

         [[nan, nan, nan,  ..., nan, nan, nan]],

         [[nan, nan, nan,  ..., nan, nan, nan]],

         ...,

         [[nan, nan, nan,  ..., nan, nan, nan]],

         [[nan, nan, nan,  ..., nan, nan, nan]],

         [[nan, nan, nan,  ..., nan, nan, nan]]]], device='cuda:0',
       dtype=torch.bfloat16)
here here
no back here:
not here, not here
conv_state: tensor([[[0., 0., 0.],
         [0., 0., 0.],
         [0., 0., 0.],
         ...,
         [0., 0., 0.],
         [0., 0., 0.],
         [0., 0., 0.]]], device='cuda:0', dtype=torch.bfloat16)
ssm_state: tensor([[[[0., 0., 0.,  ..., 0., 0., 0.]],

         [[0., 0., 0.,  ..., 0., 0., 0.]],

         [[0., 0., 0.,  ..., 0., 0., 0.]],

         ...,

         [[0., 0., 0.,  ..., 0., 0., 0.]],

         [[0., 0., 0.,  ..., 0., 0., 0.]],

         [[0., 0., 0.,  ..., 0., 0., 0.]]]], device='cuda:0',
       dtype=torch.bfloat16)
x.stride0: (873, 1, 873)
x.shape: torch.Size([1, 873, 3584])
x.stride1: (3128832, 3584, 1)
x.stride2: (3128832, 1, 3584)
conv_state1: (10752, 1, 3584)
conv_state2: (10752, 1, 3584)
conv_state3: (10752, 1, 3584)
conv_state: torch.Size([1, 3584, 3])
conv1d_out: torch.Size([1, 3584, 873])
rearrange(conv1d_out, 'b d l -> (b l) d') shape: torch.Size([873, 3584])
x_proj_weight shape: torch.Size([144, 3584])
x_dbl: torch.Size([873, 144])
delta_proj_weight @ x_dbl[:, :delta_rank].t(): torch.Size([3584, 873])
conv_state: tensor([[[nan, nan, nan],
         [nan, nan, nan],
         [nan, nan, nan],
         ...,
         [nan, nan, nan],
         [nan, nan, nan],
         [nan, nan, nan]]], device='cuda:0', dtype=torch.bfloat16,
       grad_fn=<AsStridedBackward0>)
ssm_state: tensor([[[[nan, nan, nan,  ..., nan, nan, nan]],

         [[nan, nan, nan,  ..., nan, nan, nan]],

         [[nan, nan, nan,  ..., nan, nan, nan]],

         ...,

         [[nan, nan, nan,  ..., nan, nan, nan]],

         [[nan, nan, nan,  ..., nan, nan, nan]],

         [[nan, nan, nan,  ..., nan, nan, nan]]]], device='cuda:0',
       dtype=torch.bfloat16)
here here
no back here:
not here, not here
conv_state: tensor([[[0., 0., 0.],
         [0., 0., 0.],
         [0., 0., 0.],
         ...,
         [0., 0., 0.],
         [0., 0., 0.],
         [0., 0., 0.]]], device='cuda:0', dtype=torch.bfloat16)
ssm_state: tensor([[[[0., 0., 0.,  ..., 0., 0., 0.]],

         [[0., 0., 0.,  ..., 0., 0., 0.]],

         [[0., 0., 0.,  ..., 0., 0., 0.]],

         ...,

         [[0., 0., 0.,  ..., 0., 0., 0.]],

         [[0., 0., 0.,  ..., 0., 0., 0.]],

         [[0., 0., 0.,  ..., 0., 0., 0.]]]], device='cuda:0',
       dtype=torch.bfloat16)
x.stride0: (873, 1, 873)
x.shape: torch.Size([1, 873, 3584])
x.stride1: (3128832, 3584, 1)
x.stride2: (3128832, 1, 3584)
conv_state1: (10752, 1, 3584)
conv_state2: (10752, 1, 3584)
conv_state3: (10752, 1, 3584)
conv_state: torch.Size([1, 3584, 3])
conv1d_out: torch.Size([1, 3584, 873])
rearrange(conv1d_out, 'b d l -> (b l) d') shape: torch.Size([873, 3584])
x_proj_weight shape: torch.Size([144, 3584])
x_dbl: torch.Size([873, 144])
delta_proj_weight @ x_dbl[:, :delta_rank].t(): torch.Size([3584, 873])
conv_state: tensor([[[nan, nan, nan],
         [nan, nan, nan],
         [nan, nan, nan],
         ...,
         [nan, nan, nan],
         [nan, nan, nan],
         [nan, nan, nan]]], device='cuda:0', dtype=torch.bfloat16,
       grad_fn=<AsStridedBackward0>)
ssm_state: tensor([[[[nan, nan, nan,  ..., nan, nan, nan]],

         [[nan, nan, nan,  ..., nan, nan, nan]],

         [[nan, nan, nan,  ..., nan, nan, nan]],

         ...,

         [[nan, nan, nan,  ..., nan, nan, nan]],

         [[nan, nan, nan,  ..., nan, nan, nan]],

         [[nan, nan, nan,  ..., nan, nan, nan]]]], device='cuda:0',
       dtype=torch.bfloat16)
here here
no back here:
not here, not here
conv_state: tensor([[[0., 0., 0.],
         [0., 0., 0.],
         [0., 0., 0.],
         ...,
         [0., 0., 0.],
         [0., 0., 0.],
         [0., 0., 0.]]], device='cuda:0', dtype=torch.bfloat16)
ssm_state: tensor([[[[0., 0., 0.,  ..., 0., 0., 0.]],

         [[0., 0., 0.,  ..., 0., 0., 0.]],

         [[0., 0., 0.,  ..., 0., 0., 0.]],

         ...,

         [[0., 0., 0.,  ..., 0., 0., 0.]],

         [[0., 0., 0.,  ..., 0., 0., 0.]],

         [[0., 0., 0.,  ..., 0., 0., 0.]]]], device='cuda:0',
       dtype=torch.bfloat16)
x.stride0: (873, 1, 873)
x.shape: torch.Size([1, 873, 3584])
x.stride1: (3128832, 3584, 1)
x.stride2: (3128832, 1, 3584)
conv_state1: (10752, 1, 3584)
conv_state2: (10752, 1, 3584)
conv_state3: (10752, 1, 3584)
conv_state: torch.Size([1, 3584, 3])
conv1d_out: torch.Size([1, 3584, 873])
rearrange(conv1d_out, 'b d l -> (b l) d') shape: torch.Size([873, 3584])
x_proj_weight shape: torch.Size([144, 3584])
x_dbl: torch.Size([873, 144])
delta_proj_weight @ x_dbl[:, :delta_rank].t(): torch.Size([3584, 873])
conv_state: tensor([[[nan, nan, nan],
         [nan, nan, nan],
         [nan, nan, nan],
         ...,
         [nan, nan, nan],
         [nan, nan, nan],
         [nan, nan, nan]]], device='cuda:0', dtype=torch.bfloat16,
       grad_fn=<AsStridedBackward0>)
ssm_state: tensor([[[[nan, nan, nan,  ..., nan, nan, nan]],

         [[nan, nan, nan,  ..., nan, nan, nan]],

         [[nan, nan, nan,  ..., nan, nan, nan]],

         ...,

         [[nan, nan, nan,  ..., nan, nan, nan]],

         [[nan, nan, nan,  ..., nan, nan, nan]],

         [[nan, nan, nan,  ..., nan, nan, nan]]]], device='cuda:0',
       dtype=torch.bfloat16)
here here
no back here:
not here, not here
conv_state: tensor([[[0., 0., 0.],
         [0., 0., 0.],
         [0., 0., 0.],
         ...,
         [0., 0., 0.],
         [0., 0., 0.],
         [0., 0., 0.]]], device='cuda:0', dtype=torch.bfloat16)
ssm_state: tensor([[[[0., 0., 0.,  ..., 0., 0., 0.]],

         [[0., 0., 0.,  ..., 0., 0., 0.]],

         [[0., 0., 0.,  ..., 0., 0., 0.]],

         ...,

         [[0., 0., 0.,  ..., 0., 0., 0.]],

         [[0., 0., 0.,  ..., 0., 0., 0.]],

         [[0., 0., 0.,  ..., 0., 0., 0.]]]], device='cuda:0',
       dtype=torch.bfloat16)
x.stride0: (873, 1, 873)
x.shape: torch.Size([1, 873, 3584])
x.stride1: (3128832, 3584, 1)
x.stride2: (3128832, 1, 3584)
conv_state1: (10752, 1, 3584)
conv_state2: (10752, 1, 3584)
conv_state3: (10752, 1, 3584)
conv_state: torch.Size([1, 3584, 3])
conv1d_out: torch.Size([1, 3584, 873])
rearrange(conv1d_out, 'b d l -> (b l) d') shape: torch.Size([873, 3584])
x_proj_weight shape: torch.Size([144, 3584])
x_dbl: torch.Size([873, 144])
delta_proj_weight @ x_dbl[:, :delta_rank].t(): torch.Size([3584, 873])
conv_state: tensor([[[nan, nan, nan],
         [nan, nan, nan],
         [nan, nan, nan],
         ...,
         [nan, nan, nan],
         [nan, nan, nan],
         [nan, nan, nan]]], device='cuda:0', dtype=torch.bfloat16,
       grad_fn=<AsStridedBackward0>)
ssm_state: tensor([[[[nan, nan, nan,  ..., nan, nan, nan]],

         [[nan, nan, nan,  ..., nan, nan, nan]],

         [[nan, nan, nan,  ..., nan, nan, nan]],

         ...,

         [[nan, nan, nan,  ..., nan, nan, nan]],

         [[nan, nan, nan,  ..., nan, nan, nan]],

         [[nan, nan, nan,  ..., nan, nan, nan]]]], device='cuda:0',
       dtype=torch.bfloat16)
here here
no back here:
not here, not here
conv_state: tensor([[[0., 0., 0.],
         [0., 0., 0.],
         [0., 0., 0.],
         ...,
         [0., 0., 0.],
         [0., 0., 0.],
         [0., 0., 0.]]], device='cuda:0', dtype=torch.bfloat16)
ssm_state: tensor([[[[0., 0., 0.,  ..., 0., 0., 0.]],

         [[0., 0., 0.,  ..., 0., 0., 0.]],

         [[0., 0., 0.,  ..., 0., 0., 0.]],

         ...,

         [[0., 0., 0.,  ..., 0., 0., 0.]],

         [[0., 0., 0.,  ..., 0., 0., 0.]],

         [[0., 0., 0.,  ..., 0., 0., 0.]]]], device='cuda:0',
       dtype=torch.bfloat16)
x.stride0: (873, 1, 873)
x.shape: torch.Size([1, 873, 3584])
x.stride1: (3128832, 3584, 1)
x.stride2: (3128832, 1, 3584)
conv_state1: (10752, 1, 3584)
conv_state2: (10752, 1, 3584)
conv_state3: (10752, 1, 3584)
conv_state: torch.Size([1, 3584, 3])
conv1d_out: torch.Size([1, 3584, 873])
rearrange(conv1d_out, 'b d l -> (b l) d') shape: torch.Size([873, 3584])
x_proj_weight shape: torch.Size([144, 3584])
x_dbl: torch.Size([873, 144])
delta_proj_weight @ x_dbl[:, :delta_rank].t(): torch.Size([3584, 873])
conv_state: tensor([[[nan, nan, nan],
         [nan, nan, nan],
         [nan, nan, nan],
         ...,
         [nan, nan, nan],
         [nan, nan, nan],
         [nan, nan, nan]]], device='cuda:0', dtype=torch.bfloat16,
       grad_fn=<AsStridedBackward0>)
ssm_state: tensor([[[[nan, nan, nan,  ..., nan, nan, nan]],

         [[nan, nan, nan,  ..., nan, nan, nan]],

         [[nan, nan, nan,  ..., nan, nan, nan]],

         ...,

         [[nan, nan, nan,  ..., nan, nan, nan]],

         [[nan, nan, nan,  ..., nan, nan, nan]],

         [[nan, nan, nan,  ..., nan, nan, nan]]]], device='cuda:0',
       dtype=torch.bfloat16)
here here
no back here:
not here, not here
conv_state: tensor([[[0., 0., 0.],
         [0., 0., 0.],
         [0., 0., 0.],
         ...,
         [0., 0., 0.],
         [0., 0., 0.],
         [0., 0., 0.]]], device='cuda:0', dtype=torch.bfloat16)
ssm_state: tensor([[[[0., 0., 0.,  ..., 0., 0., 0.]],

         [[0., 0., 0.,  ..., 0., 0., 0.]],

         [[0., 0., 0.,  ..., 0., 0., 0.]],

         ...,

         [[0., 0., 0.,  ..., 0., 0., 0.]],

         [[0., 0., 0.,  ..., 0., 0., 0.]],

         [[0., 0., 0.,  ..., 0., 0., 0.]]]], device='cuda:0',
       dtype=torch.bfloat16)
x.stride0: (873, 1, 873)
x.shape: torch.Size([1, 873, 3584])
x.stride1: (3128832, 3584, 1)
x.stride2: (3128832, 1, 3584)
conv_state1: (10752, 1, 3584)
conv_state2: (10752, 1, 3584)
conv_state3: (10752, 1, 3584)
conv_state: torch.Size([1, 3584, 3])
conv1d_out: torch.Size([1, 3584, 873])
rearrange(conv1d_out, 'b d l -> (b l) d') shape: torch.Size([873, 3584])
x_proj_weight shape: torch.Size([144, 3584])
x_dbl: torch.Size([873, 144])
delta_proj_weight @ x_dbl[:, :delta_rank].t(): torch.Size([3584, 873])
conv_state: tensor([[[nan, nan, nan],
         [nan, nan, nan],
         [nan, nan, nan],
         ...,
         [nan, nan, nan],
         [nan, nan, nan],
         [nan, nan, nan]]], device='cuda:0', dtype=torch.bfloat16,
       grad_fn=<AsStridedBackward0>)
ssm_state: tensor([[[[nan, nan, nan,  ..., nan, nan, nan]],

         [[nan, nan, nan,  ..., nan, nan, nan]],

         [[nan, nan, nan,  ..., nan, nan, nan]],

         ...,

         [[nan, nan, nan,  ..., nan, nan, nan]],

         [[nan, nan, nan,  ..., nan, nan, nan]],

         [[nan, nan, nan,  ..., nan, nan, nan]]]], device='cuda:0',
       dtype=torch.bfloat16)
here here
no back here:
not here, not here
conv_state: tensor([[[0., 0., 0.],
         [0., 0., 0.],
         [0., 0., 0.],
         ...,
         [0., 0., 0.],
         [0., 0., 0.],
         [0., 0., 0.]]], device='cuda:0', dtype=torch.bfloat16)
ssm_state: tensor([[[[0., 0., 0.,  ..., 0., 0., 0.]],

         [[0., 0., 0.,  ..., 0., 0., 0.]],

         [[0., 0., 0.,  ..., 0., 0., 0.]],

         ...,

         [[0., 0., 0.,  ..., 0., 0., 0.]],

         [[0., 0., 0.,  ..., 0., 0., 0.]],

         [[0., 0., 0.,  ..., 0., 0., 0.]]]], device='cuda:0',
       dtype=torch.bfloat16)
x.stride0: (873, 1, 873)
x.shape: torch.Size([1, 873, 3584])
x.stride1: (3128832, 3584, 1)
x.stride2: (3128832, 1, 3584)
conv_state1: (10752, 1, 3584)
conv_state2: (10752, 1, 3584)
conv_state3: (10752, 1, 3584)
conv_state: torch.Size([1, 3584, 3])
conv1d_out: torch.Size([1, 3584, 873])
rearrange(conv1d_out, 'b d l -> (b l) d') shape: torch.Size([873, 3584])
x_proj_weight shape: torch.Size([144, 3584])
x_dbl: torch.Size([873, 144])
delta_proj_weight @ x_dbl[:, :delta_rank].t(): torch.Size([3584, 873])
conv_state: tensor([[[nan, nan, nan],
         [nan, nan, nan],
         [nan, nan, nan],
         ...,
         [nan, nan, nan],
         [nan, nan, nan],
         [nan, nan, nan]]], device='cuda:0', dtype=torch.bfloat16,
       grad_fn=<AsStridedBackward0>)
ssm_state: tensor([[[[nan, nan, nan,  ..., nan, nan, nan]],

         [[nan, nan, nan,  ..., nan, nan, nan]],

         [[nan, nan, nan,  ..., nan, nan, nan]],

         ...,

         [[nan, nan, nan,  ..., nan, nan, nan]],

         [[nan, nan, nan,  ..., nan, nan, nan]],

         [[nan, nan, nan,  ..., nan, nan, nan]]]], device='cuda:0',
       dtype=torch.bfloat16)
here here
no back here:
not here, not here
conv_state: tensor([[[0., 0., 0.],
         [0., 0., 0.],
         [0., 0., 0.],
         ...,
         [0., 0., 0.],
         [0., 0., 0.],
         [0., 0., 0.]]], device='cuda:0', dtype=torch.bfloat16)
ssm_state: tensor([[[[0., 0., 0.,  ..., 0., 0., 0.]],

         [[0., 0., 0.,  ..., 0., 0., 0.]],

         [[0., 0., 0.,  ..., 0., 0., 0.]],

         ...,

         [[0., 0., 0.,  ..., 0., 0., 0.]],

         [[0., 0., 0.,  ..., 0., 0., 0.]],

         [[0., 0., 0.,  ..., 0., 0., 0.]]]], device='cuda:0',
       dtype=torch.bfloat16)
x.stride0: (873, 1, 873)
x.shape: torch.Size([1, 873, 3584])
x.stride1: (3128832, 3584, 1)
x.stride2: (3128832, 1, 3584)
conv_state1: (10752, 1, 3584)
conv_state2: (10752, 1, 3584)
conv_state3: (10752, 1, 3584)
conv_state: torch.Size([1, 3584, 3])
conv1d_out: torch.Size([1, 3584, 873])
rearrange(conv1d_out, 'b d l -> (b l) d') shape: torch.Size([873, 3584])
x_proj_weight shape: torch.Size([144, 3584])
x_dbl: torch.Size([873, 144])
delta_proj_weight @ x_dbl[:, :delta_rank].t(): torch.Size([3584, 873])
conv_state: tensor([[[nan, nan, nan],
         [nan, nan, nan],
         [nan, nan, nan],
         ...,
         [nan, nan, nan],
         [nan, nan, nan],
         [nan, nan, nan]]], device='cuda:0', dtype=torch.bfloat16,
       grad_fn=<AsStridedBackward0>)
ssm_state: tensor([[[[nan, nan, nan,  ..., nan, nan, nan]],

         [[nan, nan, nan,  ..., nan, nan, nan]],

         [[nan, nan, nan,  ..., nan, nan, nan]],

         ...,

         [[nan, nan, nan,  ..., nan, nan, nan]],

         [[nan, nan, nan,  ..., nan, nan, nan]],

         [[nan, nan, nan,  ..., nan, nan, nan]]]], device='cuda:0',
       dtype=torch.bfloat16)
here here
no back here:
not here, not here
conv_state: tensor([[[0., 0., 0.],
         [0., 0., 0.],
         [0., 0., 0.],
         ...,
         [0., 0., 0.],
         [0., 0., 0.],
         [0., 0., 0.]]], device='cuda:0', dtype=torch.bfloat16)
ssm_state: tensor([[[[0., 0., 0.,  ..., 0., 0., 0.]],

         [[0., 0., 0.,  ..., 0., 0., 0.]],

         [[0., 0., 0.,  ..., 0., 0., 0.]],

         ...,

         [[0., 0., 0.,  ..., 0., 0., 0.]],

         [[0., 0., 0.,  ..., 0., 0., 0.]],

         [[0., 0., 0.,  ..., 0., 0., 0.]]]], device='cuda:0',
       dtype=torch.bfloat16)
x.stride0: (873, 1, 873)
x.shape: torch.Size([1, 873, 3584])
x.stride1: (3128832, 3584, 1)
x.stride2: (3128832, 1, 3584)
conv_state1: (10752, 1, 3584)
conv_state2: (10752, 1, 3584)
conv_state3: (10752, 1, 3584)
conv_state: torch.Size([1, 3584, 3])
conv1d_out: torch.Size([1, 3584, 873])
rearrange(conv1d_out, 'b d l -> (b l) d') shape: torch.Size([873, 3584])
x_proj_weight shape: torch.Size([144, 3584])
x_dbl: torch.Size([873, 144])
delta_proj_weight @ x_dbl[:, :delta_rank].t(): torch.Size([3584, 873])
conv_state: tensor([[[nan, nan, nan],
         [nan, nan, nan],
         [nan, nan, nan],
         ...,
         [nan, nan, nan],
         [nan, nan, nan],
         [nan, nan, nan]]], device='cuda:0', dtype=torch.bfloat16,
       grad_fn=<AsStridedBackward0>)
ssm_state: tensor([[[[nan, nan, nan,  ..., nan, nan, nan]],

         [[nan, nan, nan,  ..., nan, nan, nan]],

         [[nan, nan, nan,  ..., nan, nan, nan]],

         ...,

         [[nan, nan, nan,  ..., nan, nan, nan]],

         [[nan, nan, nan,  ..., nan, nan, nan]],

         [[nan, nan, nan,  ..., nan, nan, nan]]]], device='cuda:0',
       dtype=torch.bfloat16)
here here
no back here:
not here, not here
conv_state: tensor([[[-14.0625, -14.5000, -14.0625],
         [ 12.5625,  12.8125,  12.5625],
         [  5.0938,   2.2500,   5.0938],
         ...,
         [-16.2500, -13.5625, -16.2500],
         [ 14.8125,  12.6250,  14.8125],
         [  7.5625,   9.5625,   7.5625]]], device='cuda:0',
       dtype=torch.bfloat16, grad_fn=<AsStridedBackward0>)
ssm_state: tensor([[[[ 0.0000e+00,  0.0000e+00,  1.0775e+14,  ...,  0.0000e+00,
            6.5021e+16,  6.1512e-05]],

         [[ 0.0000e+00,  0.0000e+00, -1.7280e+04,  ...,  0.0000e+00,
            1.9022e+14, -3.3188e-04]],

         [[ 0.0000e+00,  0.0000e+00,  6.3477e-02,  ...,  0.0000e+00,
           -3.5341e-28,  1.2338e-05]],

         ...,

         [[ 0.0000e+00,  0.0000e+00, -4.2600e+02,  ...,  0.0000e+00,
           -7.2142e+08, -3.7909e-05]],

         [[ 0.0000e+00,  0.0000e+00, -4.1633e-16,  ...,  0.0000e+00,
           -5.7440e+30,  1.7166e-05]],

         [[ 0.0000e+00,  0.0000e+00, -8.8471e-16,  ...,  0.0000e+00,
            3.4833e-15, -4.2439e-05]]]], device='cuda:0', dtype=torch.bfloat16)
x.stride0: (1, 1, 1)
x.shape: torch.Size([1, 1, 3584])
x.stride1: (7168, 3584, 1)
x.stride2: (7168, 1, 3584)
conv_state1: (10752, 1, 3584)
conv_state2: (10752, 1, 3584)
conv_state3: (10752, 1, 3584)
conv_state: torch.Size([1, 3584, 3])
conv1d_out: torch.Size([1, 3584, 1])
rearrange(conv1d_out, 'b d l -> (b l) d') shape: torch.Size([1, 3584])
x_proj_weight shape: torch.Size([144, 3584])
x_dbl: torch.Size([1, 144])
delta_proj_weight @ x_dbl[:, :delta_rank].t(): torch.Size([3584, 1])
conv_state: tensor([[[-14.5000, -14.0625, -16.3750],
         [ 12.8125,  12.5625,  10.5625],
         [  2.2500,   5.0938,   5.3125],
         ...,
         [-13.5625, -16.2500, -17.5000],
         [ 12.6250,  14.8125,  13.9375],
         [  9.5625,   7.5625,   8.8750]]], device='cuda:0',
       dtype=torch.bfloat16, grad_fn=<AsStridedBackward0>)
ssm_state: tensor([[[[ 0.0000e+00,  0.0000e+00,  3.8281e+16,  ...,  0.0000e+00,
            1.9185e-12,  6.1035e-05]],

         [[ 0.0000e+00,  0.0000e+00,  1.7005e+35,  ...,  0.0000e+00,
            8.1605e+30, -3.2997e-04]],

         [[ 0.0000e+00,  0.0000e+00,  5.7646e+19,  ...,  0.0000e+00,
           -3.1320e+29,  7.2122e-06]],

         ...,

         [[ 0.0000e+00,  0.0000e+00, -1.5572e-06,  ...,  0.0000e+00,
            1.6640e-31, -3.6240e-05]],

         [[ 0.0000e+00,  0.0000e+00,  4.0996e-26,  ...,  0.0000e+00,
           -5.7936e+29,  5.0068e-06]],

         [[ 0.0000e+00,  0.0000e+00, -4.4922e-01,  ...,  0.0000e+00,
            7.1320e-34, -3.5763e-05]]]], device='cuda:0', dtype=torch.bfloat16)
here here
no back here:
not here, not here
conv_state: tensor([[[-4.5312, -4.9688, -5.0938],
         [ 7.5000,  7.0312,  7.6250],
         [ 2.0000,  1.7969,  1.7031],
         ...,
         [-6.6250, -4.6250, -6.4062],
         [-3.9062, -5.6875, -4.5312],
         [ 2.4531, -1.6250,  1.6719]]], device='cuda:0', dtype=torch.bfloat16,
       grad_fn=<AsStridedBackward0>)
ssm_state: tensor([[[[ 0.0000e+00,  0.0000e+00, -9.1508e-28,  ...,  0.0000e+00,
           -1.8376e+27, -7.4387e-04]],

         [[ 0.0000e+00,  0.0000e+00,  3.5885e+19,  ...,  0.0000e+00,
           -3.0268e-08, -2.9144e-03]],

         [[ 0.0000e+00,  0.0000e+00, -2.3283e-09,  ...,  0.0000e+00,
            5.4974e-30, -9.3460e-04]],

         ...,

         [[ 0.0000e+00,  0.0000e+00,  1.2509e+20,  ...,  0.0000e+00,
            5.7386e-24, -2.4292e-02]],

         [[ 0.0000e+00,  0.0000e+00,  3.1139e+11,  ...,  0.0000e+00,
            4.5438e-26, -3.5667e-04]],

         [[ 0.0000e+00,  0.0000e+00,  7.1337e+18,  ...,  0.0000e+00,
            2.9711e+28, -6.1340e-03]]]], device='cuda:0', dtype=torch.bfloat16)
x.stride0: (1, 1, 1)
x.shape: torch.Size([1, 1, 3584])
x.stride1: (7168, 3584, 1)
x.stride2: (7168, 1, 3584)
conv_state1: (10752, 1, 3584)
conv_state2: (10752, 1, 3584)
conv_state3: (10752, 1, 3584)
conv_state: torch.Size([1, 3584, 3])
conv1d_out: torch.Size([1, 3584, 1])
rearrange(conv1d_out, 'b d l -> (b l) d') shape: torch.Size([1, 3584])
x_proj_weight shape: torch.Size([144, 3584])
x_dbl: torch.Size([1, 144])
delta_proj_weight @ x_dbl[:, :delta_rank].t(): torch.Size([3584, 1])
conv_state: tensor([[[-4.9688, -5.0938, -5.1875],
         [ 7.0312,  7.6250,  9.1875],
         [ 1.7969,  1.7031,  3.8750],
         ...,
         [-4.6250, -6.4062, -8.3125],
         [-5.6875, -4.5312, -7.1250],
         [-1.6250,  1.6719, -0.6172]]], device='cuda:0', dtype=torch.bfloat16,
       grad_fn=<AsStridedBackward0>)
ssm_state: tensor([[[[ 0.0000e+00,  0.0000e+00,  3.8335e+19,  ...,  0.0000e+00,
            1.7199e+34, -7.4768e-04]],

         [[ 0.0000e+00,  0.0000e+00, -5.6946e-30,  ...,  0.0000e+00,
            6.2864e-08, -2.7313e-03]],

         [[ 0.0000e+00,  0.0000e+00, -1.5573e-34,  ...,  0.0000e+00,
           -2.8711e-01, -7.4387e-04]],

         ...,

         [[ 0.0000e+00,  0.0000e+00, -1.1339e+12,  ...,  0.0000e+00,
            6.3477e-36, -2.3804e-02]],

         [[ 0.0000e+00,  0.0000e+00, -8.0559e-08,  ...,  0.0000e+00,
            6.8770e+31, -4.8065e-04]],

         [[ 0.0000e+00,  0.0000e+00,  4.0426e+27,  ...,  0.0000e+00,
            3.5219e+11, -5.7373e-03]]]], device='cuda:0', dtype=torch.bfloat16)
here here
no back here:
not here, not here
conv_state: tensor([[[-6.4688, -8.4375, -8.0000],
         [ 1.1484, -0.3125,  0.4902],
         [ 3.6562,  3.5781,  4.9688],
         ...,
         [-5.6562, -5.8125, -6.5625],
         [-5.2812, -5.6562, -5.3125],
         [-1.8281, -1.8203, -1.5859]]], device='cuda:0', dtype=torch.bfloat16,
       grad_fn=<AsStridedBackward0>)
ssm_state: tensor([[[[ 0.0000e+00,  0.0000e+00,  1.0138e-25,  ...,  0.0000e+00,
            1.1498e-22, -6.1691e-06]],

         [[ 0.0000e+00,  0.0000e+00, -1.1439e+34,  ...,  0.0000e+00,
            7.6093e+19,  5.0354e-03]],

         [[ 0.0000e+00,  0.0000e+00, -1.8239e-05,  ...,  0.0000e+00,
           -4.8392e+36, -1.6098e-03]],

         ...,

         [[ 0.0000e+00,  0.0000e+00,  2.1767e+21,  ...,  0.0000e+00,
            4.3368e-18,  1.2695e-02]],

         [[ 0.0000e+00,  0.0000e+00,  2.1094e-14,  ...,  0.0000e+00,
            9.6436e-03, -3.9062e-03]],

         [[ 0.0000e+00,  0.0000e+00,  5.9360e-18,  ...,  0.0000e+00,
            4.4375e+00,  3.1250e-02]]]], device='cuda:0', dtype=torch.bfloat16)
x.stride0: (1, 1, 1)
x.shape: torch.Size([1, 1, 3584])
x.stride1: (7168, 3584, 1)
x.stride2: (7168, 1, 3584)
conv_state1: (10752, 1, 3584)
conv_state2: (10752, 1, 3584)
conv_state3: (10752, 1, 3584)
conv_state: torch.Size([1, 3584, 3])
conv1d_out: torch.Size([1, 3584, 1])
rearrange(conv1d_out, 'b d l -> (b l) d') shape: torch.Size([1, 3584])
x_proj_weight shape: torch.Size([144, 3584])
x_dbl: torch.Size([1, 144])
delta_proj_weight @ x_dbl[:, :delta_rank].t(): torch.Size([3584, 1])
conv_state: tensor([[[-8.4375, -8.0000, -9.5625],
         [-0.3125,  0.4902,  2.3750],
         [ 3.5781,  4.9688,  5.1875],
         ...,
         [-5.8125, -6.5625, -7.4375],
         [-5.6562, -5.3125, -6.6875],
         [-1.8203, -1.5859, -2.6250]]], device='cuda:0', dtype=torch.bfloat16,
       grad_fn=<AsStridedBackward0>)
ssm_state: tensor([[[[ 0.0000e+00,  0.0000e+00, -5.0440e+18,  ...,  0.0000e+00,
            1.6434e+24,  5.0962e-06]],

         [[ 0.0000e+00,  0.0000e+00,  7.0867e+11,  ...,  0.0000e+00,
           -1.7355e-28,  4.9438e-03]],

         [[ 0.0000e+00,  0.0000e+00,  3.2496e+27,  ...,  0.0000e+00,
           -3.8168e+08, -1.5793e-03]],

         ...,

         [[ 0.0000e+00,  0.0000e+00, -2.6661e+18,  ...,  0.0000e+00,
            3.1748e-34,  1.2573e-02]],

         [[ 0.0000e+00,  0.0000e+00,  2.5344e+04,  ...,  0.0000e+00,
                   nan, -7.1716e-03]],

         [[ 0.0000e+00,  0.0000e+00, -7.9581e-12,  ...,  0.0000e+00,
            2.9843e-13,  4.8828e-04]]]], device='cuda:0', dtype=torch.bfloat16)
here here
no back here:
not here, not here
conv_state: tensor([[[-0.3457, -0.8320, -2.0000],
         [-4.5000, -3.0781, -3.9688],
         [ 0.8125,  1.4609,  0.0359],
         ...,
         [ 3.2656,  1.5781,  1.5156],
         [-1.9062, -2.1719, -2.0000],
         [ 1.9531,  1.9766,  2.4062]]], device='cuda:0', dtype=torch.bfloat16,
       grad_fn=<AsStridedBackward0>)
ssm_state: tensor([[[[ 0.0000e+00,  0.0000e+00,  5.0037e+20,  ...,  0.0000e+00,
            2.4442e+20, -4.4250e-03]],

         [[ 0.0000e+00,  0.0000e+00, -3.2370e+16,  ...,  0.0000e+00,
            1.3708e+37, -7.8125e-03]],

         [[ 0.0000e+00,  0.0000e+00, -5.1457e+14,  ...,  0.0000e+00,
            7.8583e+21,  1.2970e-03]],

         ...,

         [[ 0.0000e+00,  0.0000e+00,  5.8787e+10,  ...,  0.0000e+00,
            3.8814e-17, -1.2283e-03]],

         [[ 0.0000e+00,  0.0000e+00, -3.4079e-28,  ...,  0.0000e+00,
            1.0403e-36,  3.8452e-03]],

         [[ 0.0000e+00,  0.0000e+00, -3.3742e-31,  ...,  0.0000e+00,
            5.6133e-13,  1.2939e-02]]]], device='cuda:0', dtype=torch.bfloat16)
x.stride0: (1, 1, 1)
x.shape: torch.Size([1, 1, 3584])
x.stride1: (7168, 3584, 1)
x.stride2: (7168, 1, 3584)
conv_state1: (10752, 1, 3584)
conv_state2: (10752, 1, 3584)
conv_state3: (10752, 1, 3584)
conv_state: torch.Size([1, 3584, 3])
conv1d_out: torch.Size([1, 3584, 1])
rearrange(conv1d_out, 'b d l -> (b l) d') shape: torch.Size([1, 3584])
x_proj_weight shape: torch.Size([144, 3584])
x_dbl: torch.Size([1, 144])
delta_proj_weight @ x_dbl[:, :delta_rank].t(): torch.Size([3584, 1])
conv_state: tensor([[[-0.8320, -2.0000, -0.9297],
         [-3.0781, -3.9688, -3.3750],
         [ 1.4609,  0.0359,  0.8320],
         ...,
         [ 1.5781,  1.5156,  3.3594],
         [-2.1719, -2.0000, -0.9727],
         [ 1.9766,  2.4062,  1.5625]]], device='cuda:0', dtype=torch.bfloat16,
       grad_fn=<AsStridedBackward0>)
ssm_state: tensor([[[[ 0.0000e+00,  0.0000e+00,  1.3884e-27,  ...,  0.0000e+00,
            6.2369e-30, -4.5471e-03]],

         [[ 0.0000e+00,  0.0000e+00, -2.7273e+27,  ...,  0.0000e+00,
           -2.0539e-15, -7.6599e-03]],

         [[ 0.0000e+00,  0.0000e+00, -4.5440e-34,  ...,  0.0000e+00,
            7.1484e-01,  5.3711e-03]],

         ...,

         [[ 0.0000e+00,  0.0000e+00,  1.1216e+23,  ...,  0.0000e+00,
           -4.0771e-02, -1.1520e-03]],

         [[ 0.0000e+00,  0.0000e+00, -4.2021e-22,  ...,  0.0000e+00,
           -7.3185e-33,  5.5847e-03]],

         [[ 0.0000e+00,  0.0000e+00, -2.5037e-32,  ...,  0.0000e+00,
            2.9582e-30,  1.1780e-02]]]], device='cuda:0', dtype=torch.bfloat16)
here here
no back here:
not here, not here
conv_state: tensor([[[-2.2812, -1.6875, -3.0312],
         [ 0.9297,  0.5273,  1.0703],
         [-2.1250, -1.3125, -2.0156],
         ...,
         [ 0.3652,  0.2109,  1.7656],
         [-1.9922, -1.2578, -0.5195],
         [ 0.8320,  1.7500,  2.8906]]], device='cuda:0', dtype=torch.bfloat16,
       grad_fn=<AsStridedBackward0>)
ssm_state: tensor([[[[ 0.0000e+00,  0.0000e+00,  9.7649e+06,  ...,  0.0000e+00,
            2.5422e-32,  1.5869e-03]],

         [[ 0.0000e+00,  0.0000e+00, -6.9389e-16,  ...,  0.0000e+00,
           -7.7345e-31, -6.4087e-03]],

         [[ 0.0000e+00,  0.0000e+00, -1.6797e-01,  ...,  0.0000e+00,
            3.6264e-21, -2.9755e-03]],

         ...,

         [[ 0.0000e+00,  0.0000e+00,  3.8163e+35,  ...,  0.0000e+00,
           -8.4679e+32, -7.1335e-04]],

         [[ 0.0000e+00,  0.0000e+00,  2.2259e-07,  ...,  0.0000e+00,
           -2.8690e+12, -8.8120e-04]],

         [[ 0.0000e+00,  0.0000e+00,  1.6096e+36,  ...,  0.0000e+00,
           -1.2062e+01, -1.8311e-03]]]], device='cuda:0', dtype=torch.bfloat16)
x.stride0: (1, 1, 1)
x.shape: torch.Size([1, 1, 3584])
x.stride1: (7168, 3584, 1)
x.stride2: (7168, 1, 3584)
conv_state1: (10752, 1, 3584)
conv_state2: (10752, 1, 3584)
conv_state3: (10752, 1, 3584)
conv_state: torch.Size([1, 3584, 3])
conv1d_out: torch.Size([1, 3584, 1])
rearrange(conv1d_out, 'b d l -> (b l) d') shape: torch.Size([1, 3584])
x_proj_weight shape: torch.Size([144, 3584])
x_dbl: torch.Size([1, 144])
delta_proj_weight @ x_dbl[:, :delta_rank].t(): torch.Size([3584, 1])
conv_state: tensor([[[-1.6875, -3.0312, -1.6406],
         [ 0.5273,  1.0703, -0.3906],
         [-1.3125, -2.0156, -1.8672],
         ...,
         [ 0.2109,  1.7656, -0.0615],
         [-1.2578, -0.5195, -2.3594],
         [ 1.7500,  2.8906,  2.2344]]], device='cuda:0', dtype=torch.bfloat16,
       grad_fn=<AsStridedBackward0>)
ssm_state: tensor([[[[ 0.0000e+00,  0.0000e+00, -1.9468e-25,  ...,  0.0000e+00,
           -3.3670e-20,  1.9150e-03]],

         [[ 0.0000e+00,  0.0000e+00, -3.8995e+28,  ...,  0.0000e+00,
            1.4346e-24, -4.4250e-03]],

         [[ 0.0000e+00,  0.0000e+00, -1.8822e-16,  ...,  0.0000e+00,
           -7.3468e-38, -5.7373e-03]],

         ...,

         [[ 0.0000e+00,  0.0000e+00, -1.4121e-28,  ...,  0.0000e+00,
           -1.2398e-08, -4.1809e-03]],

         [[ 0.0000e+00,  0.0000e+00,         nan,  ...,  0.0000e+00,
           -9.8978e+33,  1.5259e-03]],

         [[ 0.0000e+00,  0.0000e+00,  2.4319e+17,  ...,  0.0000e+00,
            8.8493e+26,  5.8746e-04]]]], device='cuda:0', dtype=torch.bfloat16)
here here
no back here:
not here, not here
conv_state: tensor([[[-0.4824, -2.5625, -1.7812],
         [-2.2812, -3.2500, -0.9883],
         [-0.1240,  2.4531, -0.1777],
         ...,
         [ 0.6172,  0.4570,  0.5039],
         [-3.5781, -3.5469, -3.7969],
         [-0.0164, -2.3281,  0.8984]]], device='cuda:0', dtype=torch.bfloat16,
       grad_fn=<AsStridedBackward0>)
ssm_state: tensor([[[[ 0.0000e+00,  0.0000e+00, -5.2242e+17,  ...,  0.0000e+00,
            7.0408e-07,  5.7373e-03]],

         [[ 0.0000e+00,  0.0000e+00, -8.8905e-17,  ...,  0.0000e+00,
            2.3660e+38,  1.5869e-03]],

         [[ 0.0000e+00,  0.0000e+00,  7.7760e+37,  ...,  0.0000e+00,
           -5.8301e+08, -1.2970e-04]],

         ...,

         [[ 0.0000e+00,  0.0000e+00, -3.5827e+35,  ...,  0.0000e+00,
           -1.0135e+37,  6.1035e-04]],

         [[ 0.0000e+00,  0.0000e+00,  1.2519e-36,  ...,  0.0000e+00,
           -2.0805e-11, -7.8735e-03]],

         [[ 0.0000e+00,  0.0000e+00, -6.1866e+07,  ...,  0.0000e+00,
           -7.8613e-02,  5.9814e-03]]]], device='cuda:0', dtype=torch.bfloat16)
x.stride0: (1, 1, 1)
x.shape: torch.Size([1, 1, 3584])
x.stride1: (7168, 3584, 1)
x.stride2: (7168, 1, 3584)
conv_state1: (10752, 1, 3584)
conv_state2: (10752, 1, 3584)
conv_state3: (10752, 1, 3584)
conv_state: torch.Size([1, 3584, 3])
conv1d_out: torch.Size([1, 3584, 1])
rearrange(conv1d_out, 'b d l -> (b l) d') shape: torch.Size([1, 3584])
x_proj_weight shape: torch.Size([144, 3584])
x_dbl: torch.Size([1, 144])
delta_proj_weight @ x_dbl[:, :delta_rank].t(): torch.Size([3584, 1])
conv_state: tensor([[[-2.5625, -1.7812, -1.4062],
         [-3.2500, -0.9883, -2.3438],
         [ 2.4531, -0.1777, -2.0000],
         ...,
         [ 0.4570,  0.5039, -0.8086],
         [-3.5469, -3.7969, -2.6250],
         [-2.3281,  0.8984,  1.0312]]], device='cuda:0', dtype=torch.bfloat16,
       grad_fn=<AsStridedBackward0>)
ssm_state: tensor([[[[ 0.0000e+00,  0.0000e+00,  4.3427e-23,  ...,  0.0000e+00,
           -5.3767e-24,  7.7209e-03]],

         [[ 0.0000e+00,  0.0000e+00,  4.5495e-23,  ...,  0.0000e+00,
           -3.1893e-31, -6.2256e-03]],

         [[ 0.0000e+00,  0.0000e+00, -1.4236e-30,  ...,  0.0000e+00,
            8.9461e+25, -1.1778e-04]],

         ...,

         [[ 0.0000e+00,  0.0000e+00,  7.2122e-06,  ...,  0.0000e+00,
            1.6285e+19,  6.0654e-04]],

         [[ 0.0000e+00,  0.0000e+00,  2.0680e-24,  ...,  0.0000e+00,
           -3.0265e+31, -7.2937e-03]],

         [[ 0.0000e+00,  0.0000e+00,  6.9815e-33,  ...,  0.0000e+00,
           -3.3794e+22,  4.9133e-03]]]], device='cuda:0', dtype=torch.bfloat16)
here here
no back here:
not here, not here
conv_state: tensor([[[-2.8438, -0.4375,  0.2754],
         [ 3.2812,  2.5156,  2.3125],
         [ 1.6641, -0.6406,  1.9375],
         ...,
         [ 0.4961, -0.5938,  0.4844],
         [-0.5820, -0.5078,  0.6016],
         [ 1.4062,  0.8828, -0.0171]]], device='cuda:0', dtype=torch.bfloat16,
       grad_fn=<AsStridedBackward0>)
ssm_state: tensor([[[[ 0.0000e+00,  0.0000e+00, -1.9263e+15,  ...,  0.0000e+00,
            8.0468e-20, -1.0742e-02]],

         [[ 0.0000e+00,  0.0000e+00, -3.7816e+20,  ...,  0.0000e+00,
           -3.7218e+37,  9.0332e-03]],

         [[ 0.0000e+00,  0.0000e+00,  1.1548e-07,  ...,  0.0000e+00,
            1.4855e+29, -7.3547e-03]],

         ...,

         [[ 0.0000e+00,  0.0000e+00,  3.0966e-08,  ...,  0.0000e+00,
            1.5550e-26, -1.4343e-03]],

         [[ 0.0000e+00,  0.0000e+00,  1.7039e+07,  ...,  0.0000e+00,
           -9.4489e+11, -1.0605e-03]],

         [[ 0.0000e+00,  0.0000e+00,  7.7758e+27,  ...,  0.0000e+00,
           -4.8800e+02, -1.1063e-03]]]], device='cuda:0', dtype=torch.bfloat16)
x.stride0: (1, 1, 1)
x.shape: torch.Size([1, 1, 3584])
x.stride1: (7168, 3584, 1)
x.stride2: (7168, 1, 3584)
conv_state1: (10752, 1, 3584)
conv_state2: (10752, 1, 3584)
conv_state3: (10752, 1, 3584)
conv_state: torch.Size([1, 3584, 3])
conv1d_out: torch.Size([1, 3584, 1])
rearrange(conv1d_out, 'b d l -> (b l) d') shape: torch.Size([1, 3584])
x_proj_weight shape: torch.Size([144, 3584])
x_dbl: torch.Size([1, 144])
delta_proj_weight @ x_dbl[:, :delta_rank].t(): torch.Size([3584, 1])
conv_state: tensor([[[-0.4375,  0.2754,     nan],
         [ 2.5156,  2.3125,     nan],
         [-0.6406,  1.9375,     nan],
         ...,
         [-0.5938,  0.4844,     nan],
         [-0.5078,  0.6016,     nan],
         [ 0.8828, -0.0171,     nan]]], device='cuda:0', dtype=torch.bfloat16,
       grad_fn=<AsStridedBackward0>)
ssm_state: tensor([[[[nan, nan, nan,  ..., nan, nan, nan]],

         [[nan, nan, nan,  ..., nan, nan, nan]],

         [[nan, nan, nan,  ..., nan, nan, nan]],

         ...,

         [[nan, nan, nan,  ..., nan, nan, nan]],

         [[nan, nan, nan,  ..., nan, nan, nan]],

         [[nan, nan, nan,  ..., nan, nan, nan]]]], device='cuda:0',
       dtype=torch.bfloat16)
here here
no back here:
not here, not here
conv_state: tensor([[[ 0.2178,  2.5312,  0.5664],
         [-2.4531, -4.5312, -3.5781],
         [-1.3281, -0.7266, -0.5078],
         ...,
         [-0.2471,  0.7188,  1.4141],
         [-0.6875,  0.0679, -1.3281],
         [ 2.5469,  1.3672,  1.3281]]], device='cuda:0', dtype=torch.bfloat16,
       grad_fn=<AsStridedBackward0>)
ssm_state: tensor([[[[ 0.0000e+00,  0.0000e+00,  4.1034e-13,  ...,  0.0000e+00,
            4.0960e+04, -2.1820e-03]],

         [[ 0.0000e+00,  0.0000e+00,  5.2214e-05,  ...,  0.0000e+00,
           -1.4953e+14,  9.5215e-03]],

         [[ 0.0000e+00,  0.0000e+00, -1.6712e+06,  ...,  0.0000e+00,
            7.9346e-37, -1.3733e-03]],

         ...,

         [[ 0.0000e+00,  0.0000e+00, -1.0904e+36,  ...,  0.0000e+00,
            1.0420e+07, -9.3384e-03]],

         [[ 0.0000e+00,  0.0000e+00,  1.8529e-20,  ...,  0.0000e+00,
            8.0111e+08,  1.4191e-03]],

         [[ 0.0000e+00,  0.0000e+00,  4.1351e-07,  ...,  0.0000e+00,
           -5.5318e-24,  4.0627e-04]]]], device='cuda:0', dtype=torch.bfloat16)
x.stride0: (1, 1, 1)
x.shape: torch.Size([1, 1, 3584])
x.stride1: (7168, 3584, 1)
x.stride2: (7168, 1, 3584)
conv_state1: (10752, 1, 3584)
conv_state2: (10752, 1, 3584)
conv_state3: (10752, 1, 3584)
conv_state: torch.Size([1, 3584, 3])
conv1d_out: torch.Size([1, 3584, 1])
rearrange(conv1d_out, 'b d l -> (b l) d') shape: torch.Size([1, 3584])
x_proj_weight shape: torch.Size([144, 3584])
x_dbl: torch.Size([1, 144])
delta_proj_weight @ x_dbl[:, :delta_rank].t(): torch.Size([3584, 1])
conv_state: tensor([[[ 2.5312,  0.5664,     nan],
         [-4.5312, -3.5781,     nan],
         [-0.7266, -0.5078,     nan],
         ...,
         [ 0.7188,  1.4141,     nan],
         [ 0.0679, -1.3281,     nan],
         [ 1.3672,  1.3281,     nan]]], device='cuda:0', dtype=torch.bfloat16,
       grad_fn=<AsStridedBackward0>)
ssm_state: tensor([[[[nan, nan, nan,  ..., nan, nan, nan]],

         [[nan, nan, nan,  ..., nan, nan, nan]],

         [[nan, nan, nan,  ..., nan, nan, nan]],

         ...,

         [[nan, nan, nan,  ..., nan, nan, nan]],

         [[nan, nan, nan,  ..., nan, nan, nan]],

         [[nan, nan, nan,  ..., nan, nan, nan]]]], device='cuda:0',
       dtype=torch.bfloat16)
here here
no back here:
not here, not here
conv_state: tensor([[[ 1.7812,  0.7344, -1.7344],
         [-1.4453, -1.5078,  0.0571],
         [-0.1416,  0.1377, -0.7383],
         ...,
         [ 2.7812,  5.8125,  7.2188],
         [ 2.8594,  1.5469,  2.1250],
         [ 3.4688,  3.9531,  2.4688]]], device='cuda:0', dtype=torch.bfloat16,
       grad_fn=<AsStridedBackward0>)
ssm_state: tensor([[[[ 0.0000e+00,  0.0000e+00,  3.0305e+30,  ...,  0.0000e+00,
           -1.9478e+17,  3.6926e-03]],

         [[ 0.0000e+00,  0.0000e+00, -1.0093e+12,  ...,  0.0000e+00,
           -2.3040e+04, -1.5198e-02]],

         [[ 0.0000e+00,  0.0000e+00, -2.6871e-30,  ...,  0.0000e+00,
           -1.9670e-05,  6.8359e-03]],

         ...,

         [[ 0.0000e+00,  0.0000e+00, -6.3787e-31,  ...,  0.0000e+00,
           -1.0551e+37, -1.9775e-02]],

         [[ 0.0000e+00,  0.0000e+00,  6.6202e+35,  ...,  0.0000e+00,
            1.9268e+32,  1.0803e-02]],

         [[ 0.0000e+00,  0.0000e+00,  2.9360e+08,  ...,  0.0000e+00,
           -5.7753e-11, -4.6997e-03]]]], device='cuda:0', dtype=torch.bfloat16)
x.stride0: (1, 1, 1)
x.shape: torch.Size([1, 1, 3584])
x.stride1: (7168, 3584, 1)
x.stride2: (7168, 1, 3584)
conv_state1: (10752, 1, 3584)
conv_state2: (10752, 1, 3584)
conv_state3: (10752, 1, 3584)
conv_state: torch.Size([1, 3584, 3])
conv1d_out: torch.Size([1, 3584, 1])
rearrange(conv1d_out, 'b d l -> (b l) d') shape: torch.Size([1, 3584])
x_proj_weight shape: torch.Size([144, 3584])
x_dbl: torch.Size([1, 144])
delta_proj_weight @ x_dbl[:, :delta_rank].t(): torch.Size([3584, 1])
conv_state: tensor([[[ 0.7344, -1.7344,     nan],
         [-1.5078,  0.0571,     nan],
         [ 0.1377, -0.7383,     nan],
         ...,
         [ 5.8125,  7.2188,     nan],
         [ 1.5469,  2.1250,     nan],
         [ 3.9531,  2.4688,     nan]]], device='cuda:0', dtype=torch.bfloat16,
       grad_fn=<AsStridedBackward0>)
ssm_state: tensor([[[[nan, nan, nan,  ..., nan, nan, nan]],

         [[nan, nan, nan,  ..., nan, nan, nan]],

         [[nan, nan, nan,  ..., nan, nan, nan]],

         ...,

         [[nan, nan, nan,  ..., nan, nan, nan]],

         [[nan, nan, nan,  ..., nan, nan, nan]],

         [[nan, nan, nan,  ..., nan, nan, nan]]]], device='cuda:0',
       dtype=torch.bfloat16)
here here
no back here:
not here, not here
conv_state: tensor([[[-2.2500, -1.4375, -1.3047],
         [ 2.4219,  0.3613, -3.1250],
         [ 1.7188,  0.2441,  2.0781],
         ...,
         [-2.8750, -2.9219, -2.0469],
         [ 3.9688,  3.7656,  3.0312],
         [ 0.7383, -0.7617, -0.8398]]], device='cuda:0', dtype=torch.bfloat16,
       grad_fn=<AsStridedBackward0>)
ssm_state: tensor([[[[ 0.0000e+00,  0.0000e+00, -1.3475e+19,  ...,  0.0000e+00,
           -3.0045e-32,  9.5215e-03]],

         [[ 0.0000e+00,  0.0000e+00, -4.2992e+07,  ...,  0.0000e+00,
            4.3195e-16,  7.4768e-03]],

         [[ 0.0000e+00,  0.0000e+00,  1.6483e+18,  ...,  0.0000e+00,
            3.3073e+16, -5.9891e-04]],

         ...,

         [[ 0.0000e+00,  0.0000e+00, -5.9557e-22,  ...,  0.0000e+00,
           -1.8175e-27,  6.2256e-03]],

         [[ 0.0000e+00,  0.0000e+00, -1.2695e-36,  ...,  0.0000e+00,
           -1.4211e-13, -5.7373e-03]],

         [[ 0.0000e+00,  0.0000e+00,  1.6709e-22,  ...,  0.0000e+00,
            2.1906e+19,  4.4861e-03]]]], device='cuda:0', dtype=torch.bfloat16)
x.stride0: (1, 1, 1)
x.shape: torch.Size([1, 1, 3584])
x.stride1: (7168, 3584, 1)
x.stride2: (7168, 1, 3584)
conv_state1: (10752, 1, 3584)
conv_state2: (10752, 1, 3584)
conv_state3: (10752, 1, 3584)
conv_state: torch.Size([1, 3584, 3])
conv1d_out: torch.Size([1, 3584, 1])
rearrange(conv1d_out, 'b d l -> (b l) d') shape: torch.Size([1, 3584])
x_proj_weight shape: torch.Size([144, 3584])
x_dbl: torch.Size([1, 144])
delta_proj_weight @ x_dbl[:, :delta_rank].t(): torch.Size([3584, 1])
conv_state: tensor([[[-1.4375, -1.3047,     nan],
         [ 0.3613, -3.1250,     nan],
         [ 0.2441,  2.0781,     nan],
         ...,
         [-2.9219, -2.0469,     nan],
         [ 3.7656,  3.0312,     nan],
         [-0.7617, -0.8398,     nan]]], device='cuda:0', dtype=torch.bfloat16,
       grad_fn=<AsStridedBackward0>)
ssm_state: tensor([[[[nan, nan, nan,  ..., nan, nan, nan]],

         [[nan, nan, nan,  ..., nan, nan, nan]],

         [[nan, nan, nan,  ..., nan, nan, nan]],

         ...,

         [[nan, nan, nan,  ..., nan, nan, nan]],

         [[nan, nan, nan,  ..., nan, nan, nan]],

         [[nan, nan, nan,  ..., nan, nan, nan]]]], device='cuda:0',
       dtype=torch.bfloat16)
here here
no back here:
not here, not here
conv_state: tensor([[[-0.8164,  0.1465, -0.3477],
         [ 0.7188,  0.4160,  3.1250],
         [-1.9219, -2.5938, -1.2891],
         ...,
         [-1.2734, -0.2393,  0.0884],
         [ 0.8359, -0.9648, -0.4922],
         [ 1.0078,  0.0452,  1.5703]]], device='cuda:0', dtype=torch.bfloat16,
       grad_fn=<AsStridedBackward0>)
ssm_state: tensor([[[[ 0.0000e+00,  0.0000e+00,  1.5658e+34,  ...,  0.0000e+00,
           -2.4300e+02, -2.1667e-03]],

         [[ 0.0000e+00,  0.0000e+00,  6.0659e+29,  ...,  0.0000e+00,
           -1.9923e+08,  1.5747e-02]],

         [[ 0.0000e+00,  0.0000e+00, -9.5539e-23,  ...,  0.0000e+00,
           -9.0325e-29, -1.6113e-02]],

         ...,

         [[ 0.0000e+00,  0.0000e+00,  2.3173e-30,  ...,  0.0000e+00,
           -3.5593e-27, -8.6670e-03]],

         [[ 0.0000e+00,  0.0000e+00,  1.1134e-33,  ...,  0.0000e+00,
            3.8251e+23,  3.9577e-05]],

         [[ 0.0000e+00,  0.0000e+00,  4.9067e+35,  ...,  0.0000e+00,
            1.0697e-36, -1.5442e-02]]]], device='cuda:0', dtype=torch.bfloat16)
x.stride0: (1, 1, 1)
x.shape: torch.Size([1, 1, 3584])
x.stride1: (7168, 3584, 1)
x.stride2: (7168, 1, 3584)
conv_state1: (10752, 1, 3584)
conv_state2: (10752, 1, 3584)
conv_state3: (10752, 1, 3584)
conv_state: torch.Size([1, 3584, 3])
conv1d_out: torch.Size([1, 3584, 1])
rearrange(conv1d_out, 'b d l -> (b l) d') shape: torch.Size([1, 3584])
x_proj_weight shape: torch.Size([144, 3584])
x_dbl: torch.Size([1, 144])
delta_proj_weight @ x_dbl[:, :delta_rank].t(): torch.Size([3584, 1])
conv_state: tensor([[[ 0.1465, -0.3477,     nan],
         [ 0.4160,  3.1250,     nan],
         [-2.5938, -1.2891,     nan],
         ...,
         [-0.2393,  0.0884,     nan],
         [-0.9648, -0.4922,     nan],
         [ 0.0452,  1.5703,     nan]]], device='cuda:0', dtype=torch.bfloat16,
       grad_fn=<AsStridedBackward0>)
ssm_state: tensor([[[[nan, nan, nan,  ..., nan, nan, nan]],

         [[nan, nan, nan,  ..., nan, nan, nan]],

         [[nan, nan, nan,  ..., nan, nan, nan]],

         ...,

         [[nan, nan, nan,  ..., nan, nan, nan]],

         [[nan, nan, nan,  ..., nan, nan, nan]],

         [[nan, nan, nan,  ..., nan, nan, nan]]]], device='cuda:0',
       dtype=torch.bfloat16)
here here
no back here:
not here, not here
conv_state: tensor([[[-3.3438e+00, -5.2500e+00, -3.7656e+00],
         [ 1.5469e+00,  1.5547e+00,  9.8828e-01],
         [-4.6875e-01, -2.4531e+00,  4.7461e-01],
         ...,
         [-2.5156e+00, -2.0000e+00, -1.8906e+00],
         [ 4.7302e-03,  1.8164e-01,  1.4160e-01],
         [ 1.3281e+00,  2.7969e+00,  3.8281e+00]]], device='cuda:0',
       dtype=torch.bfloat16, grad_fn=<AsStridedBackward0>)
ssm_state: tensor([[[[ 0.0000e+00,  0.0000e+00, -1.7624e-03,  ...,  0.0000e+00,
           -3.4043e+28, -2.1118e-02]],

         [[ 0.0000e+00,  0.0000e+00, -1.0578e+24,  ...,  0.0000e+00,
           -1.8933e-29,  3.0518e-02]],

         [[ 0.0000e+00,  0.0000e+00,  1.0596e-10,  ...,  0.0000e+00,
           -2.8429e+16, -2.2278e-03]],

         ...,

         [[ 0.0000e+00,  0.0000e+00,  8.7043e+25,  ...,  0.0000e+00,
           -8.1914e+13, -1.7166e-03]],

         [[ 0.0000e+00,  0.0000e+00,  7.8952e+21,  ...,  0.0000e+00,
            2.5884e-30, -4.1809e-03]],

         [[ 0.0000e+00,  0.0000e+00,  7.8583e-04,  ...,  0.0000e+00,
            3.9873e-35,  1.0605e-03]]]], device='cuda:0', dtype=torch.bfloat16)
x.stride0: (1, 1, 1)
x.shape: torch.Size([1, 1, 3584])
x.stride1: (7168, 3584, 1)
x.stride2: (7168, 1, 3584)
conv_state1: (10752, 1, 3584)
conv_state2: (10752, 1, 3584)
conv_state3: (10752, 1, 3584)
conv_state: torch.Size([1, 3584, 3])
conv1d_out: torch.Size([1, 3584, 1])
rearrange(conv1d_out, 'b d l -> (b l) d') shape: torch.Size([1, 3584])
x_proj_weight shape: torch.Size([144, 3584])
x_dbl: torch.Size([1, 144])
delta_proj_weight @ x_dbl[:, :delta_rank].t(): torch.Size([3584, 1])
conv_state: tensor([[[-5.2500, -3.7656,     nan],
         [ 1.5547,  0.9883,     nan],
         [-2.4531,  0.4746,     nan],
         ...,
         [-2.0000, -1.8906,     nan],
         [ 0.1816,  0.1416,     nan],
         [ 2.7969,  3.8281,     nan]]], device='cuda:0', dtype=torch.bfloat16,
       grad_fn=<AsStridedBackward0>)
ssm_state: tensor([[[[nan, nan, nan,  ..., nan, nan, nan]],

         [[nan, nan, nan,  ..., nan, nan, nan]],

         [[nan, nan, nan,  ..., nan, nan, nan]],

         ...,

         [[nan, nan, nan,  ..., nan, nan, nan]],

         [[nan, nan, nan,  ..., nan, nan, nan]],

         [[nan, nan, nan,  ..., nan, nan, nan]]]], device='cuda:0',
       dtype=torch.bfloat16)
here here
no back here:
not here, not here
conv_state: tensor([[[ 1.3828,  0.6641, -0.1084],
         [-1.3594, -0.6211, -2.6562],
         [-2.2031,  0.0908, -1.6875],
         ...,
         [ 0.1787,  0.6133, -0.3711],
         [-1.9922, -0.3555, -1.5703],
         [ 0.7539, -0.4277, -1.3672]]], device='cuda:0', dtype=torch.bfloat16,
       grad_fn=<AsStridedBackward0>)
ssm_state: tensor([[[[ 0.0000e+00,  0.0000e+00,  6.2172e-13,  ...,  0.0000e+00,
           -3.5058e-25, -1.1719e-02]],

         [[ 0.0000e+00,  0.0000e+00,  1.4336e+06,  ...,  0.0000e+00,
            6.9625e+08,  2.1973e-02]],

         [[ 0.0000e+00,  0.0000e+00,  1.0164e-20,  ...,  0.0000e+00,
            1.4710e+11,  8.2397e-03]],

         ...,

         [[ 0.0000e+00,  0.0000e+00,  6.5020e-38,  ...,  0.0000e+00,
           -2.5411e-19, -1.3916e-02]],

         [[ 0.0000e+00,  0.0000e+00, -3.2212e+11,  ...,  0.0000e+00,
           -1.3298e+26,  1.2024e-02]],

         [[ 0.0000e+00,  0.0000e+00,  1.5485e-21,  ...,  0.0000e+00,
           -3.3951e-04, -1.2756e-02]]]], device='cuda:0', dtype=torch.bfloat16)
x.stride0: (1, 1, 1)
x.shape: torch.Size([1, 1, 3584])
x.stride1: (7168, 3584, 1)
x.stride2: (7168, 1, 3584)
conv_state1: (10752, 1, 3584)
conv_state2: (10752, 1, 3584)
conv_state3: (10752, 1, 3584)
conv_state: torch.Size([1, 3584, 3])
conv1d_out: torch.Size([1, 3584, 1])
rearrange(conv1d_out, 'b d l -> (b l) d') shape: torch.Size([1, 3584])
x_proj_weight shape: torch.Size([144, 3584])
x_dbl: torch.Size([1, 144])
delta_proj_weight @ x_dbl[:, :delta_rank].t(): torch.Size([3584, 1])
conv_state: tensor([[[ 0.6641, -0.1084,     nan],
         [-0.6211, -2.6562,     nan],
         [ 0.0908, -1.6875,     nan],
         ...,
         [ 0.6133, -0.3711,     nan],
         [-0.3555, -1.5703,     nan],
         [-0.4277, -1.3672,     nan]]], device='cuda:0', dtype=torch.bfloat16,
       grad_fn=<AsStridedBackward0>)
ssm_state: tensor([[[[nan, nan, nan,  ..., nan, nan, nan]],

         [[nan, nan, nan,  ..., nan, nan, nan]],

         [[nan, nan, nan,  ..., nan, nan, nan]],

         ...,

         [[nan, nan, nan,  ..., nan, nan, nan]],

         [[nan, nan, nan,  ..., nan, nan, nan]],

         [[nan, nan, nan,  ..., nan, nan, nan]]]], device='cuda:0',
       dtype=torch.bfloat16)
here here
no back here:
not here, not here
conv_state: tensor([[[ 0.7031,  0.0265, -1.8438],
         [ 0.8008, -1.3672,  0.3027],
         [ 3.5469,  4.5000,  2.5781],
         ...,
         [ 0.3281,  3.2344,  2.1406],
         [ 0.9648,  3.0469,  2.0938],
         [-0.9766, -1.2969, -0.4805]]], device='cuda:0', dtype=torch.bfloat16,
       grad_fn=<AsStridedBackward0>)
ssm_state: tensor([[[[ 0.0000e+00,  0.0000e+00, -5.1356e+22,  ...,  0.0000e+00,
           -3.1959e-38, -2.4719e-03]],

         [[ 0.0000e+00,  0.0000e+00,  4.5524e+24,  ...,  0.0000e+00,
            1.3084e+14, -6.0654e-04]],

         [[ 0.0000e+00,  0.0000e+00, -6.6336e-15,  ...,  0.0000e+00,
           -2.6822e-07,  3.9062e-02]],

         ...,

         [[ 0.0000e+00,  0.0000e+00,  9.0473e-25,  ...,  0.0000e+00,
            3.9387e-20, -6.9275e-03]],

         [[ 0.0000e+00,  0.0000e+00, -1.1882e-29,  ...,  0.0000e+00,
           -5.3906e-01, -1.3123e-02]],

         [[ 0.0000e+00,  0.0000e+00, -5.1034e-20,  ...,  0.0000e+00,
            1.1402e-31, -1.1963e-02]]]], device='cuda:0', dtype=torch.bfloat16)
x.stride0: (1, 1, 1)
x.shape: torch.Size([1, 1, 3584])
x.stride1: (7168, 3584, 1)
x.stride2: (7168, 1, 3584)
conv_state1: (10752, 1, 3584)
conv_state2: (10752, 1, 3584)
conv_state3: (10752, 1, 3584)
conv_state: torch.Size([1, 3584, 3])
conv1d_out: torch.Size([1, 3584, 1])
rearrange(conv1d_out, 'b d l -> (b l) d') shape: torch.Size([1, 3584])
x_proj_weight shape: torch.Size([144, 3584])
x_dbl: torch.Size([1, 144])
delta_proj_weight @ x_dbl[:, :delta_rank].t(): torch.Size([3584, 1])
conv_state: tensor([[[ 0.0265, -1.8438,     nan],
         [-1.3672,  0.3027,     nan],
         [ 4.5000,  2.5781,     nan],
         ...,
         [ 3.2344,  2.1406,     nan],
         [ 3.0469,  2.0938,     nan],
         [-1.2969, -0.4805,     nan]]], device='cuda:0', dtype=torch.bfloat16,
       grad_fn=<AsStridedBackward0>)
ssm_state: tensor([[[[nan, nan, nan,  ..., nan, nan, nan]],

         [[nan, nan, nan,  ..., nan, nan, nan]],

         [[nan, nan, nan,  ..., nan, nan, nan]],

         ...,

         [[nan, nan, nan,  ..., nan, nan, nan]],

         [[nan, nan, nan,  ..., nan, nan, nan]],

         [[nan, nan, nan,  ..., nan, nan, nan]]]], device='cuda:0',
       dtype=torch.bfloat16)
here here
no back here:
not here, not here
conv_state: tensor([[[-0.2578, -2.5156, -2.1406],
         [ 0.8086,  0.3457,  1.5625],
         [-1.3438,  0.4844, -0.0050],
         ...,
         [-1.1562, -2.8750, -1.8984],
         [-3.2188, -0.4199, -0.9141],
         [-3.6406, -4.5625, -3.2344]]], device='cuda:0', dtype=torch.bfloat16,
       grad_fn=<AsStridedBackward0>)
ssm_state: tensor([[[[ 0.0000e+00,  0.0000e+00, -7.2634e+19,  ...,  0.0000e+00,
            1.4298e-18, -1.1414e-02]],

         [[ 0.0000e+00,  0.0000e+00, -4.4813e+29,  ...,  0.0000e+00,
            1.5187e-25,  3.1982e-02]],

         [[ 0.0000e+00,  0.0000e+00, -9.0600e+14,  ...,  0.0000e+00,
           -2.4920e-36,  6.9580e-03]],

         ...,

         [[ 0.0000e+00,  0.0000e+00,  1.0800e+03,  ...,  0.0000e+00,
            1.9527e+15,  1.0452e-03]],

         [[ 0.0000e+00,  0.0000e+00,  7.5045e+32,  ...,  0.0000e+00,
           -5.3850e+28,  8.4229e-03]],

         [[ 0.0000e+00,  0.0000e+00,  2.9029e+32,  ...,  0.0000e+00,
            4.0550e+05, -6.1035e-03]]]], device='cuda:0', dtype=torch.bfloat16)
x.stride0: (1, 1, 1)
x.shape: torch.Size([1, 1, 3584])
x.stride1: (7168, 3584, 1)
x.stride2: (7168, 1, 3584)
conv_state1: (10752, 1, 3584)
conv_state2: (10752, 1, 3584)
conv_state3: (10752, 1, 3584)
conv_state: torch.Size([1, 3584, 3])
conv1d_out: torch.Size([1, 3584, 1])
rearrange(conv1d_out, 'b d l -> (b l) d') shape: torch.Size([1, 3584])
x_proj_weight shape: torch.Size([144, 3584])
x_dbl: torch.Size([1, 144])
delta_proj_weight @ x_dbl[:, :delta_rank].t(): torch.Size([3584, 1])
conv_state: tensor([[[-2.5156, -2.1406,     nan],
         [ 0.3457,  1.5625,     nan],
         [ 0.4844, -0.0050,     nan],
         ...,
         [-2.8750, -1.8984,     nan],
         [-0.4199, -0.9141,     nan],
         [-4.5625, -3.2344,     nan]]], device='cuda:0', dtype=torch.bfloat16,
       grad_fn=<AsStridedBackward0>)
ssm_state: tensor([[[[nan, nan, nan,  ..., nan, nan, nan]],

         [[nan, nan, nan,  ..., nan, nan, nan]],

         [[nan, nan, nan,  ..., nan, nan, nan]],

         ...,

         [[nan, nan, nan,  ..., nan, nan, nan]],

         [[nan, nan, nan,  ..., nan, nan, nan]],

         [[nan, nan, nan,  ..., nan, nan, nan]]]], device='cuda:0',
       dtype=torch.bfloat16)
here here
no back here:
not here, not here
conv_state: tensor([[[ 2.2344, -3.0312, -1.4297],
         [ 1.9609,  1.7031,  2.2969],
         [-0.2812,  1.3672,  0.8438],
         ...,
         [ 1.1797, -0.2617,  1.2578],
         [-3.1875, -1.4688, -3.1562],
         [-2.5469, -2.6875, -0.8555]]], device='cuda:0', dtype=torch.bfloat16,
       grad_fn=<AsStridedBackward0>)
ssm_state: tensor([[[[ 0.0000e+00,  0.0000e+00, -1.3739e-15,  ...,  0.0000e+00,
           -1.3271e-16,  1.4404e-02]],

         [[ 0.0000e+00,  0.0000e+00, -2.1853e+13,  ...,  0.0000e+00,
           -8.8905e-17, -1.0620e-02]],

         [[ 0.0000e+00,  0.0000e+00, -1.8279e+27,  ...,  0.0000e+00,
            1.0368e-18,  1.5198e-02]],

         ...,

         [[ 0.0000e+00,  0.0000e+00,  6.9152e-22,  ...,  0.0000e+00,
            1.5450e-04,  1.4648e-02]],

         [[ 0.0000e+00,  0.0000e+00,  7.3612e-06,  ...,  0.0000e+00,
           -3.3742e-31, -6.5231e-04]],

         [[ 0.0000e+00,  0.0000e+00, -9.0599e-05,  ...,  0.0000e+00,
            3.6716e-05,  1.1292e-02]]]], device='cuda:0', dtype=torch.bfloat16)
x.stride0: (1, 1, 1)
x.shape: torch.Size([1, 1, 3584])
x.stride1: (7168, 3584, 1)
x.stride2: (7168, 1, 3584)
conv_state1: (10752, 1, 3584)
conv_state2: (10752, 1, 3584)
conv_state3: (10752, 1, 3584)
conv_state: torch.Size([1, 3584, 3])
conv1d_out: torch.Size([1, 3584, 1])
rearrange(conv1d_out, 'b d l -> (b l) d') shape: torch.Size([1, 3584])
x_proj_weight shape: torch.Size([144, 3584])
x_dbl: torch.Size([1, 144])
delta_proj_weight @ x_dbl[:, :delta_rank].t(): torch.Size([3584, 1])
conv_state: tensor([[[-3.0312, -1.4297,     nan],
         [ 1.7031,  2.2969,     nan],
         [ 1.3672,  0.8438,     nan],
         ...,
         [-0.2617,  1.2578,     nan],
         [-1.4688, -3.1562,     nan],
         [-2.6875, -0.8555,     nan]]], device='cuda:0', dtype=torch.bfloat16,
       grad_fn=<AsStridedBackward0>)
ssm_state: tensor([[[[nan, nan, nan,  ..., nan, nan, nan]],

         [[nan, nan, nan,  ..., nan, nan, nan]],

         [[nan, nan, nan,  ..., nan, nan, nan]],

         ...,

         [[nan, nan, nan,  ..., nan, nan, nan]],

         [[nan, nan, nan,  ..., nan, nan, nan]],

         [[nan, nan, nan,  ..., nan, nan, nan]]]], device='cuda:0',
       dtype=torch.bfloat16)
here here
no back here:
not here, not here
conv_state: tensor([[[ 1.1641,  3.2500,  3.4688],
         [ 0.3633, -1.3047, -0.2070],
         [ 1.3984, -0.5664, -0.5547],
         ...,
         [ 2.0000, -1.2969,  0.4512],
         [ 2.8750,  2.1562,  2.2031],
         [-1.5391,  1.5234, -0.2676]]], device='cuda:0', dtype=torch.bfloat16,
       grad_fn=<AsStridedBackward0>)
ssm_state: tensor([[[[ 0.0000e+00,  0.0000e+00,  6.3941e-32,  ...,  0.0000e+00,
           -1.0304e-29, -5.8289e-03]],

         [[ 0.0000e+00,  0.0000e+00,  7.1305e+29,  ...,  0.0000e+00,
           -3.7000e+02,  5.5420e-02]],

         [[ 0.0000e+00,  0.0000e+00, -1.9837e-07,  ...,  0.0000e+00,
           -2.8731e+08, -3.7384e-03]],

         ...,

         [[ 0.0000e+00,  0.0000e+00,  2.2334e-37,  ...,  0.0000e+00,
                   nan, -1.0107e-01]],

         [[ 0.0000e+00,  0.0000e+00, -2.8776e+32,  ...,  0.0000e+00,
            2.0133e+08,  2.5757e-02]],

         [[ 0.0000e+00,  0.0000e+00, -4.1708e+25,  ...,  0.0000e+00,
            1.0853e-21, -3.7689e-03]]]], device='cuda:0', dtype=torch.bfloat16)
x.stride0: (1, 1, 1)
x.shape: torch.Size([1, 1, 3584])
x.stride1: (7168, 3584, 1)
x.stride2: (7168, 1, 3584)
conv_state1: (10752, 1, 3584)
conv_state2: (10752, 1, 3584)
conv_state3: (10752, 1, 3584)
conv_state: torch.Size([1, 3584, 3])
conv1d_out: torch.Size([1, 3584, 1])
rearrange(conv1d_out, 'b d l -> (b l) d') shape: torch.Size([1, 3584])
x_proj_weight shape: torch.Size([144, 3584])
x_dbl: torch.Size([1, 144])
delta_proj_weight @ x_dbl[:, :delta_rank].t(): torch.Size([3584, 1])
conv_state: tensor([[[ 3.2500,  3.4688,     nan],
         [-1.3047, -0.2070,     nan],
         [-0.5664, -0.5547,     nan],
         ...,
         [-1.2969,  0.4512,     nan],
         [ 2.1562,  2.2031,     nan],
         [ 1.5234, -0.2676,     nan]]], device='cuda:0', dtype=torch.bfloat16,
       grad_fn=<AsStridedBackward0>)
ssm_state: tensor([[[[nan, nan, nan,  ..., nan, nan, nan]],

         [[nan, nan, nan,  ..., nan, nan, nan]],

         [[nan, nan, nan,  ..., nan, nan, nan]],

         ...,

         [[nan, nan, nan,  ..., nan, nan, nan]],

         [[nan, nan, nan,  ..., nan, nan, nan]],

         [[nan, nan, nan,  ..., nan, nan, nan]]]], device='cuda:0',
       dtype=torch.bfloat16)
here here
no back here:
not here, not here
conv_state: tensor([[[ 0.3418,  1.6562,  1.7969],
         [ 1.4141, -0.8555, -1.0234],
         [-0.2314, -0.4199, -0.5938],
         ...,
         [-1.2578,  1.0781,  1.5547],
         [ 1.1953, -1.9609, -3.1719],
         [-3.9844, -5.3438, -3.7188]]], device='cuda:0', dtype=torch.bfloat16,
       grad_fn=<AsStridedBackward0>)
ssm_state: tensor([[[[ 0.0000e+00,  0.0000e+00, -1.7213e-33,  ...,  0.0000e+00,
            2.3732e-12,  1.9775e-02]],

         [[ 0.0000e+00,  0.0000e+00,  4.4646e+05,  ...,  0.0000e+00,
           -3.5861e+08,  1.2283e-03]],

         [[ 0.0000e+00,  0.0000e+00,  5.9691e+24,  ...,  0.0000e+00,
           -3.3244e-38, -8.8379e-02]],

         ...,

         [[ 0.0000e+00,  0.0000e+00, -1.7166e-27,  ...,  0.0000e+00,
            6.3332e+15, -1.1047e-02]],

         [[ 0.0000e+00,  0.0000e+00, -8.9615e-28,  ...,  0.0000e+00,
           -6.9796e+06,  3.7842e-02]],

         [[ 0.0000e+00,  0.0000e+00, -3.7616e-34,  ...,  0.0000e+00,
            3.1585e-31,  6.9580e-03]]]], device='cuda:0', dtype=torch.bfloat16)
x.stride0: (1, 1, 1)
x.shape: torch.Size([1, 1, 3584])
x.stride1: (7168, 3584, 1)
x.stride2: (7168, 1, 3584)
conv_state1: (10752, 1, 3584)
conv_state2: (10752, 1, 3584)
conv_state3: (10752, 1, 3584)
conv_state: torch.Size([1, 3584, 3])
conv1d_out: torch.Size([1, 3584, 1])
rearrange(conv1d_out, 'b d l -> (b l) d') shape: torch.Size([1, 3584])
x_proj_weight shape: torch.Size([144, 3584])
x_dbl: torch.Size([1, 144])
delta_proj_weight @ x_dbl[:, :delta_rank].t(): torch.Size([3584, 1])
conv_state: tensor([[[ 1.6562,  1.7969,     nan],
         [-0.8555, -1.0234,     nan],
         [-0.4199, -0.5938,     nan],
         ...,
         [ 1.0781,  1.5547,     nan],
         [-1.9609, -3.1719,     nan],
         [-5.3438, -3.7188,     nan]]], device='cuda:0', dtype=torch.bfloat16,
       grad_fn=<AsStridedBackward0>)
ssm_state: tensor([[[[nan, nan, nan,  ..., nan, nan, nan]],

         [[nan, nan, nan,  ..., nan, nan, nan]],

         [[nan, nan, nan,  ..., nan, nan, nan]],

         ...,

         [[nan, nan, nan,  ..., nan, nan, nan]],

         [[nan, nan, nan,  ..., nan, nan, nan]],

         [[nan, nan, nan,  ..., nan, nan, nan]]]], device='cuda:0',
       dtype=torch.bfloat16)
here here
no back here:
not here, not here
conv_state: tensor([[[-1.7734,  0.1250,  0.8359],
         [-0.8672, -1.2891, -1.4375],
         [ 1.9844,  1.5625,  1.7656],
         ...,
         [ 0.7969,  1.6797,  0.7773],
         [ 2.2344, -0.3672, -1.9922],
         [ 1.9297,  1.8594,  0.7266]]], device='cuda:0', dtype=torch.bfloat16,
       grad_fn=<AsStridedBackward0>)
ssm_state: tensor([[[[ 0.0000e+00,  0.0000e+00,  4.5300e-05,  ...,  0.0000e+00,
            1.0274e+16, -1.0315e-02]],

         [[ 0.0000e+00,  0.0000e+00,  1.2754e+15,  ...,  0.0000e+00,
            6.5536e+07, -2.7466e-02]],

         [[ 0.0000e+00,  0.0000e+00, -1.0648e+33,  ...,  0.0000e+00,
            1.4641e+32, -7.1106e-03]],

         ...,

         [[ 0.0000e+00,  0.0000e+00, -2.0808e+06,  ...,  0.0000e+00,
           -4.0676e-31,  4.5471e-03]],

         [[ 0.0000e+00,  0.0000e+00,  1.8254e+33,  ...,  0.0000e+00,
           -8.7041e-14,  1.9409e-02]],

         [[ 0.0000e+00,  0.0000e+00,  1.5022e-31,  ...,  0.0000e+00,
           -7.6514e-23, -8.1787e-03]]]], device='cuda:0', dtype=torch.bfloat16)
x.stride0: (1, 1, 1)
x.shape: torch.Size([1, 1, 3584])
x.stride1: (7168, 3584, 1)
x.stride2: (7168, 1, 3584)
conv_state1: (10752, 1, 3584)
conv_state2: (10752, 1, 3584)
conv_state3: (10752, 1, 3584)
conv_state: torch.Size([1, 3584, 3])
conv1d_out: torch.Size([1, 3584, 1])
rearrange(conv1d_out, 'b d l -> (b l) d') shape: torch.Size([1, 3584])
x_proj_weight shape: torch.Size([144, 3584])
x_dbl: torch.Size([1, 144])
delta_proj_weight @ x_dbl[:, :delta_rank].t(): torch.Size([3584, 1])
conv_state: tensor([[[ 0.1250,  0.8359,     nan],
         [-1.2891, -1.4375,     nan],
         [ 1.5625,  1.7656,     nan],
         ...,
         [ 1.6797,  0.7773,     nan],
         [-0.3672, -1.9922,     nan],
         [ 1.8594,  0.7266,     nan]]], device='cuda:0', dtype=torch.bfloat16,
       grad_fn=<AsStridedBackward0>)
ssm_state: tensor([[[[nan, nan, nan,  ..., nan, nan, nan]],

         [[nan, nan, nan,  ..., nan, nan, nan]],

         [[nan, nan, nan,  ..., nan, nan, nan]],

         ...,

         [[nan, nan, nan,  ..., nan, nan, nan]],

         [[nan, nan, nan,  ..., nan, nan, nan]],

         [[nan, nan, nan,  ..., nan, nan, nan]]]], device='cuda:0',
       dtype=torch.bfloat16)
here here
no back here:
not here, not here
conv_state: tensor([[[nan, nan, nan],
         [nan, nan, nan],
         [nan, nan, nan],
         ...,
         [nan, nan, nan],
         [nan, nan, nan],
         [nan, nan, nan]]], device='cuda:0', dtype=torch.bfloat16,
       grad_fn=<AsStridedBackward0>)
ssm_state: tensor([[[[nan, nan, nan,  ..., nan, nan, nan]],

         [[nan, nan, nan,  ..., nan, nan, nan]],

         [[nan, nan, nan,  ..., nan, nan, nan]],

         ...,

         [[nan, nan, nan,  ..., nan, nan, nan]],

         [[nan, nan, nan,  ..., nan, nan, nan]],

         [[nan, nan, nan,  ..., nan, nan, nan]]]], device='cuda:0',
       dtype=torch.bfloat16)
x.stride0: (1, 1, 1)
x.shape: torch.Size([1, 1, 3584])
x.stride1: (7168, 3584, 1)
x.stride2: (7168, 1, 3584)
conv_state1: (10752, 1, 3584)
conv_state2: (10752, 1, 3584)
conv_state3: (10752, 1, 3584)
conv_state: torch.Size([1, 3584, 3])
conv1d_out: torch.Size([1, 3584, 1])
rearrange(conv1d_out, 'b d l -> (b l) d') shape: torch.Size([1, 3584])
x_proj_weight shape: torch.Size([144, 3584])
x_dbl: torch.Size([1, 144])
delta_proj_weight @ x_dbl[:, :delta_rank].t(): torch.Size([3584, 1])
conv_state: tensor([[[nan, nan, nan],
         [nan, nan, nan],
         [nan, nan, nan],
         ...,
         [nan, nan, nan],
         [nan, nan, nan],
         [nan, nan, nan]]], device='cuda:0', dtype=torch.bfloat16,
       grad_fn=<AsStridedBackward0>)
ssm_state: tensor([[[[nan, nan, nan,  ..., nan, nan, nan]],

         [[nan, nan, nan,  ..., nan, nan, nan]],

         [[nan, nan, nan,  ..., nan, nan, nan]],

         ...,

         [[nan, nan, nan,  ..., nan, nan, nan]],

         [[nan, nan, nan,  ..., nan, nan, nan]],

         [[nan, nan, nan,  ..., nan, nan, nan]]]], device='cuda:0',
       dtype=torch.bfloat16)
here here
no back here:
not here, not here
conv_state: tensor([[[nan, nan, nan],
         [nan, nan, nan],
         [nan, nan, nan],
         ...,
         [nan, nan, nan],
         [nan, nan, nan],
         [nan, nan, nan]]], device='cuda:0', dtype=torch.bfloat16,
       grad_fn=<AsStridedBackward0>)
ssm_state: tensor([[[[nan, nan, nan,  ..., nan, nan, nan]],

         [[nan, nan, nan,  ..., nan, nan, nan]],

         [[nan, nan, nan,  ..., nan, nan, nan]],

         ...,

         [[nan, nan, nan,  ..., nan, nan, nan]],

         [[nan, nan, nan,  ..., nan, nan, nan]],

         [[nan, nan, nan,  ..., nan, nan, nan]]]], device='cuda:0',
       dtype=torch.bfloat16)
x.stride0: (1, 1, 1)
x.shape: torch.Size([1, 1, 3584])
x.stride1: (7168, 3584, 1)
x.stride2: (7168, 1, 3584)
conv_state1: (10752, 1, 3584)
conv_state2: (10752, 1, 3584)
conv_state3: (10752, 1, 3584)
conv_state: torch.Size([1, 3584, 3])
conv1d_out: torch.Size([1, 3584, 1])
rearrange(conv1d_out, 'b d l -> (b l) d') shape: torch.Size([1, 3584])
x_proj_weight shape: torch.Size([144, 3584])
x_dbl: torch.Size([1, 144])
delta_proj_weight @ x_dbl[:, :delta_rank].t(): torch.Size([3584, 1])
conv_state: tensor([[[nan, nan, nan],
         [nan, nan, nan],
         [nan, nan, nan],
         ...,
         [nan, nan, nan],
         [nan, nan, nan],
         [nan, nan, nan]]], device='cuda:0', dtype=torch.bfloat16,
       grad_fn=<AsStridedBackward0>)
ssm_state: tensor([[[[nan, nan, nan,  ..., nan, nan, nan]],

         [[nan, nan, nan,  ..., nan, nan, nan]],

         [[nan, nan, nan,  ..., nan, nan, nan]],

         ...,

         [[nan, nan, nan,  ..., nan, nan, nan]],

         [[nan, nan, nan,  ..., nan, nan, nan]],

         [[nan, nan, nan,  ..., nan, nan, nan]]]], device='cuda:0',
       dtype=torch.bfloat16)
here here
no back here:
not here, not here
conv_state: tensor([[[nan, nan, nan],
         [nan, nan, nan],
         [nan, nan, nan],
         ...,
         [nan, nan, nan],
         [nan, nan, nan],
         [nan, nan, nan]]], device='cuda:0', dtype=torch.bfloat16,
       grad_fn=<AsStridedBackward0>)
ssm_state: tensor([[[[nan, nan, nan,  ..., nan, nan, nan]],

         [[nan, nan, nan,  ..., nan, nan, nan]],

         [[nan, nan, nan,  ..., nan, nan, nan]],

         ...,

         [[nan, nan, nan,  ..., nan, nan, nan]],

         [[nan, nan, nan,  ..., nan, nan, nan]],

         [[nan, nan, nan,  ..., nan, nan, nan]]]], device='cuda:0',
       dtype=torch.bfloat16)
x.stride0: (1, 1, 1)
x.shape: torch.Size([1, 1, 3584])
x.stride1: (7168, 3584, 1)
x.stride2: (7168, 1, 3584)
conv_state1: (10752, 1, 3584)
conv_state2: (10752, 1, 3584)
conv_state3: (10752, 1, 3584)
conv_state: torch.Size([1, 3584, 3])
conv1d_out: torch.Size([1, 3584, 1])
rearrange(conv1d_out, 'b d l -> (b l) d') shape: torch.Size([1, 3584])
x_proj_weight shape: torch.Size([144, 3584])
x_dbl: torch.Size([1, 144])
delta_proj_weight @ x_dbl[:, :delta_rank].t(): torch.Size([3584, 1])
conv_state: tensor([[[nan, nan, nan],
         [nan, nan, nan],
         [nan, nan, nan],
         ...,
         [nan, nan, nan],
         [nan, nan, nan],
         [nan, nan, nan]]], device='cuda:0', dtype=torch.bfloat16,
       grad_fn=<AsStridedBackward0>)
ssm_state: tensor([[[[nan, nan, nan,  ..., nan, nan, nan]],

         [[nan, nan, nan,  ..., nan, nan, nan]],

         [[nan, nan, nan,  ..., nan, nan, nan]],

         ...,

         [[nan, nan, nan,  ..., nan, nan, nan]],

         [[nan, nan, nan,  ..., nan, nan, nan]],

         [[nan, nan, nan,  ..., nan, nan, nan]]]], device='cuda:0',
       dtype=torch.bfloat16)
here here
no back here:
not here, not here
conv_state: tensor([[[nan, nan, nan],
         [nan, nan, nan],
         [nan, nan, nan],
         ...,
         [nan, nan, nan],
         [nan, nan, nan],
         [nan, nan, nan]]], device='cuda:0', dtype=torch.bfloat16,
       grad_fn=<AsStridedBackward0>)
ssm_state: tensor([[[[nan, nan, nan,  ..., nan, nan, nan]],

         [[nan, nan, nan,  ..., nan, nan, nan]],

         [[nan, nan, nan,  ..., nan, nan, nan]],

         ...,

         [[nan, nan, nan,  ..., nan, nan, nan]],

         [[nan, nan, nan,  ..., nan, nan, nan]],

         [[nan, nan, nan,  ..., nan, nan, nan]]]], device='cuda:0',
       dtype=torch.bfloat16)
x.stride0: (1, 1, 1)
x.shape: torch.Size([1, 1, 3584])
x.stride1: (7168, 3584, 1)
x.stride2: (7168, 1, 3584)
conv_state1: (10752, 1, 3584)
conv_state2: (10752, 1, 3584)
conv_state3: (10752, 1, 3584)
conv_state: torch.Size([1, 3584, 3])
conv1d_out: torch.Size([1, 3584, 1])
rearrange(conv1d_out, 'b d l -> (b l) d') shape: torch.Size([1, 3584])
x_proj_weight shape: torch.Size([144, 3584])
x_dbl: torch.Size([1, 144])
delta_proj_weight @ x_dbl[:, :delta_rank].t(): torch.Size([3584, 1])
conv_state: tensor([[[nan, nan, nan],
         [nan, nan, nan],
         [nan, nan, nan],
         ...,
         [nan, nan, nan],
         [nan, nan, nan],
         [nan, nan, nan]]], device='cuda:0', dtype=torch.bfloat16,
       grad_fn=<AsStridedBackward0>)
ssm_state: tensor([[[[nan, nan, nan,  ..., nan, nan, nan]],

         [[nan, nan, nan,  ..., nan, nan, nan]],

         [[nan, nan, nan,  ..., nan, nan, nan]],

         ...,

         [[nan, nan, nan,  ..., nan, nan, nan]],

         [[nan, nan, nan,  ..., nan, nan, nan]],

         [[nan, nan, nan,  ..., nan, nan, nan]]]], device='cuda:0',
       dtype=torch.bfloat16)
here here
no back here:
not here, not here
conv_state: tensor([[[nan, nan, nan],
         [nan, nan, nan],
         [nan, nan, nan],
         ...,
         [nan, nan, nan],
         [nan, nan, nan],
         [nan, nan, nan]]], device='cuda:0', dtype=torch.bfloat16,
       grad_fn=<AsStridedBackward0>)
ssm_state: tensor([[[[nan, nan, nan,  ..., nan, nan, nan]],

         [[nan, nan, nan,  ..., nan, nan, nan]],

         [[nan, nan, nan,  ..., nan, nan, nan]],

         ...,

         [[nan, nan, nan,  ..., nan, nan, nan]],

         [[nan, nan, nan,  ..., nan, nan, nan]],

         [[nan, nan, nan,  ..., nan, nan, nan]]]], device='cuda:0',
       dtype=torch.bfloat16)
x.stride0: (1, 1, 1)
x.shape: torch.Size([1, 1, 3584])
x.stride1: (7168, 3584, 1)
x.stride2: (7168, 1, 3584)
conv_state1: (10752, 1, 3584)
conv_state2: (10752, 1, 3584)
conv_state3: (10752, 1, 3584)
conv_state: torch.Size([1, 3584, 3])
conv1d_out: torch.Size([1, 3584, 1])
rearrange(conv1d_out, 'b d l -> (b l) d') shape: torch.Size([1, 3584])
x_proj_weight shape: torch.Size([144, 3584])
x_dbl: torch.Size([1, 144])
delta_proj_weight @ x_dbl[:, :delta_rank].t(): torch.Size([3584, 1])
conv_state: tensor([[[nan, nan, nan],
         [nan, nan, nan],
         [nan, nan, nan],
         ...,
         [nan, nan, nan],
         [nan, nan, nan],
         [nan, nan, nan]]], device='cuda:0', dtype=torch.bfloat16,
       grad_fn=<AsStridedBackward0>)
ssm_state: tensor([[[[nan, nan, nan,  ..., nan, nan, nan]],

         [[nan, nan, nan,  ..., nan, nan, nan]],

         [[nan, nan, nan,  ..., nan, nan, nan]],

         ...,

         [[nan, nan, nan,  ..., nan, nan, nan]],

         [[nan, nan, nan,  ..., nan, nan, nan]],

         [[nan, nan, nan,  ..., nan, nan, nan]]]], device='cuda:0',
       dtype=torch.bfloat16)
here here
no back here:
not here, not here
conv_state: tensor([[[nan, nan, nan],
         [nan, nan, nan],
         [nan, nan, nan],
         ...,
         [nan, nan, nan],
         [nan, nan, nan],
         [nan, nan, nan]]], device='cuda:0', dtype=torch.bfloat16,
       grad_fn=<AsStridedBackward0>)
ssm_state: tensor([[[[nan, nan, nan,  ..., nan, nan, nan]],

         [[nan, nan, nan,  ..., nan, nan, nan]],

         [[nan, nan, nan,  ..., nan, nan, nan]],

         ...,

         [[nan, nan, nan,  ..., nan, nan, nan]],

         [[nan, nan, nan,  ..., nan, nan, nan]],

         [[nan, nan, nan,  ..., nan, nan, nan]]]], device='cuda:0',
       dtype=torch.bfloat16)
x.stride0: (1, 1, 1)
x.shape: torch.Size([1, 1, 3584])
x.stride1: (7168, 3584, 1)
x.stride2: (7168, 1, 3584)
conv_state1: (10752, 1, 3584)
conv_state2: (10752, 1, 3584)
conv_state3: (10752, 1, 3584)
conv_state: torch.Size([1, 3584, 3])
conv1d_out: torch.Size([1, 3584, 1])
rearrange(conv1d_out, 'b d l -> (b l) d') shape: torch.Size([1, 3584])
x_proj_weight shape: torch.Size([144, 3584])
x_dbl: torch.Size([1, 144])
delta_proj_weight @ x_dbl[:, :delta_rank].t(): torch.Size([3584, 1])
conv_state: tensor([[[nan, nan, nan],
         [nan, nan, nan],
         [nan, nan, nan],
         ...,
         [nan, nan, nan],
         [nan, nan, nan],
         [nan, nan, nan]]], device='cuda:0', dtype=torch.bfloat16,
       grad_fn=<AsStridedBackward0>)
ssm_state: tensor([[[[nan, nan, nan,  ..., nan, nan, nan]],

         [[nan, nan, nan,  ..., nan, nan, nan]],

         [[nan, nan, nan,  ..., nan, nan, nan]],

         ...,

         [[nan, nan, nan,  ..., nan, nan, nan]],

         [[nan, nan, nan,  ..., nan, nan, nan]],

         [[nan, nan, nan,  ..., nan, nan, nan]]]], device='cuda:0',
       dtype=torch.bfloat16)
here here
no back here:
not here, not here
conv_state: tensor([[[nan, nan, nan],
         [nan, nan, nan],
         [nan, nan, nan],
         ...,
         [nan, nan, nan],
         [nan, nan, nan],
         [nan, nan, nan]]], device='cuda:0', dtype=torch.bfloat16,
       grad_fn=<AsStridedBackward0>)
ssm_state: tensor([[[[nan, nan, nan,  ..., nan, nan, nan]],

         [[nan, nan, nan,  ..., nan, nan, nan]],

         [[nan, nan, nan,  ..., nan, nan, nan]],

         ...,

         [[nan, nan, nan,  ..., nan, nan, nan]],

         [[nan, nan, nan,  ..., nan, nan, nan]],

         [[nan, nan, nan,  ..., nan, nan, nan]]]], device='cuda:0',
       dtype=torch.bfloat16)
x.stride0: (1, 1, 1)
x.shape: torch.Size([1, 1, 3584])
x.stride1: (7168, 3584, 1)
x.stride2: (7168, 1, 3584)
conv_state1: (10752, 1, 3584)
conv_state2: (10752, 1, 3584)
conv_state3: (10752, 1, 3584)
conv_state: torch.Size([1, 3584, 3])
conv1d_out: torch.Size([1, 3584, 1])
rearrange(conv1d_out, 'b d l -> (b l) d') shape: torch.Size([1, 3584])
x_proj_weight shape: torch.Size([144, 3584])
x_dbl: torch.Size([1, 144])
delta_proj_weight @ x_dbl[:, :delta_rank].t(): torch.Size([3584, 1])
conv_state: tensor([[[nan, nan, nan],
         [nan, nan, nan],
         [nan, nan, nan],
         ...,
         [nan, nan, nan],
         [nan, nan, nan],
         [nan, nan, nan]]], device='cuda:0', dtype=torch.bfloat16,
       grad_fn=<AsStridedBackward0>)
ssm_state: tensor([[[[nan, nan, nan,  ..., nan, nan, nan]],

         [[nan, nan, nan,  ..., nan, nan, nan]],

         [[nan, nan, nan,  ..., nan, nan, nan]],

         ...,

         [[nan, nan, nan,  ..., nan, nan, nan]],

         [[nan, nan, nan,  ..., nan, nan, nan]],

         [[nan, nan, nan,  ..., nan, nan, nan]]]], device='cuda:0',
       dtype=torch.bfloat16)
here here
no back here:
not here, not here
conv_state: tensor([[[nan, nan, nan],
         [nan, nan, nan],
         [nan, nan, nan],
         ...,
         [nan, nan, nan],
         [nan, nan, nan],
         [nan, nan, nan]]], device='cuda:0', dtype=torch.bfloat16,
       grad_fn=<AsStridedBackward0>)
ssm_state: tensor([[[[nan, nan, nan,  ..., nan, nan, nan]],

         [[nan, nan, nan,  ..., nan, nan, nan]],

         [[nan, nan, nan,  ..., nan, nan, nan]],

         ...,

         [[nan, nan, nan,  ..., nan, nan, nan]],

         [[nan, nan, nan,  ..., nan, nan, nan]],

         [[nan, nan, nan,  ..., nan, nan, nan]]]], device='cuda:0',
       dtype=torch.bfloat16)
x.stride0: (1, 1, 1)
x.shape: torch.Size([1, 1, 3584])
x.stride1: (7168, 3584, 1)
x.stride2: (7168, 1, 3584)
conv_state1: (10752, 1, 3584)
conv_state2: (10752, 1, 3584)
conv_state3: (10752, 1, 3584)
conv_state: torch.Size([1, 3584, 3])
conv1d_out: torch.Size([1, 3584, 1])
rearrange(conv1d_out, 'b d l -> (b l) d') shape: torch.Size([1, 3584])
x_proj_weight shape: torch.Size([144, 3584])
x_dbl: torch.Size([1, 144])
delta_proj_weight @ x_dbl[:, :delta_rank].t(): torch.Size([3584, 1])
conv_state: tensor([[[nan, nan, nan],
         [nan, nan, nan],
         [nan, nan, nan],
         ...,
         [nan, nan, nan],
         [nan, nan, nan],
         [nan, nan, nan]]], device='cuda:0', dtype=torch.bfloat16,
       grad_fn=<AsStridedBackward0>)
ssm_state: tensor([[[[nan, nan, nan,  ..., nan, nan, nan]],

         [[nan, nan, nan,  ..., nan, nan, nan]],

         [[nan, nan, nan,  ..., nan, nan, nan]],

         ...,

         [[nan, nan, nan,  ..., nan, nan, nan]],

         [[nan, nan, nan,  ..., nan, nan, nan]],

         [[nan, nan, nan,  ..., nan, nan, nan]]]], device='cuda:0',
       dtype=torch.bfloat16)
here here
no back here:
not here, not here
conv_state: tensor([[[nan, nan, nan],
         [nan, nan, nan],
         [nan, nan, nan],
         ...,
         [nan, nan, nan],
         [nan, nan, nan],
         [nan, nan, nan]]], device='cuda:0', dtype=torch.bfloat16,
       grad_fn=<AsStridedBackward0>)
ssm_state: tensor([[[[nan, nan, nan,  ..., nan, nan, nan]],

         [[nan, nan, nan,  ..., nan, nan, nan]],

         [[nan, nan, nan,  ..., nan, nan, nan]],

         ...,

         [[nan, nan, nan,  ..., nan, nan, nan]],

         [[nan, nan, nan,  ..., nan, nan, nan]],

         [[nan, nan, nan,  ..., nan, nan, nan]]]], device='cuda:0',
       dtype=torch.bfloat16)
x.stride0: (1, 1, 1)
x.shape: torch.Size([1, 1, 3584])
x.stride1: (7168, 3584, 1)
x.stride2: (7168, 1, 3584)
conv_state1: (10752, 1, 3584)
conv_state2: (10752, 1, 3584)
conv_state3: (10752, 1, 3584)
conv_state: torch.Size([1, 3584, 3])
conv1d_out: torch.Size([1, 3584, 1])
rearrange(conv1d_out, 'b d l -> (b l) d') shape: torch.Size([1, 3584])
x_proj_weight shape: torch.Size([144, 3584])
x_dbl: torch.Size([1, 144])
delta_proj_weight @ x_dbl[:, :delta_rank].t(): torch.Size([3584, 1])
conv_state: tensor([[[nan, nan, nan],
         [nan, nan, nan],
         [nan, nan, nan],
         ...,
         [nan, nan, nan],
         [nan, nan, nan],
         [nan, nan, nan]]], device='cuda:0', dtype=torch.bfloat16,
       grad_fn=<AsStridedBackward0>)
ssm_state: tensor([[[[nan, nan, nan,  ..., nan, nan, nan]],

         [[nan, nan, nan,  ..., nan, nan, nan]],

         [[nan, nan, nan,  ..., nan, nan, nan]],

         ...,

         [[nan, nan, nan,  ..., nan, nan, nan]],

         [[nan, nan, nan,  ..., nan, nan, nan]],

         [[nan, nan, nan,  ..., nan, nan, nan]]]], device='cuda:0',
       dtype=torch.bfloat16)
here here
no back here:
not here, not here
conv_state: tensor([[[nan, nan, nan],
         [nan, nan, nan],
         [nan, nan, nan],
         ...,
         [nan, nan, nan],
         [nan, nan, nan],
         [nan, nan, nan]]], device='cuda:0', dtype=torch.bfloat16,
       grad_fn=<AsStridedBackward0>)
ssm_state: tensor([[[[nan, nan, nan,  ..., nan, nan, nan]],

         [[nan, nan, nan,  ..., nan, nan, nan]],

         [[nan, nan, nan,  ..., nan, nan, nan]],

         ...,

         [[nan, nan, nan,  ..., nan, nan, nan]],

         [[nan, nan, nan,  ..., nan, nan, nan]],

         [[nan, nan, nan,  ..., nan, nan, nan]]]], device='cuda:0',
       dtype=torch.bfloat16)
x.stride0: (1, 1, 1)
x.shape: torch.Size([1, 1, 3584])
x.stride1: (7168, 3584, 1)
x.stride2: (7168, 1, 3584)
conv_state1: (10752, 1, 3584)
conv_state2: (10752, 1, 3584)
conv_state3: (10752, 1, 3584)
conv_state: torch.Size([1, 3584, 3])
conv1d_out: torch.Size([1, 3584, 1])
rearrange(conv1d_out, 'b d l -> (b l) d') shape: torch.Size([1, 3584])
x_proj_weight shape: torch.Size([144, 3584])
x_dbl: torch.Size([1, 144])
delta_proj_weight @ x_dbl[:, :delta_rank].t(): torch.Size([3584, 1])
conv_state: tensor([[[nan, nan, nan],
         [nan, nan, nan],
         [nan, nan, nan],
         ...,
         [nan, nan, nan],
         [nan, nan, nan],
         [nan, nan, nan]]], device='cuda:0', dtype=torch.bfloat16,
       grad_fn=<AsStridedBackward0>)
ssm_state: tensor([[[[nan, nan, nan,  ..., nan, nan, nan]],

         [[nan, nan, nan,  ..., nan, nan, nan]],

         [[nan, nan, nan,  ..., nan, nan, nan]],

         ...,

         [[nan, nan, nan,  ..., nan, nan, nan]],

         [[nan, nan, nan,  ..., nan, nan, nan]],

         [[nan, nan, nan,  ..., nan, nan, nan]]]], device='cuda:0',
       dtype=torch.bfloat16)
here here
no back here:
not here, not here
conv_state: tensor([[[nan, nan, nan],
         [nan, nan, nan],
         [nan, nan, nan],
         ...,
         [nan, nan, nan],
         [nan, nan, nan],
         [nan, nan, nan]]], device='cuda:0', dtype=torch.bfloat16,
       grad_fn=<AsStridedBackward0>)
ssm_state: tensor([[[[nan, nan, nan,  ..., nan, nan, nan]],

         [[nan, nan, nan,  ..., nan, nan, nan]],

         [[nan, nan, nan,  ..., nan, nan, nan]],

         ...,

         [[nan, nan, nan,  ..., nan, nan, nan]],

         [[nan, nan, nan,  ..., nan, nan, nan]],

         [[nan, nan, nan,  ..., nan, nan, nan]]]], device='cuda:0',
       dtype=torch.bfloat16)
x.stride0: (1, 1, 1)
x.shape: torch.Size([1, 1, 3584])
x.stride1: (7168, 3584, 1)
x.stride2: (7168, 1, 3584)
conv_state1: (10752, 1, 3584)
conv_state2: (10752, 1, 3584)
conv_state3: (10752, 1, 3584)
conv_state: torch.Size([1, 3584, 3])
conv1d_out: torch.Size([1, 3584, 1])
rearrange(conv1d_out, 'b d l -> (b l) d') shape: torch.Size([1, 3584])
x_proj_weight shape: torch.Size([144, 3584])
x_dbl: torch.Size([1, 144])
delta_proj_weight @ x_dbl[:, :delta_rank].t(): torch.Size([3584, 1])
conv_state: tensor([[[nan, nan, nan],
         [nan, nan, nan],
         [nan, nan, nan],
         ...,
         [nan, nan, nan],
         [nan, nan, nan],
         [nan, nan, nan]]], device='cuda:0', dtype=torch.bfloat16,
       grad_fn=<AsStridedBackward0>)
ssm_state: tensor([[[[nan, nan, nan,  ..., nan, nan, nan]],

         [[nan, nan, nan,  ..., nan, nan, nan]],

         [[nan, nan, nan,  ..., nan, nan, nan]],

         ...,

         [[nan, nan, nan,  ..., nan, nan, nan]],

         [[nan, nan, nan,  ..., nan, nan, nan]],

         [[nan, nan, nan,  ..., nan, nan, nan]]]], device='cuda:0',
       dtype=torch.bfloat16)
here here
no back here:
not here, not here
conv_state: tensor([[[nan, nan, nan],
         [nan, nan, nan],
         [nan, nan, nan],
         ...,
         [nan, nan, nan],
         [nan, nan, nan],
         [nan, nan, nan]]], device='cuda:0', dtype=torch.bfloat16,
       grad_fn=<AsStridedBackward0>)
ssm_state: tensor([[[[nan, nan, nan,  ..., nan, nan, nan]],

         [[nan, nan, nan,  ..., nan, nan, nan]],

         [[nan, nan, nan,  ..., nan, nan, nan]],

         ...,

         [[nan, nan, nan,  ..., nan, nan, nan]],

         [[nan, nan, nan,  ..., nan, nan, nan]],

         [[nan, nan, nan,  ..., nan, nan, nan]]]], device='cuda:0',
       dtype=torch.bfloat16)
x.stride0: (1, 1, 1)
x.shape: torch.Size([1, 1, 3584])
x.stride1: (7168, 3584, 1)
x.stride2: (7168, 1, 3584)
conv_state1: (10752, 1, 3584)
conv_state2: (10752, 1, 3584)
conv_state3: (10752, 1, 3584)
conv_state: torch.Size([1, 3584, 3])
conv1d_out: torch.Size([1, 3584, 1])
rearrange(conv1d_out, 'b d l -> (b l) d') shape: torch.Size([1, 3584])
x_proj_weight shape: torch.Size([144, 3584])
x_dbl: torch.Size([1, 144])
delta_proj_weight @ x_dbl[:, :delta_rank].t(): torch.Size([3584, 1])
conv_state: tensor([[[nan, nan, nan],
         [nan, nan, nan],
         [nan, nan, nan],
         ...,
         [nan, nan, nan],
         [nan, nan, nan],
         [nan, nan, nan]]], device='cuda:0', dtype=torch.bfloat16,
       grad_fn=<AsStridedBackward0>)
ssm_state: tensor([[[[nan, nan, nan,  ..., nan, nan, nan]],

         [[nan, nan, nan,  ..., nan, nan, nan]],

         [[nan, nan, nan,  ..., nan, nan, nan]],

         ...,

         [[nan, nan, nan,  ..., nan, nan, nan]],

         [[nan, nan, nan,  ..., nan, nan, nan]],

         [[nan, nan, nan,  ..., nan, nan, nan]]]], device='cuda:0',
       dtype=torch.bfloat16)
here here
no back here:
not here, not here
conv_state: tensor([[[nan, nan, nan],
         [nan, nan, nan],
         [nan, nan, nan],
         ...,
         [nan, nan, nan],
         [nan, nan, nan],
         [nan, nan, nan]]], device='cuda:0', dtype=torch.bfloat16,
       grad_fn=<AsStridedBackward0>)
ssm_state: tensor([[[[nan, nan, nan,  ..., nan, nan, nan]],

         [[nan, nan, nan,  ..., nan, nan, nan]],

         [[nan, nan, nan,  ..., nan, nan, nan]],

         ...,

         [[nan, nan, nan,  ..., nan, nan, nan]],

         [[nan, nan, nan,  ..., nan, nan, nan]],

         [[nan, nan, nan,  ..., nan, nan, nan]]]], device='cuda:0',
       dtype=torch.bfloat16)
x.stride0: (1, 1, 1)
x.shape: torch.Size([1, 1, 3584])
x.stride1: (7168, 3584, 1)
x.stride2: (7168, 1, 3584)
conv_state1: (10752, 1, 3584)
conv_state2: (10752, 1, 3584)
conv_state3: (10752, 1, 3584)
conv_state: torch.Size([1, 3584, 3])
conv1d_out: torch.Size([1, 3584, 1])
rearrange(conv1d_out, 'b d l -> (b l) d') shape: torch.Size([1, 3584])
x_proj_weight shape: torch.Size([144, 3584])
x_dbl: torch.Size([1, 144])
delta_proj_weight @ x_dbl[:, :delta_rank].t(): torch.Size([3584, 1])
conv_state: tensor([[[nan, nan, nan],
         [nan, nan, nan],
         [nan, nan, nan],
         ...,
         [nan, nan, nan],
         [nan, nan, nan],
         [nan, nan, nan]]], device='cuda:0', dtype=torch.bfloat16,
       grad_fn=<AsStridedBackward0>)
ssm_state: tensor([[[[nan, nan, nan,  ..., nan, nan, nan]],

         [[nan, nan, nan,  ..., nan, nan, nan]],

         [[nan, nan, nan,  ..., nan, nan, nan]],

         ...,

         [[nan, nan, nan,  ..., nan, nan, nan]],

         [[nan, nan, nan,  ..., nan, nan, nan]],

         [[nan, nan, nan,  ..., nan, nan, nan]]]], device='cuda:0',
       dtype=torch.bfloat16)
here here
no back here:
not here, not here
conv_state: tensor([[[nan, nan, nan],
         [nan, nan, nan],
         [nan, nan, nan],
         ...,
         [nan, nan, nan],
         [nan, nan, nan],
         [nan, nan, nan]]], device='cuda:0', dtype=torch.bfloat16,
       grad_fn=<AsStridedBackward0>)
ssm_state: tensor([[[[nan, nan, nan,  ..., nan, nan, nan]],

         [[nan, nan, nan,  ..., nan, nan, nan]],

         [[nan, nan, nan,  ..., nan, nan, nan]],

         ...,

         [[nan, nan, nan,  ..., nan, nan, nan]],

         [[nan, nan, nan,  ..., nan, nan, nan]],

         [[nan, nan, nan,  ..., nan, nan, nan]]]], device='cuda:0',
       dtype=torch.bfloat16)
x.stride0: (1, 1, 1)
x.shape: torch.Size([1, 1, 3584])
x.stride1: (7168, 3584, 1)
x.stride2: (7168, 1, 3584)
conv_state1: (10752, 1, 3584)
conv_state2: (10752, 1, 3584)
conv_state3: (10752, 1, 3584)
conv_state: torch.Size([1, 3584, 3])
conv1d_out: torch.Size([1, 3584, 1])
rearrange(conv1d_out, 'b d l -> (b l) d') shape: torch.Size([1, 3584])
x_proj_weight shape: torch.Size([144, 3584])
x_dbl: torch.Size([1, 144])
delta_proj_weight @ x_dbl[:, :delta_rank].t(): torch.Size([3584, 1])
conv_state: tensor([[[nan, nan, nan],
         [nan, nan, nan],
         [nan, nan, nan],
         ...,
         [nan, nan, nan],
         [nan, nan, nan],
         [nan, nan, nan]]], device='cuda:0', dtype=torch.bfloat16,
       grad_fn=<AsStridedBackward0>)
ssm_state: tensor([[[[nan, nan, nan,  ..., nan, nan, nan]],

         [[nan, nan, nan,  ..., nan, nan, nan]],

         [[nan, nan, nan,  ..., nan, nan, nan]],

         ...,

         [[nan, nan, nan,  ..., nan, nan, nan]],

         [[nan, nan, nan,  ..., nan, nan, nan]],

         [[nan, nan, nan,  ..., nan, nan, nan]]]], device='cuda:0',
       dtype=torch.bfloat16)
here here
no back here:
not here, not here
conv_state: tensor([[[nan, nan, nan],
         [nan, nan, nan],
         [nan, nan, nan],
         ...,
         [nan, nan, nan],
         [nan, nan, nan],
         [nan, nan, nan]]], device='cuda:0', dtype=torch.bfloat16,
       grad_fn=<AsStridedBackward0>)
ssm_state: tensor([[[[nan, nan, nan,  ..., nan, nan, nan]],

         [[nan, nan, nan,  ..., nan, nan, nan]],

         [[nan, nan, nan,  ..., nan, nan, nan]],

         ...,

         [[nan, nan, nan,  ..., nan, nan, nan]],

         [[nan, nan, nan,  ..., nan, nan, nan]],

         [[nan, nan, nan,  ..., nan, nan, nan]]]], device='cuda:0',
       dtype=torch.bfloat16)
x.stride0: (1, 1, 1)
x.shape: torch.Size([1, 1, 3584])
x.stride1: (7168, 3584, 1)
x.stride2: (7168, 1, 3584)
conv_state1: (10752, 1, 3584)
conv_state2: (10752, 1, 3584)
conv_state3: (10752, 1, 3584)
conv_state: torch.Size([1, 3584, 3])
conv1d_out: torch.Size([1, 3584, 1])
rearrange(conv1d_out, 'b d l -> (b l) d') shape: torch.Size([1, 3584])
x_proj_weight shape: torch.Size([144, 3584])
x_dbl: torch.Size([1, 144])
delta_proj_weight @ x_dbl[:, :delta_rank].t(): torch.Size([3584, 1])
conv_state: tensor([[[nan, nan, nan],
         [nan, nan, nan],
         [nan, nan, nan],
         ...,
         [nan, nan, nan],
         [nan, nan, nan],
         [nan, nan, nan]]], device='cuda:0', dtype=torch.bfloat16,
       grad_fn=<AsStridedBackward0>)
ssm_state: tensor([[[[nan, nan, nan,  ..., nan, nan, nan]],

         [[nan, nan, nan,  ..., nan, nan, nan]],

         [[nan, nan, nan,  ..., nan, nan, nan]],

         ...,

         [[nan, nan, nan,  ..., nan, nan, nan]],

         [[nan, nan, nan,  ..., nan, nan, nan]],

         [[nan, nan, nan,  ..., nan, nan, nan]]]], device='cuda:0',
       dtype=torch.bfloat16)
here here
no back here:
not here, not here
conv_state: tensor([[[nan, nan, nan],
         [nan, nan, nan],
         [nan, nan, nan],
         ...,
         [nan, nan, nan],
         [nan, nan, nan],
         [nan, nan, nan]]], device='cuda:0', dtype=torch.bfloat16,
       grad_fn=<AsStridedBackward0>)
ssm_state: tensor([[[[nan, nan, nan,  ..., nan, nan, nan]],

         [[nan, nan, nan,  ..., nan, nan, nan]],

         [[nan, nan, nan,  ..., nan, nan, nan]],

         ...,

         [[nan, nan, nan,  ..., nan, nan, nan]],

         [[nan, nan, nan,  ..., nan, nan, nan]],

         [[nan, nan, nan,  ..., nan, nan, nan]]]], device='cuda:0',
       dtype=torch.bfloat16)
x.stride0: (1, 1, 1)
x.shape: torch.Size([1, 1, 3584])
x.stride1: (7168, 3584, 1)
x.stride2: (7168, 1, 3584)
conv_state1: (10752, 1, 3584)
conv_state2: (10752, 1, 3584)
conv_state3: (10752, 1, 3584)
conv_state: torch.Size([1, 3584, 3])
conv1d_out: torch.Size([1, 3584, 1])
rearrange(conv1d_out, 'b d l -> (b l) d') shape: torch.Size([1, 3584])
x_proj_weight shape: torch.Size([144, 3584])
x_dbl: torch.Size([1, 144])
delta_proj_weight @ x_dbl[:, :delta_rank].t(): torch.Size([3584, 1])
conv_state: tensor([[[nan, nan, nan],
         [nan, nan, nan],
         [nan, nan, nan],
         ...,
         [nan, nan, nan],
         [nan, nan, nan],
         [nan, nan, nan]]], device='cuda:0', dtype=torch.bfloat16,
       grad_fn=<AsStridedBackward0>)
ssm_state: tensor([[[[nan, nan, nan,  ..., nan, nan, nan]],

         [[nan, nan, nan,  ..., nan, nan, nan]],

         [[nan, nan, nan,  ..., nan, nan, nan]],

         ...,

         [[nan, nan, nan,  ..., nan, nan, nan]],

         [[nan, nan, nan,  ..., nan, nan, nan]],

         [[nan, nan, nan,  ..., nan, nan, nan]]]], device='cuda:0',
       dtype=torch.bfloat16)
here here
no back here:
not here, not here
conv_state: tensor([[[nan, nan, nan],
         [nan, nan, nan],
         [nan, nan, nan],
         ...,
         [nan, nan, nan],
         [nan, nan, nan],
         [nan, nan, nan]]], device='cuda:0', dtype=torch.bfloat16,
       grad_fn=<AsStridedBackward0>)
ssm_state: tensor([[[[nan, nan, nan,  ..., nan, nan, nan]],

         [[nan, nan, nan,  ..., nan, nan, nan]],

         [[nan, nan, nan,  ..., nan, nan, nan]],

         ...,

         [[nan, nan, nan,  ..., nan, nan, nan]],

         [[nan, nan, nan,  ..., nan, nan, nan]],

         [[nan, nan, nan,  ..., nan, nan, nan]]]], device='cuda:0',
       dtype=torch.bfloat16)
x.stride0: (1, 1, 1)
x.shape: torch.Size([1, 1, 3584])
x.stride1: (7168, 3584, 1)
x.stride2: (7168, 1, 3584)
conv_state1: (10752, 1, 3584)
conv_state2: (10752, 1, 3584)
conv_state3: (10752, 1, 3584)
conv_state: torch.Size([1, 3584, 3])
conv1d_out: torch.Size([1, 3584, 1])
rearrange(conv1d_out, 'b d l -> (b l) d') shape: torch.Size([1, 3584])
x_proj_weight shape: torch.Size([144, 3584])
x_dbl: torch.Size([1, 144])
delta_proj_weight @ x_dbl[:, :delta_rank].t(): torch.Size([3584, 1])
conv_state: tensor([[[nan, nan, nan],
         [nan, nan, nan],
         [nan, nan, nan],
         ...,
         [nan, nan, nan],
         [nan, nan, nan],
         [nan, nan, nan]]], device='cuda:0', dtype=torch.bfloat16,
       grad_fn=<AsStridedBackward0>)
ssm_state: tensor([[[[nan, nan, nan,  ..., nan, nan, nan]],

         [[nan, nan, nan,  ..., nan, nan, nan]],

         [[nan, nan, nan,  ..., nan, nan, nan]],

         ...,

         [[nan, nan, nan,  ..., nan, nan, nan]],

         [[nan, nan, nan,  ..., nan, nan, nan]],

         [[nan, nan, nan,  ..., nan, nan, nan]]]], device='cuda:0',
       dtype=torch.bfloat16)
here here
no back here:
not here, not here
conv_state: tensor([[[nan, nan, nan],
         [nan, nan, nan],
         [nan, nan, nan],
         ...,
         [nan, nan, nan],
         [nan, nan, nan],
         [nan, nan, nan]]], device='cuda:0', dtype=torch.bfloat16,
       grad_fn=<AsStridedBackward0>)
ssm_state: tensor([[[[nan, nan, nan,  ..., nan, nan, nan]],

         [[nan, nan, nan,  ..., nan, nan, nan]],

         [[nan, nan, nan,  ..., nan, nan, nan]],

         ...,

         [[nan, nan, nan,  ..., nan, nan, nan]],

         [[nan, nan, nan,  ..., nan, nan, nan]],

         [[nan, nan, nan,  ..., nan, nan, nan]]]], device='cuda:0',
       dtype=torch.bfloat16)
x.stride0: (1, 1, 1)
x.shape: torch.Size([1, 1, 3584])
x.stride1: (7168, 3584, 1)
x.stride2: (7168, 1, 3584)
conv_state1: (10752, 1, 3584)
conv_state2: (10752, 1, 3584)
conv_state3: (10752, 1, 3584)
conv_state: torch.Size([1, 3584, 3])
conv1d_out: torch.Size([1, 3584, 1])
rearrange(conv1d_out, 'b d l -> (b l) d') shape: torch.Size([1, 3584])
x_proj_weight shape: torch.Size([144, 3584])
x_dbl: torch.Size([1, 144])
delta_proj_weight @ x_dbl[:, :delta_rank].t(): torch.Size([3584, 1])
conv_state: tensor([[[nan, nan, nan],
         [nan, nan, nan],
         [nan, nan, nan],
         ...,
         [nan, nan, nan],
         [nan, nan, nan],
         [nan, nan, nan]]], device='cuda:0', dtype=torch.bfloat16,
       grad_fn=<AsStridedBackward0>)
ssm_state: tensor([[[[nan, nan, nan,  ..., nan, nan, nan]],

         [[nan, nan, nan,  ..., nan, nan, nan]],

         [[nan, nan, nan,  ..., nan, nan, nan]],

         ...,

         [[nan, nan, nan,  ..., nan, nan, nan]],

         [[nan, nan, nan,  ..., nan, nan, nan]],

         [[nan, nan, nan,  ..., nan, nan, nan]]]], device='cuda:0',
       dtype=torch.bfloat16)
here here
no back here:
not here, not here
conv_state: tensor([[[nan, nan, nan],
         [nan, nan, nan],
         [nan, nan, nan],
         ...,
         [nan, nan, nan],
         [nan, nan, nan],
         [nan, nan, nan]]], device='cuda:0', dtype=torch.bfloat16,
       grad_fn=<AsStridedBackward0>)
ssm_state: tensor([[[[nan, nan, nan,  ..., nan, nan, nan]],

         [[nan, nan, nan,  ..., nan, nan, nan]],

         [[nan, nan, nan,  ..., nan, nan, nan]],

         ...,

         [[nan, nan, nan,  ..., nan, nan, nan]],

         [[nan, nan, nan,  ..., nan, nan, nan]],

         [[nan, nan, nan,  ..., nan, nan, nan]]]], device='cuda:0',
       dtype=torch.bfloat16)
x.stride0: (1, 1, 1)
x.shape: torch.Size([1, 1, 3584])
x.stride1: (7168, 3584, 1)
x.stride2: (7168, 1, 3584)
conv_state1: (10752, 1, 3584)
conv_state2: (10752, 1, 3584)
conv_state3: (10752, 1, 3584)
conv_state: torch.Size([1, 3584, 3])
conv1d_out: torch.Size([1, 3584, 1])
rearrange(conv1d_out, 'b d l -> (b l) d') shape: torch.Size([1, 3584])
x_proj_weight shape: torch.Size([144, 3584])
x_dbl: torch.Size([1, 144])
delta_proj_weight @ x_dbl[:, :delta_rank].t(): torch.Size([3584, 1])
conv_state: tensor([[[nan, nan, nan],
         [nan, nan, nan],
         [nan, nan, nan],
         ...,
         [nan, nan, nan],
         [nan, nan, nan],
         [nan, nan, nan]]], device='cuda:0', dtype=torch.bfloat16,
       grad_fn=<AsStridedBackward0>)
ssm_state: tensor([[[[nan, nan, nan,  ..., nan, nan, nan]],

         [[nan, nan, nan,  ..., nan, nan, nan]],

         [[nan, nan, nan,  ..., nan, nan, nan]],

         ...,

         [[nan, nan, nan,  ..., nan, nan, nan]],

         [[nan, nan, nan,  ..., nan, nan, nan]],

         [[nan, nan, nan,  ..., nan, nan, nan]]]], device='cuda:0',
       dtype=torch.bfloat16)
here here
no back here:
not here, not here
conv_state: tensor([[[nan, nan, nan],
         [nan, nan, nan],
         [nan, nan, nan],
         ...,
         [nan, nan, nan],
         [nan, nan, nan],
         [nan, nan, nan]]], device='cuda:0', dtype=torch.bfloat16,
       grad_fn=<AsStridedBackward0>)
ssm_state: tensor([[[[nan, nan, nan,  ..., nan, nan, nan]],

         [[nan, nan, nan,  ..., nan, nan, nan]],

         [[nan, nan, nan,  ..., nan, nan, nan]],

         ...,

         [[nan, nan, nan,  ..., nan, nan, nan]],

         [[nan, nan, nan,  ..., nan, nan, nan]],

         [[nan, nan, nan,  ..., nan, nan, nan]]]], device='cuda:0',
       dtype=torch.bfloat16)
x.stride0: (1, 1, 1)
x.shape: torch.Size([1, 1, 3584])
x.stride1: (7168, 3584, 1)
x.stride2: (7168, 1, 3584)
conv_state1: (10752, 1, 3584)
conv_state2: (10752, 1, 3584)
conv_state3: (10752, 1, 3584)
conv_state: torch.Size([1, 3584, 3])
conv1d_out: torch.Size([1, 3584, 1])
rearrange(conv1d_out, 'b d l -> (b l) d') shape: torch.Size([1, 3584])
x_proj_weight shape: torch.Size([144, 3584])
x_dbl: torch.Size([1, 144])
delta_proj_weight @ x_dbl[:, :delta_rank].t(): torch.Size([3584, 1])
conv_state: tensor([[[nan, nan, nan],
         [nan, nan, nan],
         [nan, nan, nan],
         ...,
         [nan, nan, nan],
         [nan, nan, nan],
         [nan, nan, nan]]], device='cuda:0', dtype=torch.bfloat16,
       grad_fn=<AsStridedBackward0>)
ssm_state: tensor([[[[nan, nan, nan,  ..., nan, nan, nan]],

         [[nan, nan, nan,  ..., nan, nan, nan]],

         [[nan, nan, nan,  ..., nan, nan, nan]],

         ...,

         [[nan, nan, nan,  ..., nan, nan, nan]],

         [[nan, nan, nan,  ..., nan, nan, nan]],

         [[nan, nan, nan,  ..., nan, nan, nan]]]], device='cuda:0',
       dtype=torch.bfloat16)
here here
no back here:
not here, not here
conv_state: tensor([[[nan, nan, nan],
         [nan, nan, nan],
         [nan, nan, nan],
         ...,
         [nan, nan, nan],
         [nan, nan, nan],
         [nan, nan, nan]]], device='cuda:0', dtype=torch.bfloat16,
       grad_fn=<AsStridedBackward0>)
ssm_state: tensor([[[[nan, nan, nan,  ..., nan, nan, nan]],

         [[nan, nan, nan,  ..., nan, nan, nan]],

         [[nan, nan, nan,  ..., nan, nan, nan]],

         ...,

         [[nan, nan, nan,  ..., nan, nan, nan]],

         [[nan, nan, nan,  ..., nan, nan, nan]],

         [[nan, nan, nan,  ..., nan, nan, nan]]]], device='cuda:0',
       dtype=torch.bfloat16)
x.stride0: (1, 1, 1)
x.shape: torch.Size([1, 1, 3584])
x.stride1: (7168, 3584, 1)
x.stride2: (7168, 1, 3584)
conv_state1: (10752, 1, 3584)
conv_state2: (10752, 1, 3584)
conv_state3: (10752, 1, 3584)
conv_state: torch.Size([1, 3584, 3])
conv1d_out: torch.Size([1, 3584, 1])
rearrange(conv1d_out, 'b d l -> (b l) d') shape: torch.Size([1, 3584])
x_proj_weight shape: torch.Size([144, 3584])
x_dbl: torch.Size([1, 144])
delta_proj_weight @ x_dbl[:, :delta_rank].t(): torch.Size([3584, 1])
conv_state: tensor([[[nan, nan, nan],
         [nan, nan, nan],
         [nan, nan, nan],
         ...,
         [nan, nan, nan],
         [nan, nan, nan],
         [nan, nan, nan]]], device='cuda:0', dtype=torch.bfloat16,
       grad_fn=<AsStridedBackward0>)
ssm_state: tensor([[[[nan, nan, nan,  ..., nan, nan, nan]],

         [[nan, nan, nan,  ..., nan, nan, nan]],

         [[nan, nan, nan,  ..., nan, nan, nan]],

         ...,

         [[nan, nan, nan,  ..., nan, nan, nan]],

         [[nan, nan, nan,  ..., nan, nan, nan]],

         [[nan, nan, nan,  ..., nan, nan, nan]]]], device='cuda:0',
       dtype=torch.bfloat16)
here here
no back here:
not here, not here
conv_state: tensor([[[nan, nan, nan],
         [nan, nan, nan],
         [nan, nan, nan],
         ...,
         [nan, nan, nan],
         [nan, nan, nan],
         [nan, nan, nan]]], device='cuda:0', dtype=torch.bfloat16,
       grad_fn=<AsStridedBackward0>)
ssm_state: tensor([[[[nan, nan, nan,  ..., nan, nan, nan]],

         [[nan, nan, nan,  ..., nan, nan, nan]],

         [[nan, nan, nan,  ..., nan, nan, nan]],

         ...,

         [[nan, nan, nan,  ..., nan, nan, nan]],

         [[nan, nan, nan,  ..., nan, nan, nan]],

         [[nan, nan, nan,  ..., nan, nan, nan]]]], device='cuda:0',
       dtype=torch.bfloat16)
x.stride0: (1, 1, 1)
x.shape: torch.Size([1, 1, 3584])
x.stride1: (7168, 3584, 1)
x.stride2: (7168, 1, 3584)
conv_state1: (10752, 1, 3584)
conv_state2: (10752, 1, 3584)
conv_state3: (10752, 1, 3584)
conv_state: torch.Size([1, 3584, 3])
conv1d_out: torch.Size([1, 3584, 1])
rearrange(conv1d_out, 'b d l -> (b l) d') shape: torch.Size([1, 3584])
x_proj_weight shape: torch.Size([144, 3584])
x_dbl: torch.Size([1, 144])
delta_proj_weight @ x_dbl[:, :delta_rank].t(): torch.Size([3584, 1])
conv_state: tensor([[[nan, nan, nan],
         [nan, nan, nan],
         [nan, nan, nan],
         ...,
         [nan, nan, nan],
         [nan, nan, nan],
         [nan, nan, nan]]], device='cuda:0', dtype=torch.bfloat16,
       grad_fn=<AsStridedBackward0>)
ssm_state: tensor([[[[nan, nan, nan,  ..., nan, nan, nan]],

         [[nan, nan, nan,  ..., nan, nan, nan]],

         [[nan, nan, nan,  ..., nan, nan, nan]],

         ...,

         [[nan, nan, nan,  ..., nan, nan, nan]],

         [[nan, nan, nan,  ..., nan, nan, nan]],

         [[nan, nan, nan,  ..., nan, nan, nan]]]], device='cuda:0',
       dtype=torch.bfloat16)
here here
no back here:
not here, not here
conv_state: tensor([[[nan, nan, nan],
         [nan, nan, nan],
         [nan, nan, nan],
         ...,
         [nan, nan, nan],
         [nan, nan, nan],
         [nan, nan, nan]]], device='cuda:0', dtype=torch.bfloat16,
       grad_fn=<AsStridedBackward0>)
ssm_state: tensor([[[[nan, nan, nan,  ..., nan, nan, nan]],

         [[nan, nan, nan,  ..., nan, nan, nan]],

         [[nan, nan, nan,  ..., nan, nan, nan]],

         ...,

         [[nan, nan, nan,  ..., nan, nan, nan]],

         [[nan, nan, nan,  ..., nan, nan, nan]],

         [[nan, nan, nan,  ..., nan, nan, nan]]]], device='cuda:0',
       dtype=torch.bfloat16)
x.stride0: (1, 1, 1)
x.shape: torch.Size([1, 1, 3584])
x.stride1: (7168, 3584, 1)
x.stride2: (7168, 1, 3584)
conv_state1: (10752, 1, 3584)
conv_state2: (10752, 1, 3584)
conv_state3: (10752, 1, 3584)
conv_state: torch.Size([1, 3584, 3])
conv1d_out: torch.Size([1, 3584, 1])
rearrange(conv1d_out, 'b d l -> (b l) d') shape: torch.Size([1, 3584])
x_proj_weight shape: torch.Size([144, 3584])
x_dbl: torch.Size([1, 144])
delta_proj_weight @ x_dbl[:, :delta_rank].t(): torch.Size([3584, 1])
conv_state: tensor([[[nan, nan, nan],
         [nan, nan, nan],
         [nan, nan, nan],
         ...,
         [nan, nan, nan],
         [nan, nan, nan],
         [nan, nan, nan]]], device='cuda:0', dtype=torch.bfloat16,
       grad_fn=<AsStridedBackward0>)
ssm_state: tensor([[[[nan, nan, nan,  ..., nan, nan, nan]],

         [[nan, nan, nan,  ..., nan, nan, nan]],

         [[nan, nan, nan,  ..., nan, nan, nan]],

         ...,

         [[nan, nan, nan,  ..., nan, nan, nan]],

         [[nan, nan, nan,  ..., nan, nan, nan]],

         [[nan, nan, nan,  ..., nan, nan, nan]]]], device='cuda:0',
       dtype=torch.bfloat16)
here here
no back here:
not here, not here
conv_state: tensor([[[nan, nan, nan],
         [nan, nan, nan],
         [nan, nan, nan],
         ...,
         [nan, nan, nan],
         [nan, nan, nan],
         [nan, nan, nan]]], device='cuda:0', dtype=torch.bfloat16,
       grad_fn=<AsStridedBackward0>)
ssm_state: tensor([[[[nan, nan, nan,  ..., nan, nan, nan]],

         [[nan, nan, nan,  ..., nan, nan, nan]],

         [[nan, nan, nan,  ..., nan, nan, nan]],

         ...,

         [[nan, nan, nan,  ..., nan, nan, nan]],

         [[nan, nan, nan,  ..., nan, nan, nan]],

         [[nan, nan, nan,  ..., nan, nan, nan]]]], device='cuda:0',
       dtype=torch.bfloat16)
x.stride0: (1, 1, 1)
x.shape: torch.Size([1, 1, 3584])
x.stride1: (7168, 3584, 1)
x.stride2: (7168, 1, 3584)
conv_state1: (10752, 1, 3584)
conv_state2: (10752, 1, 3584)
conv_state3: (10752, 1, 3584)
conv_state: torch.Size([1, 3584, 3])
conv1d_out: torch.Size([1, 3584, 1])
rearrange(conv1d_out, 'b d l -> (b l) d') shape: torch.Size([1, 3584])
x_proj_weight shape: torch.Size([144, 3584])
x_dbl: torch.Size([1, 144])
delta_proj_weight @ x_dbl[:, :delta_rank].t(): torch.Size([3584, 1])
conv_state: tensor([[[nan, nan, nan],
         [nan, nan, nan],
         [nan, nan, nan],
         ...,
         [nan, nan, nan],
         [nan, nan, nan],
         [nan, nan, nan]]], device='cuda:0', dtype=torch.bfloat16,
       grad_fn=<AsStridedBackward0>)
ssm_state: tensor([[[[nan, nan, nan,  ..., nan, nan, nan]],

         [[nan, nan, nan,  ..., nan, nan, nan]],

         [[nan, nan, nan,  ..., nan, nan, nan]],

         ...,

         [[nan, nan, nan,  ..., nan, nan, nan]],

         [[nan, nan, nan,  ..., nan, nan, nan]],

         [[nan, nan, nan,  ..., nan, nan, nan]]]], device='cuda:0',
       dtype=torch.bfloat16)
here here
no back here:
not here, not here
conv_state: tensor([[[nan, nan, nan],
         [nan, nan, nan],
         [nan, nan, nan],
         ...,
         [nan, nan, nan],
         [nan, nan, nan],
         [nan, nan, nan]]], device='cuda:0', dtype=torch.bfloat16,
       grad_fn=<AsStridedBackward0>)
ssm_state: tensor([[[[nan, nan, nan,  ..., nan, nan, nan]],

         [[nan, nan, nan,  ..., nan, nan, nan]],

         [[nan, nan, nan,  ..., nan, nan, nan]],

         ...,

         [[nan, nan, nan,  ..., nan, nan, nan]],

         [[nan, nan, nan,  ..., nan, nan, nan]],

         [[nan, nan, nan,  ..., nan, nan, nan]]]], device='cuda:0',
       dtype=torch.bfloat16)
x.stride0: (1, 1, 1)
x.shape: torch.Size([1, 1, 3584])
x.stride1: (7168, 3584, 1)
x.stride2: (7168, 1, 3584)
conv_state1: (10752, 1, 3584)
conv_state2: (10752, 1, 3584)
conv_state3: (10752, 1, 3584)
conv_state: torch.Size([1, 3584, 3])
conv1d_out: torch.Size([1, 3584, 1])
rearrange(conv1d_out, 'b d l -> (b l) d') shape: torch.Size([1, 3584])
x_proj_weight shape: torch.Size([144, 3584])
x_dbl: torch.Size([1, 144])
delta_proj_weight @ x_dbl[:, :delta_rank].t(): torch.Size([3584, 1])
conv_state: tensor([[[nan, nan, nan],
         [nan, nan, nan],
         [nan, nan, nan],
         ...,
         [nan, nan, nan],
         [nan, nan, nan],
         [nan, nan, nan]]], device='cuda:0', dtype=torch.bfloat16,
       grad_fn=<AsStridedBackward0>)
ssm_state: tensor([[[[nan, nan, nan,  ..., nan, nan, nan]],

         [[nan, nan, nan,  ..., nan, nan, nan]],

         [[nan, nan, nan,  ..., nan, nan, nan]],

         ...,

         [[nan, nan, nan,  ..., nan, nan, nan]],

         [[nan, nan, nan,  ..., nan, nan, nan]],

         [[nan, nan, nan,  ..., nan, nan, nan]]]], device='cuda:0',
       dtype=torch.bfloat16)
here here
no back here:
not here, not here
conv_state: tensor([[[nan, nan, nan],
         [nan, nan, nan],
         [nan, nan, nan],
         ...,
         [nan, nan, nan],
         [nan, nan, nan],
         [nan, nan, nan]]], device='cuda:0', dtype=torch.bfloat16,
       grad_fn=<AsStridedBackward0>)
ssm_state: tensor([[[[nan, nan, nan,  ..., nan, nan, nan]],

         [[nan, nan, nan,  ..., nan, nan, nan]],

         [[nan, nan, nan,  ..., nan, nan, nan]],

         ...,

         [[nan, nan, nan,  ..., nan, nan, nan]],

         [[nan, nan, nan,  ..., nan, nan, nan]],

         [[nan, nan, nan,  ..., nan, nan, nan]]]], device='cuda:0',
       dtype=torch.bfloat16)
x.stride0: (1, 1, 1)
x.shape: torch.Size([1, 1, 3584])
x.stride1: (7168, 3584, 1)
x.stride2: (7168, 1, 3584)
conv_state1: (10752, 1, 3584)
conv_state2: (10752, 1, 3584)
conv_state3: (10752, 1, 3584)
conv_state: torch.Size([1, 3584, 3])
conv1d_out: torch.Size([1, 3584, 1])
rearrange(conv1d_out, 'b d l -> (b l) d') shape: torch.Size([1, 3584])
x_proj_weight shape: torch.Size([144, 3584])
x_dbl: torch.Size([1, 144])
delta_proj_weight @ x_dbl[:, :delta_rank].t(): torch.Size([3584, 1])
conv_state: tensor([[[nan, nan, nan],
         [nan, nan, nan],
         [nan, nan, nan],
         ...,
         [nan, nan, nan],
         [nan, nan, nan],
         [nan, nan, nan]]], device='cuda:0', dtype=torch.bfloat16,
       grad_fn=<AsStridedBackward0>)
ssm_state: tensor([[[[nan, nan, nan,  ..., nan, nan, nan]],

         [[nan, nan, nan,  ..., nan, nan, nan]],

         [[nan, nan, nan,  ..., nan, nan, nan]],

         ...,

         [[nan, nan, nan,  ..., nan, nan, nan]],

         [[nan, nan, nan,  ..., nan, nan, nan]],

         [[nan, nan, nan,  ..., nan, nan, nan]]]], device='cuda:0',
       dtype=torch.bfloat16)
here here
no back here:
not here, not here
conv_state: tensor([[[nan, nan, nan],
         [nan, nan, nan],
         [nan, nan, nan],
         ...,
         [nan, nan, nan],
         [nan, nan, nan],
         [nan, nan, nan]]], device='cuda:0', dtype=torch.bfloat16,
       grad_fn=<AsStridedBackward0>)
ssm_state: tensor([[[[nan, nan, nan,  ..., nan, nan, nan]],

         [[nan, nan, nan,  ..., nan, nan, nan]],

         [[nan, nan, nan,  ..., nan, nan, nan]],

         ...,

         [[nan, nan, nan,  ..., nan, nan, nan]],

         [[nan, nan, nan,  ..., nan, nan, nan]],

         [[nan, nan, nan,  ..., nan, nan, nan]]]], device='cuda:0',
       dtype=torch.bfloat16)
x.stride0: (1, 1, 1)
x.shape: torch.Size([1, 1, 3584])
x.stride1: (7168, 3584, 1)
x.stride2: (7168, 1, 3584)
conv_state1: (10752, 1, 3584)
conv_state2: (10752, 1, 3584)
conv_state3: (10752, 1, 3584)
conv_state: torch.Size([1, 3584, 3])
conv1d_out: torch.Size([1, 3584, 1])
rearrange(conv1d_out, 'b d l -> (b l) d') shape: torch.Size([1, 3584])
x_proj_weight shape: torch.Size([144, 3584])
x_dbl: torch.Size([1, 144])
delta_proj_weight @ x_dbl[:, :delta_rank].t(): torch.Size([3584, 1])
conv_state: tensor([[[nan, nan, nan],
         [nan, nan, nan],
         [nan, nan, nan],
         ...,
         [nan, nan, nan],
         [nan, nan, nan],
         [nan, nan, nan]]], device='cuda:0', dtype=torch.bfloat16,
       grad_fn=<AsStridedBackward0>)
ssm_state: tensor([[[[nan, nan, nan,  ..., nan, nan, nan]],

         [[nan, nan, nan,  ..., nan, nan, nan]],

         [[nan, nan, nan,  ..., nan, nan, nan]],

         ...,

         [[nan, nan, nan,  ..., nan, nan, nan]],

         [[nan, nan, nan,  ..., nan, nan, nan]],

         [[nan, nan, nan,  ..., nan, nan, nan]]]], device='cuda:0',
       dtype=torch.bfloat16)
here here
no back here:
not here, not here
conv_state: tensor([[[nan, nan, nan],
         [nan, nan, nan],
         [nan, nan, nan],
         ...,
         [nan, nan, nan],
         [nan, nan, nan],
         [nan, nan, nan]]], device='cuda:0', dtype=torch.bfloat16,
       grad_fn=<AsStridedBackward0>)
ssm_state: tensor([[[[nan, nan, nan,  ..., nan, nan, nan]],

         [[nan, nan, nan,  ..., nan, nan, nan]],

         [[nan, nan, nan,  ..., nan, nan, nan]],

         ...,

         [[nan, nan, nan,  ..., nan, nan, nan]],

         [[nan, nan, nan,  ..., nan, nan, nan]],

         [[nan, nan, nan,  ..., nan, nan, nan]]]], device='cuda:0',
       dtype=torch.bfloat16)
x.stride0: (1, 1, 1)
x.shape: torch.Size([1, 1, 3584])
x.stride1: (7168, 3584, 1)
x.stride2: (7168, 1, 3584)
conv_state1: (10752, 1, 3584)
conv_state2: (10752, 1, 3584)
conv_state3: (10752, 1, 3584)
conv_state: torch.Size([1, 3584, 3])
conv1d_out: torch.Size([1, 3584, 1])
rearrange(conv1d_out, 'b d l -> (b l) d') shape: torch.Size([1, 3584])
x_proj_weight shape: torch.Size([144, 3584])
x_dbl: torch.Size([1, 144])
delta_proj_weight @ x_dbl[:, :delta_rank].t(): torch.Size([3584, 1])
conv_state: tensor([[[nan, nan, nan],
         [nan, nan, nan],
         [nan, nan, nan],
         ...,
         [nan, nan, nan],
         [nan, nan, nan],
         [nan, nan, nan]]], device='cuda:0', dtype=torch.bfloat16,
       grad_fn=<AsStridedBackward0>)
ssm_state: tensor([[[[nan, nan, nan,  ..., nan, nan, nan]],

         [[nan, nan, nan,  ..., nan, nan, nan]],

         [[nan, nan, nan,  ..., nan, nan, nan]],

         ...,

         [[nan, nan, nan,  ..., nan, nan, nan]],

         [[nan, nan, nan,  ..., nan, nan, nan]],

         [[nan, nan, nan,  ..., nan, nan, nan]]]], device='cuda:0',
       dtype=torch.bfloat16)
here here
no back here:
not here, not here
conv_state: tensor([[[nan, nan, nan],
         [nan, nan, nan],
         [nan, nan, nan],
         ...,
         [nan, nan, nan],
         [nan, nan, nan],
         [nan, nan, nan]]], device='cuda:0', dtype=torch.bfloat16,
       grad_fn=<AsStridedBackward0>)
ssm_state: tensor([[[[nan, nan, nan,  ..., nan, nan, nan]],

         [[nan, nan, nan,  ..., nan, nan, nan]],

         [[nan, nan, nan,  ..., nan, nan, nan]],

         ...,

         [[nan, nan, nan,  ..., nan, nan, nan]],

         [[nan, nan, nan,  ..., nan, nan, nan]],

         [[nan, nan, nan,  ..., nan, nan, nan]]]], device='cuda:0',
       dtype=torch.bfloat16)
x.stride0: (1, 1, 1)
x.shape: torch.Size([1, 1, 3584])
x.stride1: (7168, 3584, 1)
x.stride2: (7168, 1, 3584)
conv_state1: (10752, 1, 3584)
conv_state2: (10752, 1, 3584)
conv_state3: (10752, 1, 3584)
conv_state: torch.Size([1, 3584, 3])
conv1d_out: torch.Size([1, 3584, 1])
rearrange(conv1d_out, 'b d l -> (b l) d') shape: torch.Size([1, 3584])
x_proj_weight shape: torch.Size([144, 3584])
x_dbl: torch.Size([1, 144])
delta_proj_weight @ x_dbl[:, :delta_rank].t(): torch.Size([3584, 1])
conv_state: tensor([[[nan, nan, nan],
         [nan, nan, nan],
         [nan, nan, nan],
         ...,
         [nan, nan, nan],
         [nan, nan, nan],
         [nan, nan, nan]]], device='cuda:0', dtype=torch.bfloat16,
       grad_fn=<AsStridedBackward0>)
ssm_state: tensor([[[[nan, nan, nan,  ..., nan, nan, nan]],

         [[nan, nan, nan,  ..., nan, nan, nan]],

         [[nan, nan, nan,  ..., nan, nan, nan]],

         ...,

         [[nan, nan, nan,  ..., nan, nan, nan]],

         [[nan, nan, nan,  ..., nan, nan, nan]],

         [[nan, nan, nan,  ..., nan, nan, nan]]]], device='cuda:0',
       dtype=torch.bfloat16)
tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0]], device='cuda:0')
x1:                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                          
tensor([[0]], device='cuda:0')
x2:  
