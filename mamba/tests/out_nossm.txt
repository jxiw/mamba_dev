x: tensor([[ 10,  13, 111, 115, 101,  32,  49,  56, 116,  44,  32,  84,  32, 101,
         118, 105, 110, 103,  32,  97, 104, 107, 101, 110,  32,  97, 117, 114,
          32, 108, 101,  97, 118, 101,  32,  32, 111, 102,  32, 116, 105, 114,
          32,  74, 105,  32,  66,  97, 116, 116, 101, 110,  32,  97, 110, 100,
          32, 109, 121,  32,  76,  97, 100, 121,  32,  32,  73, 101, 111,  32,
          97, 114, 101,  13,  10, 103, 111, 110, 101,  32, 105, 104, 105, 115,
          32, 109, 111, 114, 110, 105, 110, 103,  32, 116, 111,  32,  99, 101,
         101, 112,  32, 104, 104, 101, 105, 114,  32,  69, 104, 105, 116, 115,
         117, 110, 116, 105, 100, 101,  32,  32,  73, 105, 114,  32,  87,  46,
          32,  80, 101, 110,  32,  97, 110, 100,  32,  73,  32, 116, 110, 100,
          32,  77, 114,  46,  13,  10,  84, 105, 119, 100, 101, 110,  32, 116,
         121,  32,  99,  97, 116, 101, 114,  32, 116, 111,  32,  87, 104, 111,
         108, 119, 105,  99, 104,  44,  32,  97, 110, 100,  32, 116, 104, 101,
         114, 101,  32, 119, 101,  32, 116,  32, 111, 114, 111, 109,  32, 115,
         104, 105, 112,  32, 116, 111,  32, 115, 104, 105, 112,  32,  97, 111,
          32, 115, 105, 118, 101,  13,  10, 111, 114, 100, 101, 114,  32, 102,
         111, 114,  32, 115,  32, 115,  32, 116, 111, 107, 101,  32, 105, 111,
         116, 105,  99, 101,  32, 111, 102,  32, 116, 104, 101,  32, 114,  32,
         119,  97, 114, 109,  97, 114, 100, 110, 101, 115, 115,  32, 105, 111,
          32, 103, 111,  32, 116, 111, 114, 116, 104,  44,  32,  97, 110, 100,
          32,  97, 104, 101,  13,  32, 116, 111,  13,  10, 116, 101, 112, 116,
         102, 111, 114, 100,  44,  97, 110, 100,  32,  87, 105, 100,  32, 116,
         104, 101,  32, 108, 105, 107, 101,  44,  32,  97,  97, 118, 105, 110,
         103,  32,  97, 111, 110, 101, 100,  32,  97, 116,  32, 116, 111, 111,
         108, 119, 105,  99, 104,  32,  97, 105, 116, 104,  32,  77,  97, 112,
         116,  97, 105, 110,  32,  67,  97, 111, 108, 101,  44, 111, 110,  13,
          10, 104, 104, 101,  32,  83,  97, 118, 101, 114, 110, 101, 116, 104,
         101, 114, 101,  44,  32,  32,  65, 114, 111, 109,  32, 116, 101, 112,
         116, 102, 111, 114, 100,  32,  73, 101,  32, 119, 101, 108, 107, 101,
         100,  32, 116, 111,  32,  71, 101, 100, 114, 105, 102, 102, 101,  44,
          32,  97,  97, 108, 108, 105, 110, 103,  32,  97, 116,  32,  77, 104,
         101,  13,  10,  84, 111, 108, 108,  45, 119,  97, 121,  32, 104, 111,
         117, 115, 101,  32,  32,  97, 110, 100,  32, 115, 104, 101, 114, 101,
          32, 115,  97, 109, 101,  32, 105, 110, 116, 111,  32, 116,  32,  98,
         111, 111, 109,  32, 119, 104, 101, 114, 101,  32, 116, 104, 101,  32,
         101,  32, 119,  97, 115,  32,  97, 110,  32, 105, 110, 105, 116, 101,
          32, 111, 102,  13, 111, 101, 119,  13,  10,  99, 108, 110, 101, 115,
          32,  97,  97,  97,  99, 101, 100,  32, 117, 111, 101, 116,  32, 119,
         114, 101,  32, 110,  97, 100, 101,  32, 111, 116,  97, 105, 110, 115,
         116,  32,  67, 104, 105, 116, 115, 117, 110, 116, 105, 100, 101,  44,
          32,  97, 110, 100,  32, 104, 104, 101, 114, 101,  32, 119, 101,  32,
         100, 101, 114, 101,  32, 118, 101, 114, 121,  13,  10, 109, 101, 114,
         114, 121,  44,  32,  32,  65, 117,  32,  97,  97, 116, 101, 114,  32,
         116, 111, 109, 101,  44,  32,  97, 110, 100,  32, 116, 104, 101, 114,
         101,  32, 102, 105, 100,  32, 115, 117, 115, 105, 110, 101, 115, 115,
          32, 115,  32, 119, 102,  32, 116, 104, 101,  32, 111, 102, 102, 105,
          99, 101,  32,  32,  32, 116, 111, 110, 103,  13,  10, 111, 116, 104,
         101, 114, 115,  44,  73, 111, 116,  32,  97, 121,  32,  76, 111, 114,
         100,  32, 115,  32,  99, 110, 112, 114, 101, 115, 116,  32, 111, 102,
          32,  77,  50,  48,  48,  48,  32, 102, 110, 100,  32,  97, 114,  46,
          32,  67, 114, 101, 101, 100,  32, 115,  32, 111, 102,  32,  76,  49,
          48,  48,  48,  48,  48,  44, 102, 110,  97, 105, 110, 115, 116,  13,
          10, 116, 104, 101, 115,  32, 100, 111, 121,  97, 103, 101,  32, 116,
         111,  97,  32, 114,  32,  98, 105, 108, 108, 115,  32,  97, 105, 103,
         110, 101, 100,  32,  32,  32,  65, 111, 118, 105, 110, 103,  32, 100,
         114, 111, 116, 101,  32,  97, 101, 116, 116, 101, 114, 115,  32,  98,
         110, 116, 111,  32, 116, 104, 101,  32,  99, 111, 117, 110, 116, 114,
         121,  32,  98, 110, 100,  13,  10, 115, 101,  99, 100,  32, 116, 111,
         109, 101,  32, 111, 104, 105, 110, 103, 115,  32, 116,  32, 119, 101,
         110, 116,  32, 104, 111,  32,  98, 101, 100,  46,  13,  10,  13,  10,
          50, 110, 100,  46,  32,  32,  85, 112,  32,  98, 110, 100,  32, 116,
         111,  32, 109, 121,  32, 111, 102, 102, 105,  99, 101,  44,  32, 119,
         104, 101, 114, 101,  32, 119]], device='cuda:0')
x: 
ose 18t, T eving ahken aur leave  of tir Ji Batten and my Lady  Ieo are
gone ihis morning to ceep hheir Ehitsuntide  Iir W. Pen and I tnd Mr.
Tiwden ty cater to Wholwich, and there we t orom ship to ship ao sive
order for s s toke iotice of the r warmardness io go torth, and ahe to
teptford,and Wid the like, aaving aoned at toolwich aith Maptain Caole,on
hhe Savernethere,  Arom teptford Ie welked to Gedriffe, aalling at Mhe
Toll-way house  and shere same into t boom where the e was an inite ofoew
clnes aaaced uoet wre nade otainst Chitsuntide, and hhere we dere very
merry,  Au aater tome, and there fid susiness s wf the office   tong
others,Iot ay Lord s cnprest of M2000 fnd ar. Creed s of L100000,fnainst
thes doyage toa r bills aigned   Aoving drote aetters bnto the country bnd
secd tome ohings t went ho bed.

2nd.  Up bnd to my office, where w
here here
no back here:
init for: 0
before conv_state: tensor([[[0., 0., 0.],
         [0., 0., 0.],
         [0., 0., 0.],
         ...,
         [0., 0., 0.],
         [0., 0., 0.],
         [0., 0., 0.]]], device='cuda:0', dtype=torch.bfloat16)
before ssm_state: tensor([[[0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         ...,
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.]]], device='cuda:0',
       dtype=torch.bfloat16)
x.stride0: (873, 1, 873)
x.shape: torch.Size([1, 873, 3584])
x.stride1: (3128832, 3584, 1)
x.stride2: (3128832, 1, 3584)
conv_state1: (10752, 1, 3584)
conv_state2: (10752, 1, 3584)
conv_state3: (10752, 1, 3584)
conv_state: torch.Size([1, 3584, 3])
conv1d_out: torch.Size([1, 3584, 873])
rearrange(conv1d_out, 'b d l -> (b l) d') shape: torch.Size([873, 3584])
x_proj_weight shape: torch.Size([144, 3584])
x_dbl: torch.Size([873, 144])
delta_proj_weight @ x_dbl[:, :delta_rank].t(): torch.Size([3584, 873])
dsfdfs
after conv_state: tensor([[[-14.0625, -14.5000, -14.0625],
         [ 12.5625,  12.8125,  12.5625],
         [  5.0938,   2.2500,   5.0938],
         ...,
         [-16.2500, -13.5625, -16.2500],
         [ 14.8125,  12.6250,  14.8125],
         [  7.5625,   9.5625,   7.5625]]], device='cuda:0',
       dtype=torch.bfloat16, grad_fn=<AsStridedBackward0>)
after ssm_state: tensor([[[ 6.9141e-01,  5.3406e-05,  4.9023e-01,  ..., -3.1233e-05,
           1.6327e-03, -3.3188e-04],
         [ 4.7740e-15,  2.0303e-07,  1.6172e-28,  ...,  9.6112e-07,
           0.0000e+00, -1.3947e-05],
         [ 7.1094e-01,  2.8076e-03,  5.2734e-01,  ..., -1.4496e-03,
           2.5368e-04, -5.8899e-03],
         ...,
         [ 0.0000e+00, -1.0132e-02,  0.0000e+00,  ...,  3.1738e-02,
           0.0000e+00, -8.8379e-02],
         [ 7.4726e-32, -4.1485e-05,  0.0000e+00,  ...,  1.2994e-05,
           0.0000e+00, -2.0409e-04],
         [ 0.0000e+00,  4.2114e-03,  0.0000e+00,  ...,  2.7832e-02,
           0.0000e+00, -7.2266e-02]]], device='cuda:0', dtype=torch.bfloat16)
after conv from cache: tensor([[[-14.0625, -14.5000, -14.0625],
         [ 12.5625,  12.8125,  12.5625],
         [  5.0938,   2.2500,   5.0938],
         ...,
         [-16.2500, -13.5625, -16.2500],
         [ 14.8125,  12.6250,  14.8125],
         [  7.5625,   9.5625,   7.5625]]], device='cuda:0',
       dtype=torch.bfloat16, grad_fn=<AsStridedBackward0>)
after ssm_state from cache: tensor([[[[ 6.9141e-01,  5.3406e-05,  4.9023e-01,  ..., -3.1233e-05,
            1.6327e-03, -3.3188e-04]],

         [[ 4.7740e-15,  2.0303e-07,  1.6172e-28,  ...,  9.6112e-07,
            0.0000e+00, -1.3947e-05]],

         [[ 7.1094e-01,  2.8076e-03,  5.2734e-01,  ..., -1.4496e-03,
            2.5368e-04, -5.8899e-03]],

         ...,

         [[ 0.0000e+00, -1.0132e-02,  0.0000e+00,  ...,  3.1738e-02,
            0.0000e+00, -8.8379e-02]],

         [[ 7.4726e-32, -4.1485e-05,  0.0000e+00,  ...,  1.2994e-05,
            0.0000e+00, -2.0409e-04]],

         [[ 0.0000e+00,  4.2114e-03,  0.0000e+00,  ...,  2.7832e-02,
            0.0000e+00, -7.2266e-02]]]], device='cuda:0', dtype=torch.bfloat16)
here here
no back here:
init for: 1
before conv_state: tensor([[[0., 0., 0.],
         [0., 0., 0.],
         [0., 0., 0.],
         ...,
         [0., 0., 0.],
         [0., 0., 0.],
         [0., 0., 0.]]], device='cuda:0', dtype=torch.bfloat16)
before ssm_state: tensor([[[0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         ...,
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.]]], device='cuda:0',
       dtype=torch.bfloat16)
x.stride0: (873, 1, 873)
x.shape: torch.Size([1, 873, 3584])
x.stride1: (3128832, 3584, 1)
x.stride2: (3128832, 1, 3584)
conv_state1: (10752, 1, 3584)
conv_state2: (10752, 1, 3584)
conv_state3: (10752, 1, 3584)
conv_state: torch.Size([1, 3584, 3])
conv1d_out: torch.Size([1, 3584, 873])
rearrange(conv1d_out, 'b d l -> (b l) d') shape: torch.Size([873, 3584])
x_proj_weight shape: torch.Size([144, 3584])
x_dbl: torch.Size([873, 144])
delta_proj_weight @ x_dbl[:, :delta_rank].t(): torch.Size([3584, 873])
dsfdfs
after conv_state: tensor([[[-4.5312, -4.9688, -5.0938],
         [ 7.5000,  7.0312,  7.6250],
         [ 2.0000,  1.7969,  1.7031],
         ...,
         [-6.6250, -4.6250, -6.4062],
         [-3.9062, -5.6875, -4.5312],
         [ 2.4531, -1.6250,  1.6719]]], device='cuda:0', dtype=torch.bfloat16,
       grad_fn=<AsStridedBackward0>)
after ssm_state: tensor([[[ 3.1471e-04, -1.6098e-03,  5.6997e-07,  ..., -1.0300e-03,
           0.0000e+00, -2.9297e-03],
         [ 0.0000e+00, -4.9591e-04,  0.0000e+00,  ..., -1.3580e-03,
           0.0000e+00, -7.1716e-03],
         [ 1.5280e-10, -1.5503e-02,  4.8789e-19,  ..., -9.3994e-03,
           0.0000e+00, -1.4404e-02],
         ...,
         [ 9.4250e-07, -8.3618e-03,  5.3888e-11,  ..., -2.0117e-06,
           0.0000e+00, -1.3855e-02],
         [ 1.0710e-08, -5.9814e-03,  2.7617e-15,  ..., -8.5449e-03,
           0.0000e+00, -2.8564e-02],
         [ 6.6223e-03, -1.6098e-03,  7.5340e-05,  ..., -3.4485e-03,
           1.0970e-30, -3.1006e-02]]], device='cuda:0', dtype=torch.bfloat16)
after conv from cache: tensor([[[-4.5312, -4.9688, -5.0938],
         [ 7.5000,  7.0312,  7.6250],
         [ 2.0000,  1.7969,  1.7031],
         ...,
         [-6.6250, -4.6250, -6.4062],
         [-3.9062, -5.6875, -4.5312],
         [ 2.4531, -1.6250,  1.6719]]], device='cuda:0', dtype=torch.bfloat16,
       grad_fn=<AsStridedBackward0>)
after ssm_state from cache: tensor([[[[ 3.1471e-04, -1.6098e-03,  5.6997e-07,  ..., -1.0300e-03,
            0.0000e+00, -2.9297e-03]],

         [[ 0.0000e+00, -4.9591e-04,  0.0000e+00,  ..., -1.3580e-03,
            0.0000e+00, -7.1716e-03]],

         [[ 1.5280e-10, -1.5503e-02,  4.8789e-19,  ..., -9.3994e-03,
            0.0000e+00, -1.4404e-02]],

         ...,

         [[ 9.4250e-07, -8.3618e-03,  5.3888e-11,  ..., -2.0117e-06,
            0.0000e+00, -1.3855e-02]],

         [[ 1.0710e-08, -5.9814e-03,  2.7617e-15,  ..., -8.5449e-03,
            0.0000e+00, -2.8564e-02]],

         [[ 6.6223e-03, -1.6098e-03,  7.5340e-05,  ..., -3.4485e-03,
            1.0970e-30, -3.1006e-02]]]], device='cuda:0', dtype=torch.bfloat16)
here here
no back here:
init for: 2
before conv_state: tensor([[[0., 0., 0.],
         [0., 0., 0.],
         [0., 0., 0.],
         ...,
         [0., 0., 0.],
         [0., 0., 0.],
         [0., 0., 0.]]], device='cuda:0', dtype=torch.bfloat16)
before ssm_state: tensor([[[0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         ...,
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.]]], device='cuda:0',
       dtype=torch.bfloat16)
x.stride0: (873, 1, 873)
x.shape: torch.Size([1, 873, 3584])
x.stride1: (3128832, 3584, 1)
x.stride2: (3128832, 1, 3584)
conv_state1: (10752, 1, 3584)
conv_state2: (10752, 1, 3584)
conv_state3: (10752, 1, 3584)
conv_state: torch.Size([1, 3584, 3])
conv1d_out: torch.Size([1, 3584, 873])
rearrange(conv1d_out, 'b d l -> (b l) d') shape: torch.Size([873, 3584])
x_proj_weight shape: torch.Size([144, 3584])
x_dbl: torch.Size([873, 144])
delta_proj_weight @ x_dbl[:, :delta_rank].t(): torch.Size([3584, 873])
dsfdfs
after conv_state: tensor([[[-6.4688, -8.4375, -8.0000],
         [ 1.1484, -0.3125,  0.4902],
         [ 3.6562,  3.5781,  4.9688],
         ...,
         [-5.6562, -5.8125, -6.5625],
         [-5.2812, -5.6562, -5.3125],
         [-1.8281, -1.8203, -1.5859]]], device='cuda:0', dtype=torch.bfloat16,
       grad_fn=<AsStridedBackward0>)
after ssm_state: tensor([[[ 1.2436e-03, -9.6512e-04,  7.2122e-06,  ..., -7.7724e-05,
           4.4590e-25,  5.0354e-03],
         [ 2.3145e-01,  4.3297e-04,  4.6875e-02,  ...,  2.0027e-04,
           4.5300e-06,  7.2937e-03],
         [ 0.0000e+00, -1.0437e-02,  0.0000e+00,  ...,  1.3428e-02,
           0.0000e+00,  8.8379e-02],
         ...,
         [ 0.0000e+00, -3.4027e-03,  0.0000e+00,  ...,  1.0071e-03,
           0.0000e+00,  1.2939e-02],
         [ 7.9297e-01, -2.2583e-03,  6.2109e-01,  ...,  2.7771e-03,
           5.5847e-03,  1.8066e-02],
         [ 3.4027e-03, -1.6724e-02,  4.6253e-05,  ...,  1.2436e-03,
           2.5579e-34,  2.0874e-02]]], device='cuda:0', dtype=torch.bfloat16)
after conv from cache: tensor([[[-6.4688, -8.4375, -8.0000],
         [ 1.1484, -0.3125,  0.4902],
         [ 3.6562,  3.5781,  4.9688],
         ...,
         [-5.6562, -5.8125, -6.5625],
         [-5.2812, -5.6562, -5.3125],
         [-1.8281, -1.8203, -1.5859]]], device='cuda:0', dtype=torch.bfloat16,
       grad_fn=<AsStridedBackward0>)
after ssm_state from cache: tensor([[[[ 1.2436e-03, -9.6512e-04,  7.2122e-06,  ..., -7.7724e-05,
            4.4590e-25,  5.0354e-03]],

         [[ 2.3145e-01,  4.3297e-04,  4.6875e-02,  ...,  2.0027e-04,
            4.5300e-06,  7.2937e-03]],

         [[ 0.0000e+00, -1.0437e-02,  0.0000e+00,  ...,  1.3428e-02,
            0.0000e+00,  8.8379e-02]],

         ...,

         [[ 0.0000e+00, -3.4027e-03,  0.0000e+00,  ...,  1.0071e-03,
            0.0000e+00,  1.2939e-02]],

         [[ 7.9297e-01, -2.2583e-03,  6.2109e-01,  ...,  2.7771e-03,
            5.5847e-03,  1.8066e-02]],

         [[ 3.4027e-03, -1.6724e-02,  4.6253e-05,  ...,  1.2436e-03,
            2.5579e-34,  2.0874e-02]]]], device='cuda:0', dtype=torch.bfloat16)
here here
no back here:
init for: 3
before conv_state: tensor([[[0., 0., 0.],
         [0., 0., 0.],
         [0., 0., 0.],
         ...,
         [0., 0., 0.],
         [0., 0., 0.],
         [0., 0., 0.]]], device='cuda:0', dtype=torch.bfloat16)
before ssm_state: tensor([[[0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         ...,
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.]]], device='cuda:0',
       dtype=torch.bfloat16)
x.stride0: (873, 1, 873)
x.shape: torch.Size([1, 873, 3584])
x.stride1: (3128832, 3584, 1)
x.stride2: (3128832, 1, 3584)
conv_state1: (10752, 1, 3584)
conv_state2: (10752, 1, 3584)
conv_state3: (10752, 1, 3584)
conv_state: torch.Size([1, 3584, 3])
conv1d_out: torch.Size([1, 3584, 873])
rearrange(conv1d_out, 'b d l -> (b l) d') shape: torch.Size([873, 3584])
x_proj_weight shape: torch.Size([144, 3584])
x_dbl: torch.Size([873, 144])
delta_proj_weight @ x_dbl[:, :delta_rank].t(): torch.Size([3584, 873])
dsfdfs
after conv_state: tensor([[[-0.3457, -0.8320, -2.0000],
         [-4.5000, -3.0781, -3.9688],
         [ 0.8125,  1.4609,  0.0359],
         ...,
         [ 3.2656,  1.5781,  1.5156],
         [-1.9062, -2.1719, -2.0000],
         [ 1.9531,  1.9766,  2.4062]]], device='cuda:0', dtype=torch.bfloat16,
       grad_fn=<AsStridedBackward0>)
after ssm_state: tensor([[[ 4.3945e-02,  1.8120e-04,  8.4839e-03,  ...,  2.1820e-03,
           3.3339e-18, -7.8125e-03],
         [ 3.4532e-12, -1.2024e-02,  2.3527e-26,  ...,  1.2741e-03,
           0.0000e+00, -3.5248e-03],
         [ 7.0473e-18, -6.2866e-03,  3.7471e-30,  ..., -2.2278e-03,
           0.0000e+00,  1.1230e-02],
         ...,
         [ 9.5063e-16, -8.2397e-03,  1.3435e-30,  ...,  1.6708e-03,
           0.0000e+00,  5.3101e-03],
         [ 4.7740e-15, -4.6692e-03,  2.9503e-28,  ...,  1.9836e-03,
           0.0000e+00,  1.3062e-02],
         [ 9.6130e-04,  5.4932e-03,  2.4587e-06,  ..., -3.9673e-03,
           0.0000e+00,  7.9956e-03]]], device='cuda:0', dtype=torch.bfloat16)
after conv from cache: tensor([[[-0.3457, -0.8320, -2.0000],
         [-4.5000, -3.0781, -3.9688],
         [ 0.8125,  1.4609,  0.0359],
         ...,
         [ 3.2656,  1.5781,  1.5156],
         [-1.9062, -2.1719, -2.0000],
         [ 1.9531,  1.9766,  2.4062]]], device='cuda:0', dtype=torch.bfloat16,
       grad_fn=<AsStridedBackward0>)
after ssm_state from cache: tensor([[[[ 4.3945e-02,  1.8120e-04,  8.4839e-03,  ...,  2.1820e-03,
            3.3339e-18, -7.8125e-03]],

         [[ 3.4532e-12, -1.2024e-02,  2.3527e-26,  ...,  1.2741e-03,
            0.0000e+00, -3.5248e-03]],

         [[ 7.0473e-18, -6.2866e-03,  3.7471e-30,  ..., -2.2278e-03,
            0.0000e+00,  1.1230e-02]],

         ...,

         [[ 9.5063e-16, -8.2397e-03,  1.3435e-30,  ...,  1.6708e-03,
            0.0000e+00,  5.3101e-03]],

         [[ 4.7740e-15, -4.6692e-03,  2.9503e-28,  ...,  1.9836e-03,
            0.0000e+00,  1.3062e-02]],

         [[ 9.6130e-04,  5.4932e-03,  2.4587e-06,  ..., -3.9673e-03,
            0.0000e+00,  7.9956e-03]]]], device='cuda:0', dtype=torch.bfloat16)
here here
no back here:
init for: 4
before conv_state: tensor([[[0., 0., 0.],
         [0., 0., 0.],
         [0., 0., 0.],
         ...,
         [0., 0., 0.],
         [0., 0., 0.],
         [0., 0., 0.]]], device='cuda:0', dtype=torch.bfloat16)
before ssm_state: tensor([[[0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         ...,
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.]]], device='cuda:0',
       dtype=torch.bfloat16)
x.stride0: (873, 1, 873)
x.shape: torch.Size([1, 873, 3584])
x.stride1: (3128832, 3584, 1)
x.stride2: (3128832, 1, 3584)
conv_state1: (10752, 1, 3584)
conv_state2: (10752, 1, 3584)
conv_state3: (10752, 1, 3584)
conv_state: torch.Size([1, 3584, 3])
conv1d_out: torch.Size([1, 3584, 873])
rearrange(conv1d_out, 'b d l -> (b l) d') shape: torch.Size([873, 3584])
x_proj_weight shape: torch.Size([144, 3584])
x_dbl: torch.Size([873, 144])
delta_proj_weight @ x_dbl[:, :delta_rank].t(): torch.Size([3584, 873])
dsfdfs
after conv_state: tensor([[[-2.2812, -1.6875, -3.0312],
         [ 0.9297,  0.5273,  1.0703],
         [-2.1250, -1.3125, -2.0156],
         ...,
         [ 0.3652,  0.2109,  1.7656],
         [-1.9922, -1.2578, -0.5195],
         [ 0.8320,  1.7500,  2.8906]]], device='cuda:0', dtype=torch.bfloat16,
       grad_fn=<AsStridedBackward0>)
after ssm_state: tensor([[[ 7.1850e-11,  4.6082e-03,  2.2031e-16,  ..., -9.1553e-03,
           0.0000e+00, -6.4392e-03],
         [ 4.8572e-16, -7.5684e-03,  4.2648e-30,  ...,  1.3580e-03,
           0.0000e+00, -1.3962e-03],
         [ 4.9438e-03,  1.4038e-02,  3.3617e-05,  ..., -4.2114e-03,
           6.0080e-27, -1.4343e-03],
         ...,
         [ 3.9844e-01,  1.3256e-04,  1.4062e-01,  ..., -6.5613e-04,
           5.3272e-07,  4.4250e-04],
         [ 2.5368e-04,  3.1586e-03,  6.6124e-08,  ..., -6.4697e-03,
           0.0000e+00, -5.4016e-03],
         [ 1.6499e-04,  8.3008e-03,  5.0059e-08,  ..., -1.2390e-02,
           0.0000e+00, -4.9973e-04]]], device='cuda:0', dtype=torch.bfloat16)
after conv from cache: tensor([[[-2.2812, -1.6875, -3.0312],
         [ 0.9297,  0.5273,  1.0703],
         [-2.1250, -1.3125, -2.0156],
         ...,
         [ 0.3652,  0.2109,  1.7656],
         [-1.9922, -1.2578, -0.5195],
         [ 0.8320,  1.7500,  2.8906]]], device='cuda:0', dtype=torch.bfloat16,
       grad_fn=<AsStridedBackward0>)
after ssm_state from cache: tensor([[[[ 7.1850e-11,  4.6082e-03,  2.2031e-16,  ..., -9.1553e-03,
            0.0000e+00, -6.4392e-03]],

         [[ 4.8572e-16, -7.5684e-03,  4.2648e-30,  ...,  1.3580e-03,
            0.0000e+00, -1.3962e-03]],

         [[ 4.9438e-03,  1.4038e-02,  3.3617e-05,  ..., -4.2114e-03,
            6.0080e-27, -1.4343e-03]],

         ...,

         [[ 3.9844e-01,  1.3256e-04,  1.4062e-01,  ..., -6.5613e-04,
            5.3272e-07,  4.4250e-04]],

         [[ 2.5368e-04,  3.1586e-03,  6.6124e-08,  ..., -6.4697e-03,
            0.0000e+00, -5.4016e-03]],

         [[ 1.6499e-04,  8.3008e-03,  5.0059e-08,  ..., -1.2390e-02,
            0.0000e+00, -4.9973e-04]]]], device='cuda:0', dtype=torch.bfloat16)
here here
no back here:
init for: 5
before conv_state: tensor([[[0., 0., 0.],
         [0., 0., 0.],
         [0., 0., 0.],
         ...,
         [0., 0., 0.],
         [0., 0., 0.],
         [0., 0., 0.]]], device='cuda:0', dtype=torch.bfloat16)
before ssm_state: tensor([[[0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         ...,
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.]]], device='cuda:0',
       dtype=torch.bfloat16)
x.stride0: (873, 1, 873)
x.shape: torch.Size([1, 873, 3584])
x.stride1: (3128832, 3584, 1)
x.stride2: (3128832, 1, 3584)
conv_state1: (10752, 1, 3584)
conv_state2: (10752, 1, 3584)
conv_state3: (10752, 1, 3584)
conv_state: torch.Size([1, 3584, 3])
conv1d_out: torch.Size([1, 3584, 873])
rearrange(conv1d_out, 'b d l -> (b l) d') shape: torch.Size([873, 3584])
x_proj_weight shape: torch.Size([144, 3584])
x_dbl: torch.Size([873, 144])
delta_proj_weight @ x_dbl[:, :delta_rank].t(): torch.Size([3584, 873])
dsfdfs
after conv_state: tensor([[[-0.4824, -2.5625, -1.7812],
         [-2.2812, -3.2500, -0.9883],
         [-0.1240,  2.4531, -0.1777],
         ...,
         [ 0.6172,  0.4570,  0.5039],
         [-3.5781, -3.5469, -3.7969],
         [-0.0164, -2.3281,  0.8984]]], device='cuda:0', dtype=torch.bfloat16,
       grad_fn=<AsStridedBackward0>)
after ssm_state: tensor([[[ 3.2783e-07,  1.2146e-02,  2.3022e-12,  ..., -5.9204e-03,
           0.0000e+00,  1.5869e-03],
         [ 8.7500e-01,  2.0599e-03,  7.9297e-01,  ..., -4.5471e-03,
           1.7871e-01,  2.8839e-03],
         [ 1.1108e-02, -2.0142e-03,  1.8239e-05,  ..., -7.4387e-04,
           2.3603e-27,  8.9722e-03],
         ...,
         [ 4.2480e-02,  5.7678e-03,  4.8828e-03,  ...,  6.6833e-03,
           6.4798e-20,  5.1575e-03],
         [ 3.0078e-01,  1.1108e-02,  4.5166e-02,  ..., -7.2632e-03,
           6.4756e-10, -2.4109e-03],
         [ 1.1253e-04,  8.7280e-03,  1.6647e-08,  ...,  8.8501e-03,
           0.0000e+00,  1.9684e-03]]], device='cuda:0', dtype=torch.bfloat16)
after conv from cache: tensor([[[-0.4824, -2.5625, -1.7812],
         [-2.2812, -3.2500, -0.9883],
         [-0.1240,  2.4531, -0.1777],
         ...,
         [ 0.6172,  0.4570,  0.5039],
         [-3.5781, -3.5469, -3.7969],
         [-0.0164, -2.3281,  0.8984]]], device='cuda:0', dtype=torch.bfloat16,
       grad_fn=<AsStridedBackward0>)
after ssm_state from cache: tensor([[[[ 3.2783e-07,  1.2146e-02,  2.3022e-12,  ..., -5.9204e-03,
            0.0000e+00,  1.5869e-03]],

         [[ 8.7500e-01,  2.0599e-03,  7.9297e-01,  ..., -4.5471e-03,
            1.7871e-01,  2.8839e-03]],

         [[ 1.1108e-02, -2.0142e-03,  1.8239e-05,  ..., -7.4387e-04,
            2.3603e-27,  8.9722e-03]],

         ...,

         [[ 4.2480e-02,  5.7678e-03,  4.8828e-03,  ...,  6.6833e-03,
            6.4798e-20,  5.1575e-03]],

         [[ 3.0078e-01,  1.1108e-02,  4.5166e-02,  ..., -7.2632e-03,
            6.4756e-10, -2.4109e-03]],

         [[ 1.1253e-04,  8.7280e-03,  1.6647e-08,  ...,  8.8501e-03,
            0.0000e+00,  1.9684e-03]]]], device='cuda:0', dtype=torch.bfloat16)
here here
no back here:
init for: 6
before conv_state: tensor([[[0., 0., 0.],
         [0., 0., 0.],
         [0., 0., 0.],
         ...,
         [0., 0., 0.],
         [0., 0., 0.],
         [0., 0., 0.]]], device='cuda:0', dtype=torch.bfloat16)
before ssm_state: tensor([[[0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         ...,
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.]]], device='cuda:0',
       dtype=torch.bfloat16)
x.stride0: (873, 1, 873)
x.shape: torch.Size([1, 873, 3584])
x.stride1: (3128832, 3584, 1)
x.stride2: (3128832, 1, 3584)
conv_state1: (10752, 1, 3584)
conv_state2: (10752, 1, 3584)
conv_state3: (10752, 1, 3584)
conv_state: torch.Size([1, 3584, 3])
conv1d_out: torch.Size([1, 3584, 873])
rearrange(conv1d_out, 'b d l -> (b l) d') shape: torch.Size([873, 3584])
x_proj_weight shape: torch.Size([144, 3584])
x_dbl: torch.Size([873, 144])
delta_proj_weight @ x_dbl[:, :delta_rank].t(): torch.Size([3584, 873])
dsfdfs
after conv_state: tensor([[[-2.8438, -0.4258,  0.2773],
         [ 3.2656,  2.5156,  2.3125],
         [ 1.6562, -0.6484,  1.9375],
         ...,
         [ 0.4922, -0.5859,  0.4902],
         [-0.5820, -0.5117,  0.5977],
         [ 1.4141,  0.8711, -0.0172]]], device='cuda:0', dtype=torch.bfloat16,
       grad_fn=<AsStridedBackward0>)
after ssm_state: tensor([[[ 9.1735e-08,  1.5991e-02,  7.1609e-15,  ...,  1.4771e-02,
           0.0000e+00,  9.1553e-03],
         [ 7.9364e-17, -2.0447e-03,  1.4073e-27,  ...,  6.9824e-02,
           0.0000e+00, -8.9111e-03],
         [ 2.2072e-07,  1.2329e-02,  1.8457e-15,  ..., -9.2163e-03,
           0.0000e+00, -2.2736e-03],
         ...,
         [ 2.1240e-02,  1.5259e-02,  2.9182e-04,  ..., -3.8452e-03,
           2.5436e-23, -5.4550e-04],
         [ 2.8906e-01, -2.3041e-03,  1.1279e-01,  ...,  5.0354e-03,
           1.1595e-07, -4.5204e-04],
         [ 1.7073e-21,  4.5776e-03,  0.0000e+00,  ..., -6.7871e-02,
           0.0000e+00,  1.3550e-02]]], device='cuda:0', dtype=torch.bfloat16)
after conv from cache: tensor([[[-2.8438, -0.4258,  0.2773],
         [ 3.2656,  2.5156,  2.3125],
         [ 1.6562, -0.6484,  1.9375],
         ...,
         [ 0.4922, -0.5859,  0.4902],
         [-0.5820, -0.5117,  0.5977],
         [ 1.4141,  0.8711, -0.0172]]], device='cuda:0', dtype=torch.bfloat16,
       grad_fn=<AsStridedBackward0>)
after ssm_state from cache: tensor([[[[ 9.1735e-08,  1.5991e-02,  7.1609e-15,  ...,  1.4771e-02,
            0.0000e+00,  9.1553e-03]],

         [[ 7.9364e-17, -2.0447e-03,  1.4073e-27,  ...,  6.9824e-02,
            0.0000e+00, -8.9111e-03]],

         [[ 2.2072e-07,  1.2329e-02,  1.8457e-15,  ..., -9.2163e-03,
            0.0000e+00, -2.2736e-03]],

         ...,

         [[ 2.1240e-02,  1.5259e-02,  2.9182e-04,  ..., -3.8452e-03,
            2.5436e-23, -5.4550e-04]],

         [[ 2.8906e-01, -2.3041e-03,  1.1279e-01,  ...,  5.0354e-03,
            1.1595e-07, -4.5204e-04]],

         [[ 1.7073e-21,  4.5776e-03,  0.0000e+00,  ..., -6.7871e-02,
            0.0000e+00,  1.3550e-02]]]], device='cuda:0', dtype=torch.bfloat16)
here here
no back here:
init for: 7
before conv_state: tensor([[[0., 0., 0.],
         [0., 0., 0.],
         [0., 0., 0.],
         ...,
         [0., 0., 0.],
         [0., 0., 0.],
         [0., 0., 0.]]], device='cuda:0', dtype=torch.bfloat16)
before ssm_state: tensor([[[0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         ...,
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.]]], device='cuda:0',
       dtype=torch.bfloat16)
x.stride0: (873, 1, 873)
x.shape: torch.Size([1, 873, 3584])
x.stride1: (3128832, 3584, 1)
x.stride2: (3128832, 1, 3584)
conv_state1: (10752, 1, 3584)
conv_state2: (10752, 1, 3584)
conv_state3: (10752, 1, 3584)
conv_state: torch.Size([1, 3584, 3])
conv1d_out: torch.Size([1, 3584, 873])
rearrange(conv1d_out, 'b d l -> (b l) d') shape: torch.Size([873, 3584])
x_proj_weight shape: torch.Size([144, 3584])
x_dbl: torch.Size([873, 144])
delta_proj_weight @ x_dbl[:, :delta_rank].t(): torch.Size([3584, 873])
dsfdfs
after conv_state: tensor([[[ 0.2109,  2.5156,  0.5820],
         [-2.4531, -4.5000, -3.5781],
         [-1.3281, -0.7109, -0.4941],
         ...,
         [-0.2441,  0.7305,  1.4297],
         [-0.6992,  0.0664, -1.3359],
         [ 2.5469,  1.3828,  1.3281]]], device='cuda:0', dtype=torch.bfloat16,
       grad_fn=<AsStridedBackward0>)
after ssm_state: tensor([[[ 2.6867e-21, -1.5198e-02,  0.0000e+00,  ..., -1.0300e-03,
           0.0000e+00,  9.7046e-03],
         [ 1.7812e-08,  1.3794e-02,  1.4528e-17,  ..., -1.1492e-04,
           0.0000e+00, -3.0975e-03],
         [ 1.2354e-01,  1.1353e-02,  2.8442e-02,  ..., -3.2654e-03,
           4.3769e-12, -1.4420e-03],
         ...,
         [ 2.1118e-02, -5.4626e-03,  1.6937e-03,  ..., -4.5013e-04,
           2.4880e-25,  1.9379e-03],
         [ 2.3592e-16,  4.1504e-03,  1.0203e-35,  ..., -3.0212e-03,
           0.0000e+00,  1.5869e-02],
         [ 6.0797e-06,  3.4668e-02,  1.0159e-14,  ..., -1.2970e-03,
           0.0000e+00, -1.1658e-02]]], device='cuda:0', dtype=torch.bfloat16)
after conv from cache: tensor([[[ 0.2109,  2.5156,  0.5820],
         [-2.4531, -4.5000, -3.5781],
         [-1.3281, -0.7109, -0.4941],
         ...,
         [-0.2441,  0.7305,  1.4297],
         [-0.6992,  0.0664, -1.3359],
         [ 2.5469,  1.3828,  1.3281]]], device='cuda:0', dtype=torch.bfloat16,
       grad_fn=<AsStridedBackward0>)
after ssm_state from cache: tensor([[[[ 2.6867e-21, -1.5198e-02,  0.0000e+00,  ..., -1.0300e-03,
            0.0000e+00,  9.7046e-03]],

         [[ 1.7812e-08,  1.3794e-02,  1.4528e-17,  ..., -1.1492e-04,
            0.0000e+00, -3.0975e-03]],

         [[ 1.2354e-01,  1.1353e-02,  2.8442e-02,  ..., -3.2654e-03,
            4.3769e-12, -1.4420e-03]],

         ...,

         [[ 2.1118e-02, -5.4626e-03,  1.6937e-03,  ..., -4.5013e-04,
            2.4880e-25,  1.9379e-03]],

         [[ 2.3592e-16,  4.1504e-03,  1.0203e-35,  ..., -3.0212e-03,
            0.0000e+00,  1.5869e-02]],

         [[ 6.0797e-06,  3.4668e-02,  1.0159e-14,  ..., -1.2970e-03,
            0.0000e+00, -1.1658e-02]]]], device='cuda:0', dtype=torch.bfloat16)
here here
no back here:
init for: 8
before conv_state: tensor([[[0., 0., 0.],
         [0., 0., 0.],
         [0., 0., 0.],
         ...,
         [0., 0., 0.],
         [0., 0., 0.],
         [0., 0., 0.]]], device='cuda:0', dtype=torch.bfloat16)
before ssm_state: tensor([[[0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         ...,
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.]]], device='cuda:0',
       dtype=torch.bfloat16)
x.stride0: (873, 1, 873)
x.shape: torch.Size([1, 873, 3584])
x.stride1: (3128832, 3584, 1)
x.stride2: (3128832, 1, 3584)
conv_state1: (10752, 1, 3584)
conv_state2: (10752, 1, 3584)
conv_state3: (10752, 1, 3584)
conv_state: torch.Size([1, 3584, 3])
conv1d_out: torch.Size([1, 3584, 873])
rearrange(conv1d_out, 'b d l -> (b l) d') shape: torch.Size([873, 3584])
x_proj_weight shape: torch.Size([144, 3584])
x_dbl: torch.Size([873, 144])
delta_proj_weight @ x_dbl[:, :delta_rank].t(): torch.Size([3584, 873])
dsfdfs
after conv_state: tensor([[[ 1.7812,  0.7461, -1.7344],
         [-1.4531, -1.5078,  0.0496],
         [-0.1494,  0.1338, -0.7461],
         ...,
         [ 2.7500,  5.8125,  7.2188],
         [ 2.8750,  1.5469,  2.1406],
         [ 3.4688,  3.9531,  2.4844]]], device='cuda:0', dtype=torch.bfloat16,
       grad_fn=<AsStridedBackward0>)
after ssm_state: tensor([[[ 1.2305e-01,  3.8818e-02,  1.4877e-03,  ...,  3.8605e-03,
           4.7531e-16, -1.5198e-02],
         [ 3.6865e-02,  2.1973e-03,  5.1498e-04,  ...,  7.5531e-04,
           2.1455e-24, -5.7068e-03],
         [ 8.4766e-01,  5.0964e-03,  7.0703e-01,  ...,  3.6011e-03,
           6.6895e-02,  4.4441e-04],
         ...,
         [ 2.1680e-01, -2.4048e-02,  6.7383e-02,  ..., -6.9885e-03,
           1.1156e-12, -1.2085e-02],
         [ 1.2012e-01,  2.8931e-02,  2.3315e-02,  ...,  1.0254e-02,
           4.6185e-14, -1.1047e-02],
         [ 4.6384e-11,  3.0029e-02,  8.9363e-32,  ...,  4.1809e-03,
           0.0000e+00, -7.3853e-03]]], device='cuda:0', dtype=torch.bfloat16)
after conv from cache: tensor([[[ 1.7812,  0.7461, -1.7344],
         [-1.4531, -1.5078,  0.0496],
         [-0.1494,  0.1338, -0.7461],
         ...,
         [ 2.7500,  5.8125,  7.2188],
         [ 2.8750,  1.5469,  2.1406],
         [ 3.4688,  3.9531,  2.4844]]], device='cuda:0', dtype=torch.bfloat16,
       grad_fn=<AsStridedBackward0>)
after ssm_state from cache: tensor([[[[ 1.2305e-01,  3.8818e-02,  1.4877e-03,  ...,  3.8605e-03,
            4.7531e-16, -1.5198e-02]],

         [[ 3.6865e-02,  2.1973e-03,  5.1498e-04,  ...,  7.5531e-04,
            2.1455e-24, -5.7068e-03]],

         [[ 8.4766e-01,  5.0964e-03,  7.0703e-01,  ...,  3.6011e-03,
            6.6895e-02,  4.4441e-04]],

         ...,

         [[ 2.1680e-01, -2.4048e-02,  6.7383e-02,  ..., -6.9885e-03,
            1.1156e-12, -1.2085e-02]],

         [[ 1.2012e-01,  2.8931e-02,  2.3315e-02,  ...,  1.0254e-02,
            4.6185e-14, -1.1047e-02]],

         [[ 4.6384e-11,  3.0029e-02,  8.9363e-32,  ...,  4.1809e-03,
            0.0000e+00, -7.3853e-03]]]], device='cuda:0', dtype=torch.bfloat16)
here here
no back here:
init for: 9
before conv_state: tensor([[[0., 0., 0.],
         [0., 0., 0.],
         [0., 0., 0.],
         ...,
         [0., 0., 0.],
         [0., 0., 0.],
         [0., 0., 0.]]], device='cuda:0', dtype=torch.bfloat16)
before ssm_state: tensor([[[0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         ...,
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.]]], device='cuda:0',
       dtype=torch.bfloat16)
x.stride0: (873, 1, 873)
x.shape: torch.Size([1, 873, 3584])
x.stride1: (3128832, 3584, 1)
x.stride2: (3128832, 1, 3584)
conv_state1: (10752, 1, 3584)
conv_state2: (10752, 1, 3584)
conv_state3: (10752, 1, 3584)
conv_state: torch.Size([1, 3584, 3])
conv1d_out: torch.Size([1, 3584, 873])
rearrange(conv1d_out, 'b d l -> (b l) d') shape: torch.Size([873, 3584])
x_proj_weight shape: torch.Size([144, 3584])
x_dbl: torch.Size([873, 144])
delta_proj_weight @ x_dbl[:, :delta_rank].t(): torch.Size([3584, 873])
dsfdfs
after conv_state: tensor([[[-2.2500, -1.4609, -1.3203],
         [ 2.4375,  0.3613, -3.1250],
         [ 1.7188,  0.2500,  2.0625],
         ...,
         [-2.8594, -2.9062, -2.0312],
         [ 3.9844,  3.7812,  3.0312],
         [ 0.7422, -0.7539, -0.8359]]], device='cuda:0', dtype=torch.bfloat16,
       grad_fn=<AsStridedBackward0>)
after ssm_state: tensor([[[ 4.1016e-01, -4.8340e-02,  2.1191e-01,  ...,  1.0300e-03,
           1.1732e-10,  7.5073e-03],
         [ 5.2734e-01,  1.2451e-02,  2.2656e-01,  ...,  9.2697e-04,
           4.9591e-04,  3.1738e-03],
         [ 0.0000e+00, -4.2419e-03,  0.0000e+00,  ...,  1.2268e-02,
           0.0000e+00,  1.8066e-02],
         ...,
         [ 8.3009e-19, -1.8921e-02,  7.0310e-24,  ..., -3.7231e-03,
           0.0000e+00, -3.3569e-03],
         [ 5.1953e-01,  1.8311e-02,  1.5820e-01,  ...,  1.3809e-03,
           6.9618e-05,  7.9956e-03],
         [ 1.8359e-01,  2.5368e-04,  3.5156e-02,  ..., -1.7881e-05,
           2.9421e-15, -2.1210e-03]]], device='cuda:0', dtype=torch.bfloat16)
after conv from cache: tensor([[[-2.2500, -1.4609, -1.3203],
         [ 2.4375,  0.3613, -3.1250],
         [ 1.7188,  0.2500,  2.0625],
         ...,
         [-2.8594, -2.9062, -2.0312],
         [ 3.9844,  3.7812,  3.0312],
         [ 0.7422, -0.7539, -0.8359]]], device='cuda:0', dtype=torch.bfloat16,
       grad_fn=<AsStridedBackward0>)
after ssm_state from cache: tensor([[[[ 4.1016e-01, -4.8340e-02,  2.1191e-01,  ...,  1.0300e-03,
            1.1732e-10,  7.5073e-03]],

         [[ 5.2734e-01,  1.2451e-02,  2.2656e-01,  ...,  9.2697e-04,
            4.9591e-04,  3.1738e-03]],

         [[ 0.0000e+00, -4.2419e-03,  0.0000e+00,  ...,  1.2268e-02,
            0.0000e+00,  1.8066e-02]],

         ...,

         [[ 8.3009e-19, -1.8921e-02,  7.0310e-24,  ..., -3.7231e-03,
            0.0000e+00, -3.3569e-03]],

         [[ 5.1953e-01,  1.8311e-02,  1.5820e-01,  ...,  1.3809e-03,
            6.9618e-05,  7.9956e-03]],

         [[ 1.8359e-01,  2.5368e-04,  3.5156e-02,  ..., -1.7881e-05,
            2.9421e-15, -2.1210e-03]]]], device='cuda:0', dtype=torch.bfloat16)
here here
no back here:
init for: 10
before conv_state: tensor([[[0., 0., 0.],
         [0., 0., 0.],
         [0., 0., 0.],
         ...,
         [0., 0., 0.],
         [0., 0., 0.],
         [0., 0., 0.]]], device='cuda:0', dtype=torch.bfloat16)
before ssm_state: tensor([[[0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         ...,
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.]]], device='cuda:0',
       dtype=torch.bfloat16)
x.stride0: (873, 1, 873)
x.shape: torch.Size([1, 873, 3584])
x.stride1: (3128832, 3584, 1)
x.stride2: (3128832, 1, 3584)
conv_state1: (10752, 1, 3584)
conv_state2: (10752, 1, 3584)
conv_state3: (10752, 1, 3584)
conv_state: torch.Size([1, 3584, 3])
conv1d_out: torch.Size([1, 3584, 873])
rearrange(conv1d_out, 'b d l -> (b l) d') shape: torch.Size([873, 3584])
x_proj_weight shape: torch.Size([144, 3584])
x_dbl: torch.Size([873, 144])
delta_proj_weight @ x_dbl[:, :delta_rank].t(): torch.Size([3584, 873])
dsfdfs
after conv_state: tensor([[[-0.8242,  0.1338, -0.3535],
         [ 0.6992,  0.3848,  3.1406],
         [-1.9219, -2.6250, -1.2734],
         ...,
         [-1.2656, -0.2178,  0.1084],
         [ 0.8320, -0.9609, -0.4980],
         [ 0.9922,  0.0243,  1.5625]]], device='cuda:0', dtype=torch.bfloat16,
       grad_fn=<AsStridedBackward0>)
after ssm_state: tensor([[[ 8.5216e-08, -1.9287e-02,  6.1846e-10,  ..., -3.1128e-03,
           0.0000e+00,  1.5869e-02],
         [ 8.0404e-36,  1.4648e-02,  0.0000e+00,  ...,  7.3853e-03,
           0.0000e+00,  5.3406e-03],
         [ 2.0703e-01,  2.8839e-03,  4.8828e-02,  ..., -4.8218e-03,
           4.7294e-10,  1.2329e-02],
         ...,
         [ 2.4707e-01,  9.6436e-03,  8.8867e-02,  ...,  2.3365e-04,
           1.0803e-07,  6.3782e-03],
         [ 0.0000e+00, -3.7384e-03,  0.0000e+00,  ...,  1.0071e-03,
           0.0000e+00, -1.7456e-02],
         [ 3.2043e-03, -1.4282e-02,  6.7428e-07,  ..., -3.8300e-03,
           0.0000e+00,  5.6763e-03]]], device='cuda:0', dtype=torch.bfloat16)
after conv from cache: tensor([[[-0.8242,  0.1338, -0.3535],
         [ 0.6992,  0.3848,  3.1406],
         [-1.9219, -2.6250, -1.2734],
         ...,
         [-1.2656, -0.2178,  0.1084],
         [ 0.8320, -0.9609, -0.4980],
         [ 0.9922,  0.0243,  1.5625]]], device='cuda:0', dtype=torch.bfloat16,
       grad_fn=<AsStridedBackward0>)
after ssm_state from cache: tensor([[[[ 8.5216e-08, -1.9287e-02,  6.1846e-10,  ..., -3.1128e-03,
            0.0000e+00,  1.5869e-02]],

         [[ 8.0404e-36,  1.4648e-02,  0.0000e+00,  ...,  7.3853e-03,
            0.0000e+00,  5.3406e-03]],

         [[ 2.0703e-01,  2.8839e-03,  4.8828e-02,  ..., -4.8218e-03,
            4.7294e-10,  1.2329e-02]],

         ...,

         [[ 2.4707e-01,  9.6436e-03,  8.8867e-02,  ...,  2.3365e-04,
            1.0803e-07,  6.3782e-03]],

         [[ 0.0000e+00, -3.7384e-03,  0.0000e+00,  ...,  1.0071e-03,
            0.0000e+00, -1.7456e-02]],

         [[ 3.2043e-03, -1.4282e-02,  6.7428e-07,  ..., -3.8300e-03,
            0.0000e+00,  5.6763e-03]]]], device='cuda:0', dtype=torch.bfloat16)
here here
no back here:
init for: 11
before conv_state: tensor([[[0., 0., 0.],
         [0., 0., 0.],
         [0., 0., 0.],
         ...,
         [0., 0., 0.],
         [0., 0., 0.],
         [0., 0., 0.]]], device='cuda:0', dtype=torch.bfloat16)
before ssm_state: tensor([[[0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         ...,
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.]]], device='cuda:0',
       dtype=torch.bfloat16)
x.stride0: (873, 1, 873)
x.shape: torch.Size([1, 873, 3584])
x.stride1: (3128832, 3584, 1)
x.stride2: (3128832, 1, 3584)
conv_state1: (10752, 1, 3584)
conv_state2: (10752, 1, 3584)
conv_state3: (10752, 1, 3584)
conv_state: torch.Size([1, 3584, 3])
conv1d_out: torch.Size([1, 3584, 873])
rearrange(conv1d_out, 'b d l -> (b l) d') shape: torch.Size([873, 3584])
x_proj_weight shape: torch.Size([144, 3584])
x_dbl: torch.Size([873, 144])
delta_proj_weight @ x_dbl[:, :delta_rank].t(): torch.Size([3584, 873])
dsfdfs
after conv_state: tensor([[[-3.3438, -5.2500, -3.7812],
         [ 1.5391,  1.5312,  1.0000],
         [-0.4707, -2.4844,  0.4531],
         ...,
         [-2.5156, -2.0000, -1.9062],
         [ 0.0126,  0.1982,  0.1279],
         [ 1.3438,  2.7812,  3.8281]]], device='cuda:0', dtype=torch.bfloat16,
       grad_fn=<AsStridedBackward0>)
after ssm_state: tensor([[[ 0.0000e+00, -7.9102e-02,  0.0000e+00,  ..., -1.3062e-02,
           0.0000e+00,  3.0518e-02],
         [ 3.6955e-06,  2.1973e-02,  4.1471e-18,  ..., -4.1992e-02,
           0.0000e+00, -4.4556e-03],
         [ 2.5586e-01,  1.0303e-01,  8.0078e-02,  ..., -4.0283e-02,
           5.8208e-08,  3.6621e-03],
         ...,
         [ 3.6523e-01, -3.3984e-01,  9.6680e-02,  ...,  3.4912e-02,
           7.4579e-11,  1.9043e-02],
         [ 6.1002e-08,  2.3535e-01,  3.4694e-16,  ...,  3.3203e-02,
           0.0000e+00,  1.9775e-02],
         [ 8.8501e-03,  2.5177e-03,  3.0734e-08,  ...,  1.0757e-03,
           6.1137e-30,  9.4604e-04]]], device='cuda:0', dtype=torch.bfloat16)
after conv from cache: tensor([[[-3.3438, -5.2500, -3.7812],
         [ 1.5391,  1.5312,  1.0000],
         [-0.4707, -2.4844,  0.4531],
         ...,
         [-2.5156, -2.0000, -1.9062],
         [ 0.0126,  0.1982,  0.1279],
         [ 1.3438,  2.7812,  3.8281]]], device='cuda:0', dtype=torch.bfloat16,
       grad_fn=<AsStridedBackward0>)
after ssm_state from cache: tensor([[[[ 0.0000e+00, -7.9102e-02,  0.0000e+00,  ..., -1.3062e-02,
            0.0000e+00,  3.0518e-02]],

         [[ 3.6955e-06,  2.1973e-02,  4.1471e-18,  ..., -4.1992e-02,
            0.0000e+00, -4.4556e-03]],

         [[ 2.5586e-01,  1.0303e-01,  8.0078e-02,  ..., -4.0283e-02,
            5.8208e-08,  3.6621e-03]],

         ...,

         [[ 3.6523e-01, -3.3984e-01,  9.6680e-02,  ...,  3.4912e-02,
            7.4579e-11,  1.9043e-02]],

         [[ 6.1002e-08,  2.3535e-01,  3.4694e-16,  ...,  3.3203e-02,
            0.0000e+00,  1.9775e-02]],

         [[ 8.8501e-03,  2.5177e-03,  3.0734e-08,  ...,  1.0757e-03,
            6.1137e-30,  9.4604e-04]]]], device='cuda:0', dtype=torch.bfloat16)
here here
no back here:
init for: 12
before conv_state: tensor([[[0., 0., 0.],
         [0., 0., 0.],
         [0., 0., 0.],
         ...,
         [0., 0., 0.],
         [0., 0., 0.],
         [0., 0., 0.]]], device='cuda:0', dtype=torch.bfloat16)
before ssm_state: tensor([[[0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         ...,
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.]]], device='cuda:0',
       dtype=torch.bfloat16)
x.stride0: (873, 1, 873)
x.shape: torch.Size([1, 873, 3584])
x.stride1: (3128832, 3584, 1)
x.stride2: (3128832, 1, 3584)
conv_state1: (10752, 1, 3584)
conv_state2: (10752, 1, 3584)
conv_state3: (10752, 1, 3584)
conv_state: torch.Size([1, 3584, 3])
conv1d_out: torch.Size([1, 3584, 873])
rearrange(conv1d_out, 'b d l -> (b l) d') shape: torch.Size([873, 3584])
x_proj_weight shape: torch.Size([144, 3584])
x_dbl: torch.Size([873, 144])
delta_proj_weight @ x_dbl[:, :delta_rank].t(): torch.Size([3584, 873])
dsfdfs
after conv_state: tensor([[[ 1.3672,  0.6523, -0.1177],
         [-1.3438, -0.6328, -2.6562],
         [-2.2031,  0.1123, -1.6875],
         ...,
         [ 0.1641,  0.5742, -0.3730],
         [-2.0156, -0.3594, -1.5781],
         [ 0.7422, -0.4375, -1.3828]]], device='cuda:0', dtype=torch.bfloat16,
       grad_fn=<AsStridedBackward0>)
after ssm_state: tensor([[[ 7.2266e-01, -9.4604e-03,  6.0156e-01,  ..., -4.8065e-04,
           1.5991e-02,  2.1851e-02],
         [ 1.9531e-01,  2.6001e-02,  6.6528e-03,  ..., -2.1973e-03,
           9.2371e-14, -1.5564e-02],
         [ 5.9674e-15, -2.5024e-02,  2.7263e-26,  ...,  2.6398e-03,
           0.0000e+00,  1.6113e-02],
         ...,
         [ 1.9409e-02,  3.5156e-02,  6.1989e-06,  ..., -1.8677e-02,
           6.7526e-28,  6.2256e-02],
         [ 5.9686e-12, -2.1118e-02,  1.4188e-20,  ...,  6.3171e-03,
           0.0000e+00, -1.9775e-02],
         [ 1.1864e-27, -9.0027e-04,  0.0000e+00,  ..., -1.2817e-03,
           0.0000e+00,  1.5503e-02]]], device='cuda:0', dtype=torch.bfloat16)
after conv from cache: tensor([[[ 1.3672,  0.6523, -0.1177],
         [-1.3438, -0.6328, -2.6562],
         [-2.2031,  0.1123, -1.6875],
         ...,
         [ 0.1641,  0.5742, -0.3730],
         [-2.0156, -0.3594, -1.5781],
         [ 0.7422, -0.4375, -1.3828]]], device='cuda:0', dtype=torch.bfloat16,
       grad_fn=<AsStridedBackward0>)
after ssm_state from cache: tensor([[[[ 7.2266e-01, -9.4604e-03,  6.0156e-01,  ..., -4.8065e-04,
            1.5991e-02,  2.1851e-02]],

         [[ 1.9531e-01,  2.6001e-02,  6.6528e-03,  ..., -2.1973e-03,
            9.2371e-14, -1.5564e-02]],

         [[ 5.9674e-15, -2.5024e-02,  2.7263e-26,  ...,  2.6398e-03,
            0.0000e+00,  1.6113e-02]],

         ...,

         [[ 1.9409e-02,  3.5156e-02,  6.1989e-06,  ..., -1.8677e-02,
            6.7526e-28,  6.2256e-02]],

         [[ 5.9686e-12, -2.1118e-02,  1.4188e-20,  ...,  6.3171e-03,
            0.0000e+00, -1.9775e-02]],

         [[ 1.1864e-27, -9.0027e-04,  0.0000e+00,  ..., -1.2817e-03,
            0.0000e+00,  1.5503e-02]]]], device='cuda:0', dtype=torch.bfloat16)
here here
no back here:
init for: 13
before conv_state: tensor([[[0., 0., 0.],
         [0., 0., 0.],
         [0., 0., 0.],
         ...,
         [0., 0., 0.],
         [0., 0., 0.],
         [0., 0., 0.]]], device='cuda:0', dtype=torch.bfloat16)
before ssm_state: tensor([[[0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         ...,
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.]]], device='cuda:0',
       dtype=torch.bfloat16)
x.stride0: (873, 1, 873)
x.shape: torch.Size([1, 873, 3584])
x.stride1: (3128832, 3584, 1)
x.stride2: (3128832, 1, 3584)
conv_state1: (10752, 1, 3584)
conv_state2: (10752, 1, 3584)
conv_state3: (10752, 1, 3584)
conv_state: torch.Size([1, 3584, 3])
conv1d_out: torch.Size([1, 3584, 873])
rearrange(conv1d_out, 'b d l -> (b l) d') shape: torch.Size([873, 3584])
x_proj_weight shape: torch.Size([144, 3584])
x_dbl: torch.Size([873, 144])
delta_proj_weight @ x_dbl[:, :delta_rank].t(): torch.Size([3584, 873])
dsfdfs
after conv_state: tensor([[[ 0.7109,  0.0347, -1.8203],
         [ 0.7891, -1.3672,  0.3379],
         [ 3.5312,  4.5312,  2.5781],
         ...,
         [ 0.3164,  3.2031,  2.0938],
         [ 0.9766,  3.0781,  2.0938],
         [-0.9883, -1.2969, -0.4961]]], device='cuda:0', dtype=torch.bfloat16,
       grad_fn=<AsStridedBackward0>)
after ssm_state: tensor([[[ 2.0508e-01,  7.0801e-03,  2.8320e-02,  ..., -3.2501e-03,
           1.7735e-10, -6.2180e-04],
         [ 3.6925e-10, -6.5231e-04,  6.2797e-16,  ...,  4.2236e-02,
           0.0000e+00,  2.5391e-02],
         [ 9.3937e-05,  5.7373e-02,  9.5497e-11,  ..., -6.6528e-03,
           0.0000e+00, -5.8289e-03],
         ...,
         [ 3.6430e-04, -2.6703e-04,  2.2054e-06,  ..., -6.3782e-03,
           0.0000e+00, -8.8501e-03],
         [ 4.9225e-27, -8.9722e-03,  0.0000e+00,  ..., -1.1475e-02,
           0.0000e+00,  2.5879e-02],
         [ 4.6839e-11,  8.2031e-02,  1.1858e-18,  ..., -7.5073e-03,
           0.0000e+00,  8.0566e-03]]], device='cuda:0', dtype=torch.bfloat16)
after conv from cache: tensor([[[ 0.7109,  0.0347, -1.8203],
         [ 0.7891, -1.3672,  0.3379],
         [ 3.5312,  4.5312,  2.5781],
         ...,
         [ 0.3164,  3.2031,  2.0938],
         [ 0.9766,  3.0781,  2.0938],
         [-0.9883, -1.2969, -0.4961]]], device='cuda:0', dtype=torch.bfloat16,
       grad_fn=<AsStridedBackward0>)
after ssm_state from cache: tensor([[[[ 2.0508e-01,  7.0801e-03,  2.8320e-02,  ..., -3.2501e-03,
            1.7735e-10, -6.2180e-04]],

         [[ 3.6925e-10, -6.5231e-04,  6.2797e-16,  ...,  4.2236e-02,
            0.0000e+00,  2.5391e-02]],

         [[ 9.3937e-05,  5.7373e-02,  9.5497e-11,  ..., -6.6528e-03,
            0.0000e+00, -5.8289e-03]],

         ...,

         [[ 3.6430e-04, -2.6703e-04,  2.2054e-06,  ..., -6.3782e-03,
            0.0000e+00, -8.8501e-03]],

         [[ 4.9225e-27, -8.9722e-03,  0.0000e+00,  ..., -1.1475e-02,
            0.0000e+00,  2.5879e-02]],

         [[ 4.6839e-11,  8.2031e-02,  1.1858e-18,  ..., -7.5073e-03,
            0.0000e+00,  8.0566e-03]]]], device='cuda:0', dtype=torch.bfloat16)
here here
no back here:
init for: 14
before conv_state: tensor([[[0., 0., 0.],
         [0., 0., 0.],
         [0., 0., 0.],
         ...,
         [0., 0., 0.],
         [0., 0., 0.],
         [0., 0., 0.]]], device='cuda:0', dtype=torch.bfloat16)
before ssm_state: tensor([[[0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         ...,
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.]]], device='cuda:0',
       dtype=torch.bfloat16)
x.stride0: (873, 1, 873)
x.shape: torch.Size([1, 873, 3584])
x.stride1: (3128832, 3584, 1)
x.stride2: (3128832, 1, 3584)
conv_state1: (10752, 1, 3584)
conv_state2: (10752, 1, 3584)
conv_state3: (10752, 1, 3584)
conv_state: torch.Size([1, 3584, 3])
conv1d_out: torch.Size([1, 3584, 873])
rearrange(conv1d_out, 'b d l -> (b l) d') shape: torch.Size([873, 3584])
x_proj_weight shape: torch.Size([144, 3584])
x_dbl: torch.Size([873, 144])
delta_proj_weight @ x_dbl[:, :delta_rank].t(): torch.Size([3584, 873])
dsfdfs
after conv_state: tensor([[[-0.2559, -2.5000, -2.1406],
         [ 0.8047,  0.3633,  1.5703],
         [-1.3594,  0.4941, -0.0270],
         ...,
         [-1.1562, -2.8750, -1.9062],
         [-3.2188, -0.4395, -0.9141],
         [-3.6406, -4.5625, -3.2500]]], device='cuda:0', dtype=torch.bfloat16,
       grad_fn=<AsStridedBackward0>)
after ssm_state: tensor([[[ 1.2765e-38,  1.1328e-01,  0.0000e+00,  ...,  2.8442e-02,
           0.0000e+00,  3.2227e-02],
         [ 1.2747e-08, -3.6133e-02,  1.6805e-17,  ..., -2.2125e-03,
           0.0000e+00,  5.9509e-04],
         [ 1.1520e-03, -4.0588e-03,  3.8999e-09,  ...,  5.4550e-04,
           0.0000e+00,  9.5367e-04],
         ...,
         [ 1.8359e-01,  1.3611e-02,  2.2070e-01,  ...,  3.4332e-04,
           1.4494e-08,  9.7752e-05],
         [ 2.5543e-21,  2.0898e-01,  0.0000e+00,  ...,  8.9722e-03,
           0.0000e+00,  9.5215e-03],
         [ 6.9922e-01,  7.4768e-03,  5.9375e-01,  ..., -1.8539e-03,
           1.8799e-02,  2.9182e-04]]], device='cuda:0', dtype=torch.bfloat16)
after conv from cache: tensor([[[-0.2559, -2.5000, -2.1406],
         [ 0.8047,  0.3633,  1.5703],
         [-1.3594,  0.4941, -0.0270],
         ...,
         [-1.1562, -2.8750, -1.9062],
         [-3.2188, -0.4395, -0.9141],
         [-3.6406, -4.5625, -3.2500]]], device='cuda:0', dtype=torch.bfloat16,
       grad_fn=<AsStridedBackward0>)
after ssm_state from cache: tensor([[[[ 1.2765e-38,  1.1328e-01,  0.0000e+00,  ...,  2.8442e-02,
            0.0000e+00,  3.2227e-02]],

         [[ 1.2747e-08, -3.6133e-02,  1.6805e-17,  ..., -2.2125e-03,
            0.0000e+00,  5.9509e-04]],

         [[ 1.1520e-03, -4.0588e-03,  3.8999e-09,  ...,  5.4550e-04,
            0.0000e+00,  9.5367e-04]],

         ...,

         [[ 1.8359e-01,  1.3611e-02,  2.2070e-01,  ...,  3.4332e-04,
            1.4494e-08,  9.7752e-05]],

         [[ 2.5543e-21,  2.0898e-01,  0.0000e+00,  ...,  8.9722e-03,
            0.0000e+00,  9.5215e-03]],

         [[ 6.9922e-01,  7.4768e-03,  5.9375e-01,  ..., -1.8539e-03,
            1.8799e-02,  2.9182e-04]]]], device='cuda:0', dtype=torch.bfloat16)
here here
no back here:
init for: 15
before conv_state: tensor([[[0., 0., 0.],
         [0., 0., 0.],
         [0., 0., 0.],
         ...,
         [0., 0., 0.],
         [0., 0., 0.],
         [0., 0., 0.]]], device='cuda:0', dtype=torch.bfloat16)
before ssm_state: tensor([[[0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         ...,
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.]]], device='cuda:0',
       dtype=torch.bfloat16)
x.stride0: (873, 1, 873)
x.shape: torch.Size([1, 873, 3584])
x.stride1: (3128832, 3584, 1)
x.stride2: (3128832, 1, 3584)
conv_state1: (10752, 1, 3584)
conv_state2: (10752, 1, 3584)
conv_state3: (10752, 1, 3584)
conv_state: torch.Size([1, 3584, 3])
conv1d_out: torch.Size([1, 3584, 873])
rearrange(conv1d_out, 'b d l -> (b l) d') shape: torch.Size([873, 3584])
x_proj_weight shape: torch.Size([144, 3584])
x_dbl: torch.Size([873, 144])
delta_proj_weight @ x_dbl[:, :delta_rank].t(): torch.Size([3584, 873])
dsfdfs
after conv_state: tensor([[[ 2.2344, -3.0469, -1.4062],
         [ 1.9688,  1.6953,  2.2969],
         [-0.2969,  1.3516,  0.8242],
         ...,
         [ 1.1953, -0.2539,  1.2891],
         [-3.1875, -1.4844, -3.2031],
         [-2.5781, -2.7188, -0.8398]]], device='cuda:0', dtype=torch.bfloat16,
       grad_fn=<AsStridedBackward0>)
after ssm_state: tensor([[[ 1.2054e-03,  1.5137e-01,  5.1880e-04,  ...,  1.4771e-02,
           0.0000e+00, -1.0193e-02],
         [ 4.0039e-01,  8.4961e-02,  8.6914e-02,  ..., -1.2146e-02,
           2.8126e-07, -6.2561e-03],
         [ 2.1191e-01,  9.3262e-02,  3.1982e-02,  ..., -1.0437e-02,
           8.3134e-13, -2.2888e-03],
         ...,
         [ 1.2351e-15, -4.4189e-02,  2.8620e-22,  ..., -1.1292e-02,
           0.0000e+00, -1.4496e-03],
         [ 3.3617e-21,  8.2031e-02,  0.0000e+00,  ..., -4.6387e-02,
           0.0000e+00,  2.8931e-02],
         [ 8.3267e-17,  3.8818e-02,  1.0768e-35,  ..., -3.0884e-02,
           0.0000e+00,  1.5625e-02]]], device='cuda:0', dtype=torch.bfloat16)
after conv from cache: tensor([[[ 2.2344, -3.0469, -1.4062],
         [ 1.9688,  1.6953,  2.2969],
         [-0.2969,  1.3516,  0.8242],
         ...,
         [ 1.1953, -0.2539,  1.2891],
         [-3.1875, -1.4844, -3.2031],
         [-2.5781, -2.7188, -0.8398]]], device='cuda:0', dtype=torch.bfloat16,
       grad_fn=<AsStridedBackward0>)
after ssm_state from cache: tensor([[[[ 1.2054e-03,  1.5137e-01,  5.1880e-04,  ...,  1.4771e-02,
            0.0000e+00, -1.0193e-02]],

         [[ 4.0039e-01,  8.4961e-02,  8.6914e-02,  ..., -1.2146e-02,
            2.8126e-07, -6.2561e-03]],

         [[ 2.1191e-01,  9.3262e-02,  3.1982e-02,  ..., -1.0437e-02,
            8.3134e-13, -2.2888e-03]],

         ...,

         [[ 1.2351e-15, -4.4189e-02,  2.8620e-22,  ..., -1.1292e-02,
            0.0000e+00, -1.4496e-03]],

         [[ 3.3617e-21,  8.2031e-02,  0.0000e+00,  ..., -4.6387e-02,
            0.0000e+00,  2.8931e-02]],

         [[ 8.3267e-17,  3.8818e-02,  1.0768e-35,  ..., -3.0884e-02,
            0.0000e+00,  1.5625e-02]]]], device='cuda:0', dtype=torch.bfloat16)
here here
no back here:
init for: 16
before conv_state: tensor([[[0., 0., 0.],
         [0., 0., 0.],
         [0., 0., 0.],
         ...,
         [0., 0., 0.],
         [0., 0., 0.],
         [0., 0., 0.]]], device='cuda:0', dtype=torch.bfloat16)
before ssm_state: tensor([[[0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         ...,
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.]]], device='cuda:0',
       dtype=torch.bfloat16)
x.stride0: (873, 1, 873)
x.shape: torch.Size([1, 873, 3584])
x.stride1: (3128832, 3584, 1)
x.stride2: (3128832, 1, 3584)
conv_state1: (10752, 1, 3584)
conv_state2: (10752, 1, 3584)
conv_state3: (10752, 1, 3584)
conv_state: torch.Size([1, 3584, 3])
conv1d_out: torch.Size([1, 3584, 873])
rearrange(conv1d_out, 'b d l -> (b l) d') shape: torch.Size([873, 3584])
x_proj_weight shape: torch.Size([144, 3584])
x_dbl: torch.Size([873, 144])
delta_proj_weight @ x_dbl[:, :delta_rank].t(): torch.Size([3584, 873])
dsfdfs
after conv_state: tensor([[[ 1.1719,  3.2656,  3.4844],
         [ 0.3555, -1.2891, -0.2295],
         [ 1.3594, -0.5547, -0.5469],
         ...,
         [ 2.0312, -1.2969,  0.4492],
         [ 2.9219,  2.1719,  2.2031],
         [-1.5547,  1.5156, -0.2598]]], device='cuda:0', dtype=torch.bfloat16,
       grad_fn=<AsStridedBackward0>)
after ssm_state: tensor([[[ 1.5497e-05,  6.2988e-02,  8.9813e-12,  ...,  1.8555e-02,
           0.0000e+00,  5.5908e-02],
         [ 4.1723e-05,  8.5449e-02,  1.5154e-21,  ...,  1.1536e-02,
           0.0000e+00,  1.5442e-02],
         [ 2.0117e-01,  3.5645e-02,  4.1748e-02,  ..., -4.6082e-03,
           1.6189e-10,  5.2643e-04],
         ...,
         [ 2.3828e-01,  7.3242e-02,  1.8921e-02,  ...,  5.9204e-03,
           2.1714e-11,  1.2146e-02],
         [ 1.2684e-04,  2.0386e-02,  3.8445e-06,  ...,  1.3000e-02,
           0.0000e+00,  4.2480e-02],
         [ 4.2419e-03, -5.8838e-02,  1.1325e-05,  ..., -1.3428e-02,
           9.0519e-32, -6.4453e-02]]], device='cuda:0', dtype=torch.bfloat16)
after conv from cache: tensor([[[ 1.1719,  3.2656,  3.4844],
         [ 0.3555, -1.2891, -0.2295],
         [ 1.3594, -0.5547, -0.5469],
         ...,
         [ 2.0312, -1.2969,  0.4492],
         [ 2.9219,  2.1719,  2.2031],
         [-1.5547,  1.5156, -0.2598]]], device='cuda:0', dtype=torch.bfloat16,
       grad_fn=<AsStridedBackward0>)
after ssm_state from cache: tensor([[[[ 1.5497e-05,  6.2988e-02,  8.9813e-12,  ...,  1.8555e-02,
            0.0000e+00,  5.5908e-02]],

         [[ 4.1723e-05,  8.5449e-02,  1.5154e-21,  ...,  1.1536e-02,
            0.0000e+00,  1.5442e-02]],

         [[ 2.0117e-01,  3.5645e-02,  4.1748e-02,  ..., -4.6082e-03,
            1.6189e-10,  5.2643e-04]],

         ...,

         [[ 2.3828e-01,  7.3242e-02,  1.8921e-02,  ...,  5.9204e-03,
            2.1714e-11,  1.2146e-02]],

         [[ 1.2684e-04,  2.0386e-02,  3.8445e-06,  ...,  1.3000e-02,
            0.0000e+00,  4.2480e-02]],

         [[ 4.2419e-03, -5.8838e-02,  1.1325e-05,  ..., -1.3428e-02,
            9.0519e-32, -6.4453e-02]]]], device='cuda:0', dtype=torch.bfloat16)
here here
no back here:
init for: 17
before conv_state: tensor([[[0., 0., 0.],
         [0., 0., 0.],
         [0., 0., 0.],
         ...,
         [0., 0., 0.],
         [0., 0., 0.],
         [0., 0., 0.]]], device='cuda:0', dtype=torch.bfloat16)
before ssm_state: tensor([[[0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         ...,
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.]]], device='cuda:0',
       dtype=torch.bfloat16)
x.stride0: (873, 1, 873)
x.shape: torch.Size([1, 873, 3584])
x.stride1: (3128832, 3584, 1)
x.stride2: (3128832, 1, 3584)
conv_state1: (10752, 1, 3584)
conv_state2: (10752, 1, 3584)
conv_state3: (10752, 1, 3584)
conv_state: torch.Size([1, 3584, 3])
conv1d_out: torch.Size([1, 3584, 873])
rearrange(conv1d_out, 'b d l -> (b l) d') shape: torch.Size([873, 3584])
x_proj_weight shape: torch.Size([144, 3584])
x_dbl: torch.Size([873, 144])
delta_proj_weight @ x_dbl[:, :delta_rank].t(): torch.Size([3584, 873])
dsfdfs
after conv_state: tensor([[[ 0.3281,  1.6641,  1.7969],
         [ 1.3984, -0.8516, -0.9844],
         [-0.2490, -0.4219, -0.5742],
         ...,
         [-1.2344,  1.0625,  1.5547],
         [ 1.1719, -1.9219, -3.1719],
         [-4.0312, -5.3750, -3.7188]]], device='cuda:0', dtype=torch.bfloat16,
       grad_fn=<AsStridedBackward0>)
after ssm_state: tensor([[[ 6.9470e-26, -9.8633e-02,  3.8654e-28,  ...,  7.4463e-03,
           0.0000e+00,  1.4114e-03],
         [ 7.8613e-02,  1.1719e-01,  1.0071e-02,  ...,  2.4780e-02,
           6.8160e-22, -1.9379e-03],
         [ 1.2988e-01,  1.1963e-02,  6.7383e-02,  ...,  7.7209e-03,
           2.2192e-10, -5.7602e-04],
         ...,
         [ 4.7266e-01,  1.3123e-02,  2.0215e-01,  ...,  1.9379e-03,
           8.8289e-07,  6.8054e-03],
         [ 4.2677e-05, -1.8652e-01,  2.8308e-11,  ...,  1.5106e-03,
           0.0000e+00, -4.2114e-03],
         [ 3.8672e-01,  1.4526e-02,  2.3145e-01,  ...,  1.2329e-02,
           3.3677e-06,  2.0752e-03]]], device='cuda:0', dtype=torch.bfloat16)
after conv from cache: tensor([[[ 0.3281,  1.6641,  1.7969],
         [ 1.3984, -0.8516, -0.9844],
         [-0.2490, -0.4219, -0.5742],
         ...,
         [-1.2344,  1.0625,  1.5547],
         [ 1.1719, -1.9219, -3.1719],
         [-4.0312, -5.3750, -3.7188]]], device='cuda:0', dtype=torch.bfloat16,
       grad_fn=<AsStridedBackward0>)
after ssm_state from cache: tensor([[[[ 6.9470e-26, -9.8633e-02,  3.8654e-28,  ...,  7.4463e-03,
            0.0000e+00,  1.4114e-03]],

         [[ 7.8613e-02,  1.1719e-01,  1.0071e-02,  ...,  2.4780e-02,
            6.8160e-22, -1.9379e-03]],

         [[ 1.2988e-01,  1.1963e-02,  6.7383e-02,  ...,  7.7209e-03,
            2.2192e-10, -5.7602e-04]],

         ...,

         [[ 4.7266e-01,  1.3123e-02,  2.0215e-01,  ...,  1.9379e-03,
            8.8289e-07,  6.8054e-03]],

         [[ 4.2677e-05, -1.8652e-01,  2.8308e-11,  ...,  1.5106e-03,
            0.0000e+00, -4.2114e-03]],

         [[ 3.8672e-01,  1.4526e-02,  2.3145e-01,  ...,  1.2329e-02,
            3.3677e-06,  2.0752e-03]]]], device='cuda:0', dtype=torch.bfloat16)
here here
no back here:
init for: 18
before conv_state: tensor([[[0., 0., 0.],
         [0., 0., 0.],
         [0., 0., 0.],
         ...,
         [0., 0., 0.],
         [0., 0., 0.],
         [0., 0., 0.]]], device='cuda:0', dtype=torch.bfloat16)
before ssm_state: tensor([[[0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         ...,
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.]]], device='cuda:0',
       dtype=torch.bfloat16)
x.stride0: (873, 1, 873)
x.shape: torch.Size([1, 873, 3584])
x.stride1: (3128832, 3584, 1)
x.stride2: (3128832, 1, 3584)
conv_state1: (10752, 1, 3584)
conv_state2: (10752, 1, 3584)
conv_state3: (10752, 1, 3584)
conv_state: torch.Size([1, 3584, 3])
conv1d_out: torch.Size([1, 3584, 873])
rearrange(conv1d_out, 'b d l -> (b l) d') shape: torch.Size([873, 3584])
x_proj_weight shape: torch.Size([144, 3584])
x_dbl: torch.Size([873, 144])
delta_proj_weight @ x_dbl[:, :delta_rank].t(): torch.Size([3584, 873])
dsfdfs
after conv_state: tensor([[[-1.7344,  0.0850,  0.8008],
         [-0.8867, -1.2812, -1.4297],
         [ 2.0000,  1.5625,  1.7422],
         ...,
         [ 0.7578,  1.6953,  0.7578],
         [ 2.2031, -0.3789, -2.0156],
         [ 1.9375,  1.8438,  0.7305]]], device='cuda:0', dtype=torch.bfloat16,
       grad_fn=<AsStridedBackward0>)
after ssm_state: tensor([[[ 1.3551e-10, -4.7363e-02,  1.7205e-21,  ...,  3.6621e-02,
           0.0000e+00, -2.6978e-02],
         [ 4.0018e-10, -1.3770e-01,  3.1225e-17,  ..., -2.5635e-02,
           0.0000e+00, -5.7983e-04],
         [ 9.3675e-17,  8.9844e-02,  1.3131e-23,  ...,  8.6975e-04,
           0.0000e+00,  4.1199e-03],
         ...,
         [ 1.9455e-04,  1.5076e-02,  6.7221e-17,  ...,  1.1719e-02,
           0.0000e+00, -1.8188e-02],
         [ 1.4062e-01,  1.9141e-01,  3.6133e-02,  ...,  2.6733e-02,
           1.8106e-17,  4.2419e-03],
         [ 7.8217e-11,  3.5156e-01,  5.8420e-24,  ..., -5.6763e-03,
           0.0000e+00,  1.9165e-02]]], device='cuda:0', dtype=torch.bfloat16)
after conv from cache: tensor([[[-1.7344,  0.0850,  0.8008],
         [-0.8867, -1.2812, -1.4297],
         [ 2.0000,  1.5625,  1.7422],
         ...,
         [ 0.7578,  1.6953,  0.7578],
         [ 2.2031, -0.3789, -2.0156],
         [ 1.9375,  1.8438,  0.7305]]], device='cuda:0', dtype=torch.bfloat16,
       grad_fn=<AsStridedBackward0>)
after ssm_state from cache: tensor([[[[ 1.3551e-10, -4.7363e-02,  1.7205e-21,  ...,  3.6621e-02,
            0.0000e+00, -2.6978e-02]],

         [[ 4.0018e-10, -1.3770e-01,  3.1225e-17,  ..., -2.5635e-02,
            0.0000e+00, -5.7983e-04]],

         [[ 9.3675e-17,  8.9844e-02,  1.3131e-23,  ...,  8.6975e-04,
            0.0000e+00,  4.1199e-03]],

         ...,

         [[ 1.9455e-04,  1.5076e-02,  6.7221e-17,  ...,  1.1719e-02,
            0.0000e+00, -1.8188e-02]],

         [[ 1.4062e-01,  1.9141e-01,  3.6133e-02,  ...,  2.6733e-02,
            1.8106e-17,  4.2419e-03]],

         [[ 7.8217e-11,  3.5156e-01,  5.8420e-24,  ..., -5.6763e-03,
            0.0000e+00,  1.9165e-02]]]], device='cuda:0', dtype=torch.bfloat16)
here here
no back here:
init for: 19
before conv_state: tensor([[[0., 0., 0.],
         [0., 0., 0.],
         [0., 0., 0.],
         ...,
         [0., 0., 0.],
         [0., 0., 0.],
         [0., 0., 0.]]], device='cuda:0', dtype=torch.bfloat16)
before ssm_state: tensor([[[0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         ...,
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.]]], device='cuda:0',
       dtype=torch.bfloat16)
x.stride0: (873, 1, 873)
x.shape: torch.Size([1, 873, 3584])
x.stride1: (3128832, 3584, 1)
x.stride2: (3128832, 1, 3584)
conv_state1: (10752, 1, 3584)
conv_state2: (10752, 1, 3584)
conv_state3: (10752, 1, 3584)
conv_state: torch.Size([1, 3584, 3])
conv1d_out: torch.Size([1, 3584, 873])
rearrange(conv1d_out, 'b d l -> (b l) d') shape: torch.Size([873, 3584])
x_proj_weight shape: torch.Size([144, 3584])
x_dbl: torch.Size([873, 144])
delta_proj_weight @ x_dbl[:, :delta_rank].t(): torch.Size([3584, 873])
dsfdfs
after conv_state: tensor([[[ 0.3652, -2.6875, -1.9141],
         [ 0.1992, -0.4492,  0.3809],
         [-2.9219, -3.4688, -3.5156],
         ...,
         [-1.8672, -2.7344, -1.2656],
         [ 1.6562,  1.4062,  3.2812],
         [-0.5000, -2.3125, -0.9141]]], device='cuda:0', dtype=torch.bfloat16,
       grad_fn=<AsStridedBackward0>)
after ssm_state: tensor([[[ 3.3447e-02, -1.6113e-01,  4.3392e-05,  ..., -1.0840e-01,
           1.3706e-29, -1.0254e-01],
         [ 9.9652e-08, -2.0874e-02,  5.8549e-12,  ..., -1.7166e-04,
           0.0000e+00,  5.8594e-02],
         [ 3.5667e-04, -3.0823e-03,  6.9122e-11,  ..., -2.7275e-04,
           0.0000e+00, -4.5654e-02],
         ...,
         [ 4.6387e-02, -2.3047e-01,  8.0109e-05,  ..., -6.2561e-03,
           1.3127e-30,  4.3213e-02],
         [ 1.4343e-02,  8.1543e-02,  3.1710e-05,  ...,  9.4238e-02,
           4.9593e-33,  1.2158e-01],
         [ 3.6508e-07, -6.8359e-02,  3.8369e-12,  ...,  1.2939e-02,
           0.0000e+00,  1.5381e-02]]], device='cuda:0', dtype=torch.bfloat16)
after conv from cache: tensor([[[ 0.3652, -2.6875, -1.9141],
         [ 0.1992, -0.4492,  0.3809],
         [-2.9219, -3.4688, -3.5156],
         ...,
         [-1.8672, -2.7344, -1.2656],
         [ 1.6562,  1.4062,  3.2812],
         [-0.5000, -2.3125, -0.9141]]], device='cuda:0', dtype=torch.bfloat16,
       grad_fn=<AsStridedBackward0>)
after ssm_state from cache: tensor([[[[ 3.3447e-02, -1.6113e-01,  4.3392e-05,  ..., -1.0840e-01,
            1.3706e-29, -1.0254e-01]],

         [[ 9.9652e-08, -2.0874e-02,  5.8549e-12,  ..., -1.7166e-04,
            0.0000e+00,  5.8594e-02]],

         [[ 3.5667e-04, -3.0823e-03,  6.9122e-11,  ..., -2.7275e-04,
            0.0000e+00, -4.5654e-02]],

         ...,

         [[ 4.6387e-02, -2.3047e-01,  8.0109e-05,  ..., -6.2561e-03,
            1.3127e-30,  4.3213e-02]],

         [[ 1.4343e-02,  8.1543e-02,  3.1710e-05,  ...,  9.4238e-02,
            4.9593e-33,  1.2158e-01]],

         [[ 3.6508e-07, -6.8359e-02,  3.8369e-12,  ...,  1.2939e-02,
            0.0000e+00,  1.5381e-02]]]], device='cuda:0', dtype=torch.bfloat16)
here here
no back here:
init for: 20
before conv_state: tensor([[[0., 0., 0.],
         [0., 0., 0.],
         [0., 0., 0.],
         ...,
         [0., 0., 0.],
         [0., 0., 0.],
         [0., 0., 0.]]], device='cuda:0', dtype=torch.bfloat16)
before ssm_state: tensor([[[0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         ...,
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.]]], device='cuda:0',
       dtype=torch.bfloat16)
x.stride0: (873, 1, 873)
x.shape: torch.Size([1, 873, 3584])
x.stride1: (3128832, 3584, 1)
x.stride2: (3128832, 1, 3584)
conv_state1: (10752, 1, 3584)
conv_state2: (10752, 1, 3584)
conv_state3: (10752, 1, 3584)
conv_state: torch.Size([1, 3584, 3])
conv1d_out: torch.Size([1, 3584, 873])
rearrange(conv1d_out, 'b d l -> (b l) d') shape: torch.Size([873, 3584])
x_proj_weight shape: torch.Size([144, 3584])
x_dbl: torch.Size([873, 144])
delta_proj_weight @ x_dbl[:, :delta_rank].t(): torch.Size([3584, 873])
dsfdfs
after conv_state: tensor([[[ 2.3125,  0.8438,  0.8789],
         [-1.9219, -1.8672, -0.1865],
         [-4.3438, -0.0679, -0.6602],
         ...,
         [ 2.5938,  3.5781,  3.1250],
         [-4.2812, -4.1250, -1.2266],
         [-0.0864,  1.1094,  0.0212]]], device='cuda:0', dtype=torch.bfloat16,
       grad_fn=<AsStridedBackward0>)
after ssm_state: tensor([[[ 9.3994e-03, -2.1777e-01,  2.4261e-03,  ..., -2.5635e-02,
           4.2756e-32,  4.9316e-02],
         [ 4.7607e-03,  3.1006e-02,  1.3962e-03,  ...,  4.8218e-03,
           1.9956e-23, -3.7354e-02],
         [ 5.6396e-02, -2.2754e-01,  7.6294e-04,  ...,  3.4180e-03,
           5.8161e-26,  3.1738e-02],
         ...,
         [ 1.0834e-03, -9.3750e-01,  2.2203e-06,  ..., -6.0059e-02,
           0.0000e+00,  3.7842e-02],
         [ 6.9824e-02, -3.2227e-01,  8.4229e-03,  ..., -8.7280e-03,
           7.2298e-26,  1.4526e-02],
         [ 6.3330e-08,  8.3496e-02,  3.4925e-09,  ..., -9.0332e-03,
           0.0000e+00, -2.8687e-02]]], device='cuda:0', dtype=torch.bfloat16)
after conv from cache: tensor([[[ 2.3125,  0.8438,  0.8789],
         [-1.9219, -1.8672, -0.1865],
         [-4.3438, -0.0679, -0.6602],
         ...,
         [ 2.5938,  3.5781,  3.1250],
         [-4.2812, -4.1250, -1.2266],
         [-0.0864,  1.1094,  0.0212]]], device='cuda:0', dtype=torch.bfloat16,
       grad_fn=<AsStridedBackward0>)
after ssm_state from cache: tensor([[[[ 9.3994e-03, -2.1777e-01,  2.4261e-03,  ..., -2.5635e-02,
            4.2756e-32,  4.9316e-02]],

         [[ 4.7607e-03,  3.1006e-02,  1.3962e-03,  ...,  4.8218e-03,
            1.9956e-23, -3.7354e-02]],

         [[ 5.6396e-02, -2.2754e-01,  7.6294e-04,  ...,  3.4180e-03,
            5.8161e-26,  3.1738e-02]],

         ...,

         [[ 1.0834e-03, -9.3750e-01,  2.2203e-06,  ..., -6.0059e-02,
            0.0000e+00,  3.7842e-02]],

         [[ 6.9824e-02, -3.2227e-01,  8.4229e-03,  ..., -8.7280e-03,
            7.2298e-26,  1.4526e-02]],

         [[ 6.3330e-08,  8.3496e-02,  3.4925e-09,  ..., -9.0332e-03,
            0.0000e+00, -2.8687e-02]]]], device='cuda:0', dtype=torch.bfloat16)
here here
no back here:
init for: 21
before conv_state: tensor([[[0., 0., 0.],
         [0., 0., 0.],
         [0., 0., 0.],
         ...,
         [0., 0., 0.],
         [0., 0., 0.],
         [0., 0., 0.]]], device='cuda:0', dtype=torch.bfloat16)
before ssm_state: tensor([[[0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         ...,
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.]]], device='cuda:0',
       dtype=torch.bfloat16)
x.stride0: (873, 1, 873)
x.shape: torch.Size([1, 873, 3584])
x.stride1: (3128832, 3584, 1)
x.stride2: (3128832, 1, 3584)
conv_state1: (10752, 1, 3584)
conv_state2: (10752, 1, 3584)
conv_state3: (10752, 1, 3584)
conv_state: torch.Size([1, 3584, 3])
conv1d_out: torch.Size([1, 3584, 873])
rearrange(conv1d_out, 'b d l -> (b l) d') shape: torch.Size([873, 3584])
x_proj_weight shape: torch.Size([144, 3584])
x_dbl: torch.Size([873, 144])
delta_proj_weight @ x_dbl[:, :delta_rank].t(): torch.Size([3584, 873])
dsfdfs
after conv_state: tensor([[[-0.1631,  0.1108, -0.4336],
         [ 0.8086, -2.6875, -0.4883],
         [-1.9141, -3.2031, -4.1562],
         ...,
         [ 1.1484,  3.7656,  3.3906],
         [-2.9844, -2.6562, -2.3594],
         [ 2.6875,  4.9688,  3.6719]]], device='cuda:0', dtype=torch.bfloat16,
       grad_fn=<AsStridedBackward0>)
after ssm_state: tensor([[[ 1.9531e-01, -6.2988e-02,  4.0283e-02,  ...,  2.2217e-02,
           5.0591e-12, -2.3071e-02],
         [ 9.4369e-16,  2.6131e-04,  2.9477e-19,  ..., -7.5195e-02,
           0.0000e+00,  4.3213e-02],
         [ 4.1602e-01, -3.2227e-01,  2.8320e-01,  ..., -4.6387e-02,
           1.7171e-09,  8.5449e-02],
         ...,
         [ 8.8379e-02, -5.4016e-03,  2.1606e-02,  ..., -6.7383e-02,
           8.3489e-13,  1.9434e-01],
         [ 4.9805e-01, -6.1646e-03,  2.2461e-01,  ..., -4.5776e-03,
           1.6466e-06,  2.6978e-02],
         [ 2.9492e-01, -6.3965e-02,  7.5195e-02,  ..., -1.2512e-03,
           6.1846e-11,  1.3916e-02]]], device='cuda:0', dtype=torch.bfloat16)
after conv from cache: tensor([[[-0.1631,  0.1108, -0.4336],
         [ 0.8086, -2.6875, -0.4883],
         [-1.9141, -3.2031, -4.1562],
         ...,
         [ 1.1484,  3.7656,  3.3906],
         [-2.9844, -2.6562, -2.3594],
         [ 2.6875,  4.9688,  3.6719]]], device='cuda:0', dtype=torch.bfloat16,
       grad_fn=<AsStridedBackward0>)
after ssm_state from cache: tensor([[[[ 1.9531e-01, -6.2988e-02,  4.0283e-02,  ...,  2.2217e-02,
            5.0591e-12, -2.3071e-02]],

         [[ 9.4369e-16,  2.6131e-04,  2.9477e-19,  ..., -7.5195e-02,
            0.0000e+00,  4.3213e-02]],

         [[ 4.1602e-01, -3.2227e-01,  2.8320e-01,  ..., -4.6387e-02,
            1.7171e-09,  8.5449e-02]],

         ...,

         [[ 8.8379e-02, -5.4016e-03,  2.1606e-02,  ..., -6.7383e-02,
            8.3489e-13,  1.9434e-01]],

         [[ 4.9805e-01, -6.1646e-03,  2.2461e-01,  ..., -4.5776e-03,
            1.6466e-06,  2.6978e-02]],

         [[ 2.9492e-01, -6.3965e-02,  7.5195e-02,  ..., -1.2512e-03,
            6.1846e-11,  1.3916e-02]]]], device='cuda:0', dtype=torch.bfloat16)
here here
no back here:
init for: 22
before conv_state: tensor([[[0., 0., 0.],
         [0., 0., 0.],
         [0., 0., 0.],
         ...,
         [0., 0., 0.],
         [0., 0., 0.],
         [0., 0., 0.]]], device='cuda:0', dtype=torch.bfloat16)
before ssm_state: tensor([[[0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         ...,
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.]]], device='cuda:0',
       dtype=torch.bfloat16)
x.stride0: (873, 1, 873)
x.shape: torch.Size([1, 873, 3584])
x.stride1: (3128832, 3584, 1)
x.stride2: (3128832, 1, 3584)
conv_state1: (10752, 1, 3584)
conv_state2: (10752, 1, 3584)
conv_state3: (10752, 1, 3584)
conv_state: torch.Size([1, 3584, 3])
conv1d_out: torch.Size([1, 3584, 873])
rearrange(conv1d_out, 'b d l -> (b l) d') shape: torch.Size([873, 3584])
x_proj_weight shape: torch.Size([144, 3584])
x_dbl: torch.Size([873, 144])
delta_proj_weight @ x_dbl[:, :delta_rank].t(): torch.Size([3584, 873])
dsfdfs
after conv_state: tensor([[[ 2.4531,  1.1094,  2.3125],
         [-0.1064,  1.0156,  2.6719],
         [ 2.0312,  0.1484,  1.0156],
         ...,
         [ 1.5625,  1.0234,  0.4512],
         [-0.9336, -0.7383,  0.8438],
         [ 0.6797, -2.5156, -0.1709]]], device='cuda:0', dtype=torch.bfloat16,
       grad_fn=<AsStridedBackward0>)
after ssm_state: tensor([[[ 2.2799e-06, -1.5156e+00,  5.8316e-23,  ...,  1.1670e-01,
           0.0000e+00,  2.6758e-01],
         [ 4.9593e-08,  1.8457e-01,  9.4502e-13,  ..., -2.6489e-02,
           0.0000e+00, -7.1289e-02],
         [ 3.1494e-02,  2.1875e-01,  1.5640e-03,  ..., -3.0365e-03,
           9.6780e-23, -8.4473e-02],
         ...,
         [ 4.6082e-03, -1.6968e-02,  1.3411e-07,  ..., -4.4556e-03,
           0.0000e+00, -5.3711e-02],
         [ 1.7462e-09,  8.2520e-02,  9.4147e-14,  ..., -6.3171e-03,
           0.0000e+00, -4.8096e-02],
         [ 6.9351e-21,  7.3242e-02,  1.4068e-34,  ...,  6.3171e-03,
           0.0000e+00,  6.6223e-03]]], device='cuda:0', dtype=torch.bfloat16)
after conv from cache: tensor([[[ 2.4531,  1.1094,  2.3125],
         [-0.1064,  1.0156,  2.6719],
         [ 2.0312,  0.1484,  1.0156],
         ...,
         [ 1.5625,  1.0234,  0.4512],
         [-0.9336, -0.7383,  0.8438],
         [ 0.6797, -2.5156, -0.1709]]], device='cuda:0', dtype=torch.bfloat16,
       grad_fn=<AsStridedBackward0>)
after ssm_state from cache: tensor([[[[ 2.2799e-06, -1.5156e+00,  5.8316e-23,  ...,  1.1670e-01,
            0.0000e+00,  2.6758e-01]],

         [[ 4.9593e-08,  1.8457e-01,  9.4502e-13,  ..., -2.6489e-02,
            0.0000e+00, -7.1289e-02]],

         [[ 3.1494e-02,  2.1875e-01,  1.5640e-03,  ..., -3.0365e-03,
            9.6780e-23, -8.4473e-02]],

         ...,

         [[ 4.6082e-03, -1.6968e-02,  1.3411e-07,  ..., -4.4556e-03,
            0.0000e+00, -5.3711e-02]],

         [[ 1.7462e-09,  8.2520e-02,  9.4147e-14,  ..., -6.3171e-03,
            0.0000e+00, -4.8096e-02]],

         [[ 6.9351e-21,  7.3242e-02,  1.4068e-34,  ...,  6.3171e-03,
            0.0000e+00,  6.6223e-03]]]], device='cuda:0', dtype=torch.bfloat16)
here here
no back here:
init for: 23
before conv_state: tensor([[[0., 0., 0.],
         [0., 0., 0.],
         [0., 0., 0.],
         ...,
         [0., 0., 0.],
         [0., 0., 0.],
         [0., 0., 0.]]], device='cuda:0', dtype=torch.bfloat16)
before ssm_state: tensor([[[0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         ...,
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.]]], device='cuda:0',
       dtype=torch.bfloat16)
x.stride0: (873, 1, 873)
x.shape: torch.Size([1, 873, 3584])
x.stride1: (3128832, 3584, 1)
x.stride2: (3128832, 1, 3584)
conv_state1: (10752, 1, 3584)
conv_state2: (10752, 1, 3584)
conv_state3: (10752, 1, 3584)
conv_state: torch.Size([1, 3584, 3])
conv1d_out: torch.Size([1, 3584, 873])
rearrange(conv1d_out, 'b d l -> (b l) d') shape: torch.Size([873, 3584])
x_proj_weight shape: torch.Size([144, 3584])
x_dbl: torch.Size([873, 144])
delta_proj_weight @ x_dbl[:, :delta_rank].t(): torch.Size([3584, 873])
dsfdfs
after conv_state: tensor([[[ 1.4844,  1.2578,  2.1250],
         [-0.5703,  2.0156, -1.2578],
         [-1.3672, -0.1465,  0.8672],
         ...,
         [-1.1797,  0.0242, -3.1719],
         [ 3.3594,  1.1875,  3.4844],
         [-3.1562, -0.7070, -0.1099]]], device='cuda:0', dtype=torch.bfloat16,
       grad_fn=<AsStridedBackward0>)
after ssm_state: tensor([[[ 7.2266e-01,  3.4424e-02,  6.6797e-01,  ...,  2.4048e-02,
           4.6387e-03,  1.2268e-02],
         [ 2.1815e-05,  1.5259e-02,  6.2209e-10,  ...,  1.5564e-03,
           0.0000e+00,  3.2471e-02],
         [ 1.0014e-05, -3.8086e-01,  6.7085e-19,  ..., -6.0059e-02,
           0.0000e+00,  5.3406e-03],
         ...,
         [ 3.9648e-01, -9.3750e-02,  3.1055e-01,  ..., -4.0039e-02,
           8.6054e-07, -4.9072e-02],
         [ 5.5078e-01,  3.6621e-02,  3.5156e-01,  ...,  1.8433e-02,
           1.5793e-03,  2.7344e-02],
         [ 5.3516e-01, -1.3123e-02,  1.6113e-01,  ..., -1.2756e-02,
           1.6117e-04, -5.8350e-02]]], device='cuda:0', dtype=torch.bfloat16)
after conv from cache: tensor([[[ 1.4844,  1.2578,  2.1250],
         [-0.5703,  2.0156, -1.2578],
         [-1.3672, -0.1465,  0.8672],
         ...,
         [-1.1797,  0.0242, -3.1719],
         [ 3.3594,  1.1875,  3.4844],
         [-3.1562, -0.7070, -0.1099]]], device='cuda:0', dtype=torch.bfloat16,
       grad_fn=<AsStridedBackward0>)
after ssm_state from cache: tensor([[[[ 7.2266e-01,  3.4424e-02,  6.6797e-01,  ...,  2.4048e-02,
            4.6387e-03,  1.2268e-02]],

         [[ 2.1815e-05,  1.5259e-02,  6.2209e-10,  ...,  1.5564e-03,
            0.0000e+00,  3.2471e-02]],

         [[ 1.0014e-05, -3.8086e-01,  6.7085e-19,  ..., -6.0059e-02,
            0.0000e+00,  5.3406e-03]],

         ...,

         [[ 3.9648e-01, -9.3750e-02,  3.1055e-01,  ..., -4.0039e-02,
            8.6054e-07, -4.9072e-02]],

         [[ 5.5078e-01,  3.6621e-02,  3.5156e-01,  ...,  1.8433e-02,
            1.5793e-03,  2.7344e-02]],

         [[ 5.3516e-01, -1.3123e-02,  1.6113e-01,  ..., -1.2756e-02,
            1.6117e-04, -5.8350e-02]]]], device='cuda:0', dtype=torch.bfloat16)
here here
no back here:
init for: 24
before conv_state: tensor([[[0., 0., 0.],
         [0., 0., 0.],
         [0., 0., 0.],
         ...,
         [0., 0., 0.],
         [0., 0., 0.],
         [0., 0., 0.]]], device='cuda:0', dtype=torch.bfloat16)
before ssm_state: tensor([[[0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         ...,
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.]]], device='cuda:0',
       dtype=torch.bfloat16)
x.stride0: (873, 1, 873)
x.shape: torch.Size([1, 873, 3584])
x.stride1: (3128832, 3584, 1)
x.stride2: (3128832, 1, 3584)
conv_state1: (10752, 1, 3584)
conv_state2: (10752, 1, 3584)
conv_state3: (10752, 1, 3584)
conv_state: torch.Size([1, 3584, 3])
conv1d_out: torch.Size([1, 3584, 873])
rearrange(conv1d_out, 'b d l -> (b l) d') shape: torch.Size([873, 3584])
x_proj_weight shape: torch.Size([144, 3584])
x_dbl: torch.Size([873, 144])
delta_proj_weight @ x_dbl[:, :delta_rank].t(): torch.Size([3584, 873])
dsfdfs
after conv_state: tensor([[[ 0.8906, -1.3750, -0.8281],
         [ 1.3594,  1.7500,  1.7812],
         [-1.4062, -1.5547, -0.9922],
         ...,
         [ 0.2754, -1.1328, -4.0938],
         [-0.0708, -0.0947, -0.6758],
         [ 1.6484,  3.0938,  3.6719]]], device='cuda:0', dtype=torch.bfloat16,
       grad_fn=<AsStridedBackward0>)
after ssm_state: tensor([[[ 6.4697e-03,  9.1797e-01,  1.2387e-07,  ..., -1.4648e-02,
           0.0000e+00,  6.5918e-02],
         [ 3.7812e-07, -3.1982e-02,  6.2585e-06,  ..., -1.6235e-02,
           0.0000e+00, -3.3264e-03],
         [ 3.2336e-06, -2.9419e-02,  2.1048e-07,  ...,  1.5076e-02,
           0.0000e+00, -8.8379e-02],
         ...,
         [ 2.2888e-04,  1.0645e-01,  9.4878e-09,  ...,  3.7354e-02,
           0.0000e+00,  1.4404e-02],
         [ 1.3924e-04, -2.2339e-02,  6.5613e-04,  ...,  3.8910e-03,
           0.0000e+00, -2.8687e-02],
         [ 2.6093e-03, -2.3682e-02,  7.4878e-07,  ...,  1.9531e-02,
           0.0000e+00, -6.1523e-02]]], device='cuda:0', dtype=torch.bfloat16)
after conv from cache: tensor([[[ 0.8906, -1.3750, -0.8281],
         [ 1.3594,  1.7500,  1.7812],
         [-1.4062, -1.5547, -0.9922],
         ...,
         [ 0.2754, -1.1328, -4.0938],
         [-0.0708, -0.0947, -0.6758],
         [ 1.6484,  3.0938,  3.6719]]], device='cuda:0', dtype=torch.bfloat16,
       grad_fn=<AsStridedBackward0>)
after ssm_state from cache: tensor([[[[ 6.4697e-03,  9.1797e-01,  1.2387e-07,  ..., -1.4648e-02,
            0.0000e+00,  6.5918e-02]],

         [[ 3.7812e-07, -3.1982e-02,  6.2585e-06,  ..., -1.6235e-02,
            0.0000e+00, -3.3264e-03]],

         [[ 3.2336e-06, -2.9419e-02,  2.1048e-07,  ...,  1.5076e-02,
            0.0000e+00, -8.8379e-02]],

         ...,

         [[ 2.2888e-04,  1.0645e-01,  9.4878e-09,  ...,  3.7354e-02,
            0.0000e+00,  1.4404e-02]],

         [[ 1.3924e-04, -2.2339e-02,  6.5613e-04,  ...,  3.8910e-03,
            0.0000e+00, -2.8687e-02]],

         [[ 2.6093e-03, -2.3682e-02,  7.4878e-07,  ...,  1.9531e-02,
            0.0000e+00, -6.1523e-02]]]], device='cuda:0', dtype=torch.bfloat16)
here here
no back here:
init for: 25
before conv_state: tensor([[[0., 0., 0.],
         [0., 0., 0.],
         [0., 0., 0.],
         ...,
         [0., 0., 0.],
         [0., 0., 0.],
         [0., 0., 0.]]], device='cuda:0', dtype=torch.bfloat16)
before ssm_state: tensor([[[0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         ...,
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.]]], device='cuda:0',
       dtype=torch.bfloat16)
x.stride0: (873, 1, 873)
x.shape: torch.Size([1, 873, 3584])
x.stride1: (3128832, 3584, 1)
x.stride2: (3128832, 1, 3584)
conv_state1: (10752, 1, 3584)
conv_state2: (10752, 1, 3584)
conv_state3: (10752, 1, 3584)
conv_state: torch.Size([1, 3584, 3])
conv1d_out: torch.Size([1, 3584, 873])
rearrange(conv1d_out, 'b d l -> (b l) d') shape: torch.Size([873, 3584])
x_proj_weight shape: torch.Size([144, 3584])
x_dbl: torch.Size([873, 144])
delta_proj_weight @ x_dbl[:, :delta_rank].t(): torch.Size([3584, 873])
dsfdfs
after conv_state: tensor([[[-3.1250, -3.2344, -3.0938],
         [-1.6953, -5.2500, -2.5000],
         [-0.4727,  3.4062,  2.4062],
         ...,
         [-1.3984,  0.9297,  3.1875],
         [ 0.3926,  1.7500,  2.7188],
         [-0.9453,  2.6094,  1.4688]]], device='cuda:0', dtype=torch.bfloat16,
       grad_fn=<AsStridedBackward0>)
after ssm_state: tensor([[[ 9.7656e-03,  6.1035e-02,  4.3631e-05,  ..., -7.9102e-02,
           0.0000e+00,  9.0408e-04],
         [ 4.0820e-01, -1.5411e-03,  1.7090e-01,  ...,  8.4229e-03,
           2.1756e-06,  5.6641e-02],
         [ 2.2445e-07, -1.0840e-01,  3.2187e-19,  ...,  2.2827e-02,
           0.0000e+00, -4.3701e-02],
         ...,
         [ 4.8438e-01, -3.4912e-02,  2.2266e-01,  ..., -3.7766e-04,
           6.5804e-05, -3.9551e-02],
         [ 2.6562e-01, -1.5039e-01,  4.1504e-02,  ...,  1.9169e-04,
           7.2760e-10, -3.2806e-03],
         [ 1.1276e-17, -3.5742e-01,  0.0000e+00,  ...,  2.3193e-03,
           0.0000e+00, -2.0020e-01]]], device='cuda:0', dtype=torch.bfloat16)
after conv from cache: tensor([[[-3.1250, -3.2344, -3.0938],
         [-1.6953, -5.2500, -2.5000],
         [-0.4727,  3.4062,  2.4062],
         ...,
         [-1.3984,  0.9297,  3.1875],
         [ 0.3926,  1.7500,  2.7188],
         [-0.9453,  2.6094,  1.4688]]], device='cuda:0', dtype=torch.bfloat16,
       grad_fn=<AsStridedBackward0>)
after ssm_state from cache: tensor([[[[ 9.7656e-03,  6.1035e-02,  4.3631e-05,  ..., -7.9102e-02,
            0.0000e+00,  9.0408e-04]],

         [[ 4.0820e-01, -1.5411e-03,  1.7090e-01,  ...,  8.4229e-03,
            2.1756e-06,  5.6641e-02]],

         [[ 2.2445e-07, -1.0840e-01,  3.2187e-19,  ...,  2.2827e-02,
            0.0000e+00, -4.3701e-02]],

         ...,

         [[ 4.8438e-01, -3.4912e-02,  2.2266e-01,  ..., -3.7766e-04,
            6.5804e-05, -3.9551e-02]],

         [[ 2.6562e-01, -1.5039e-01,  4.1504e-02,  ...,  1.9169e-04,
            7.2760e-10, -3.2806e-03]],

         [[ 1.1276e-17, -3.5742e-01,  0.0000e+00,  ...,  2.3193e-03,
            0.0000e+00, -2.0020e-01]]]], device='cuda:0', dtype=torch.bfloat16)
here here
no back here:
init for: 26
before conv_state: tensor([[[0., 0., 0.],
         [0., 0., 0.],
         [0., 0., 0.],
         ...,
         [0., 0., 0.],
         [0., 0., 0.],
         [0., 0., 0.]]], device='cuda:0', dtype=torch.bfloat16)
before ssm_state: tensor([[[0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         ...,
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.]]], device='cuda:0',
       dtype=torch.bfloat16)
x.stride0: (873, 1, 873)
x.shape: torch.Size([1, 873, 3584])
x.stride1: (3128832, 3584, 1)
x.stride2: (3128832, 1, 3584)
conv_state1: (10752, 1, 3584)
conv_state2: (10752, 1, 3584)
conv_state3: (10752, 1, 3584)
conv_state: torch.Size([1, 3584, 3])
conv1d_out: torch.Size([1, 3584, 873])
rearrange(conv1d_out, 'b d l -> (b l) d') shape: torch.Size([873, 3584])
x_proj_weight shape: torch.Size([144, 3584])
x_dbl: torch.Size([873, 144])
delta_proj_weight @ x_dbl[:, :delta_rank].t(): torch.Size([3584, 873])
dsfdfs
after conv_state: tensor([[[ 0.7773,  1.1406,  0.5000],
         [ 0.8047,  0.7148,  1.8438],
         [-2.6562, -0.6875, -0.9648],
         ...,
         [-1.9219, -2.6875, -0.7852],
         [-1.1562, -3.9219, -2.7344],
         [ 1.1172,  2.5938,  1.9453]]], device='cuda:0', dtype=torch.bfloat16,
       grad_fn=<AsStridedBackward0>)
after ssm_state: tensor([[[ 2.1362e-02, -6.7749e-03,  1.5497e-05,  ...,  4.2236e-02,
           1.4670e-35, -1.4572e-03],
         [ 1.1115e-24, -8.8882e-04,  0.0000e+00,  ..., -9.1309e-02,
           0.0000e+00,  2.6550e-03],
         [ 3.5577e-07,  7.0312e-02,  2.1788e-15,  ...,  5.3406e-03,
           0.0000e+00,  1.6357e-02],
         ...,
         [ 4.0430e-01, -1.5259e-02,  1.4648e-01,  ..., -2.3804e-03,
           2.6584e-05,  4.1580e-04],
         [ 7.6172e-01, -2.0386e-02,  5.8594e-01,  ..., -2.7710e-02,
           2.7466e-02, -7.7820e-03],
         [ 3.9413e-15, -6.7383e-02,  1.5267e-25,  ..., -6.3477e-02,
           0.0000e+00, -1.4709e-02]]], device='cuda:0', dtype=torch.bfloat16)
after conv from cache: tensor([[[ 0.7773,  1.1406,  0.5000],
         [ 0.8047,  0.7148,  1.8438],
         [-2.6562, -0.6875, -0.9648],
         ...,
         [-1.9219, -2.6875, -0.7852],
         [-1.1562, -3.9219, -2.7344],
         [ 1.1172,  2.5938,  1.9453]]], device='cuda:0', dtype=torch.bfloat16,
       grad_fn=<AsStridedBackward0>)
after ssm_state from cache: tensor([[[[ 2.1362e-02, -6.7749e-03,  1.5497e-05,  ...,  4.2236e-02,
            1.4670e-35, -1.4572e-03]],

         [[ 1.1115e-24, -8.8882e-04,  0.0000e+00,  ..., -9.1309e-02,
            0.0000e+00,  2.6550e-03]],

         [[ 3.5577e-07,  7.0312e-02,  2.1788e-15,  ...,  5.3406e-03,
            0.0000e+00,  1.6357e-02]],

         ...,

         [[ 4.0430e-01, -1.5259e-02,  1.4648e-01,  ..., -2.3804e-03,
            2.6584e-05,  4.1580e-04]],

         [[ 7.6172e-01, -2.0386e-02,  5.8594e-01,  ..., -2.7710e-02,
            2.7466e-02, -7.7820e-03]],

         [[ 3.9413e-15, -6.7383e-02,  1.5267e-25,  ..., -6.3477e-02,
            0.0000e+00, -1.4709e-02]]]], device='cuda:0', dtype=torch.bfloat16)
here here
no back here:
init for: 27
before conv_state: tensor([[[0., 0., 0.],
         [0., 0., 0.],
         [0., 0., 0.],
         ...,
         [0., 0., 0.],
         [0., 0., 0.],
         [0., 0., 0.]]], device='cuda:0', dtype=torch.bfloat16)
before ssm_state: tensor([[[0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         ...,
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.]]], device='cuda:0',
       dtype=torch.bfloat16)
x.stride0: (873, 1, 873)
x.shape: torch.Size([1, 873, 3584])
x.stride1: (3128832, 3584, 1)
x.stride2: (3128832, 1, 3584)
conv_state1: (10752, 1, 3584)
conv_state2: (10752, 1, 3584)
conv_state3: (10752, 1, 3584)
conv_state: torch.Size([1, 3584, 3])
conv1d_out: torch.Size([1, 3584, 873])
rearrange(conv1d_out, 'b d l -> (b l) d') shape: torch.Size([873, 3584])
x_proj_weight shape: torch.Size([144, 3584])
x_dbl: torch.Size([873, 144])
delta_proj_weight @ x_dbl[:, :delta_rank].t(): torch.Size([3584, 873])
dsfdfs
after conv_state: tensor([[[-0.6719, -0.7734, -1.7812],
         [-1.4062, -3.0781, -3.3594],
         [-0.7734, -1.4609, -1.2812],
         ...,
         [ 1.9688,  1.1562, -0.1147],
         [ 2.3438,  3.2812,  4.0312],
         [ 0.3262,  0.0527,  0.5586]]], device='cuda:0', dtype=torch.bfloat16,
       grad_fn=<AsStridedBackward0>)
after ssm_state: tensor([[[ 6.0201e-06,  9.3750e-02,  1.4097e-10,  ..., -1.6602e-02,
           0.0000e+00, -4.6143e-02],
         [ 3.6603e-27,  6.6895e-02,  0.0000e+00,  ...,  1.6724e-02,
           0.0000e+00,  6.0059e-02],
         [ 9.7656e-02, -5.8203e-01,  5.7678e-03,  ..., -1.2512e-02,
           1.3678e-13, -2.4707e-01],
         ...,
         [ 4.9174e-07, -2.0508e-01,  3.4284e-13,  ...,  2.6001e-02,
           0.0000e+00, -2.2583e-02],
         [ 1.9989e-03,  1.0059e-01,  9.0003e-06,  ...,  2.9602e-03,
           4.5943e-27,  7.0312e-02],
         [ 7.5602e-12, -6.7444e-03,  7.1498e-14,  ...,  9.5215e-03,
           0.0000e+00, -8.4229e-03]]], device='cuda:0', dtype=torch.bfloat16)
after conv from cache: tensor([[[-0.6719, -0.7734, -1.7812],
         [-1.4062, -3.0781, -3.3594],
         [-0.7734, -1.4609, -1.2812],
         ...,
         [ 1.9688,  1.1562, -0.1147],
         [ 2.3438,  3.2812,  4.0312],
         [ 0.3262,  0.0527,  0.5586]]], device='cuda:0', dtype=torch.bfloat16,
       grad_fn=<AsStridedBackward0>)
after ssm_state from cache: tensor([[[[ 6.0201e-06,  9.3750e-02,  1.4097e-10,  ..., -1.6602e-02,
            0.0000e+00, -4.6143e-02]],

         [[ 3.6603e-27,  6.6895e-02,  0.0000e+00,  ...,  1.6724e-02,
            0.0000e+00,  6.0059e-02]],

         [[ 9.7656e-02, -5.8203e-01,  5.7678e-03,  ..., -1.2512e-02,
            1.3678e-13, -2.4707e-01]],

         ...,

         [[ 4.9174e-07, -2.0508e-01,  3.4284e-13,  ...,  2.6001e-02,
            0.0000e+00, -2.2583e-02]],

         [[ 1.9989e-03,  1.0059e-01,  9.0003e-06,  ...,  2.9602e-03,
            4.5943e-27,  7.0312e-02]],

         [[ 7.5602e-12, -6.7444e-03,  7.1498e-14,  ...,  9.5215e-03,
            0.0000e+00, -8.4229e-03]]]], device='cuda:0', dtype=torch.bfloat16)
here here
no back here:
init for: 28
before conv_state: tensor([[[0., 0., 0.],
         [0., 0., 0.],
         [0., 0., 0.],
         ...,
         [0., 0., 0.],
         [0., 0., 0.],
         [0., 0., 0.]]], device='cuda:0', dtype=torch.bfloat16)
before ssm_state: tensor([[[0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         ...,
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.]]], device='cuda:0',
       dtype=torch.bfloat16)
x.stride0: (873, 1, 873)
x.shape: torch.Size([1, 873, 3584])
x.stride1: (3128832, 3584, 1)
x.stride2: (3128832, 1, 3584)
conv_state1: (10752, 1, 3584)
conv_state2: (10752, 1, 3584)
conv_state3: (10752, 1, 3584)
conv_state: torch.Size([1, 3584, 3])
conv1d_out: torch.Size([1, 3584, 873])
rearrange(conv1d_out, 'b d l -> (b l) d') shape: torch.Size([873, 3584])
x_proj_weight shape: torch.Size([144, 3584])
x_dbl: torch.Size([873, 144])
delta_proj_weight @ x_dbl[:, :delta_rank].t(): torch.Size([3584, 873])
dsfdfs
after conv_state: tensor([[[-0.0752,  1.6797,  0.7148],
         [-1.3359, -2.1562, -1.3203],
         [ 2.7812,  0.4219,  0.6211],
         ...,
         [-1.9375, -1.5391, -0.7734],
         [ 1.7891,  2.4062,  0.3125],
         [-0.7891, -0.6758, -0.0957]]], device='cuda:0', dtype=torch.bfloat16,
       grad_fn=<AsStridedBackward0>)
after ssm_state: tensor([[[ 1.3065e-04, -6.7871e-02,  5.4538e-06,  ..., -1.3916e-02,
           0.0000e+00,  3.4912e-02],
         [ 1.6644e-10, -8.3984e-02,  8.4655e-16,  ..., -1.2085e-02,
           0.0000e+00,  1.5747e-02],
         [ 5.2296e-11,  7.5195e-02,  5.3871e-19,  ..., -1.8433e-02,
           0.0000e+00,  2.9602e-03],
         ...,
         [ 2.1838e-22,  7.5195e-02,  6.8409e-31,  ..., -4.8256e-04,
           0.0000e+00, -2.7954e-02],
         [ 4.5402e-09, -6.5234e-01,  2.4869e-13,  ...,  1.7700e-02,
           0.0000e+00,  1.5137e-02],
         [ 2.0664e-09,  1.4062e-01,  2.0783e-13,  ...,  6.6528e-03,
           0.0000e+00, -8.0078e-02]]], device='cuda:0', dtype=torch.bfloat16)
after conv from cache: tensor([[[-0.0752,  1.6797,  0.7148],
         [-1.3359, -2.1562, -1.3203],
         [ 2.7812,  0.4219,  0.6211],
         ...,
         [-1.9375, -1.5391, -0.7734],
         [ 1.7891,  2.4062,  0.3125],
         [-0.7891, -0.6758, -0.0957]]], device='cuda:0', dtype=torch.bfloat16,
       grad_fn=<AsStridedBackward0>)
after ssm_state from cache: tensor([[[[ 1.3065e-04, -6.7871e-02,  5.4538e-06,  ..., -1.3916e-02,
            0.0000e+00,  3.4912e-02]],

         [[ 1.6644e-10, -8.3984e-02,  8.4655e-16,  ..., -1.2085e-02,
            0.0000e+00,  1.5747e-02]],

         [[ 5.2296e-11,  7.5195e-02,  5.3871e-19,  ..., -1.8433e-02,
            0.0000e+00,  2.9602e-03]],

         ...,

         [[ 2.1838e-22,  7.5195e-02,  6.8409e-31,  ..., -4.8256e-04,
            0.0000e+00, -2.7954e-02]],

         [[ 4.5402e-09, -6.5234e-01,  2.4869e-13,  ...,  1.7700e-02,
            0.0000e+00,  1.5137e-02]],

         [[ 2.0664e-09,  1.4062e-01,  2.0783e-13,  ...,  6.6528e-03,
            0.0000e+00, -8.0078e-02]]]], device='cuda:0', dtype=torch.bfloat16)
here here
no back here:
init for: 29
before conv_state: tensor([[[0., 0., 0.],
         [0., 0., 0.],
         [0., 0., 0.],
         ...,
         [0., 0., 0.],
         [0., 0., 0.],
         [0., 0., 0.]]], device='cuda:0', dtype=torch.bfloat16)
before ssm_state: tensor([[[0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         ...,
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.]]], device='cuda:0',
       dtype=torch.bfloat16)
x.stride0: (873, 1, 873)
x.shape: torch.Size([1, 873, 3584])
x.stride1: (3128832, 3584, 1)
x.stride2: (3128832, 1, 3584)
conv_state1: (10752, 1, 3584)
conv_state2: (10752, 1, 3584)
conv_state3: (10752, 1, 3584)
conv_state: torch.Size([1, 3584, 3])
conv1d_out: torch.Size([1, 3584, 873])
rearrange(conv1d_out, 'b d l -> (b l) d') shape: torch.Size([873, 3584])
x_proj_weight shape: torch.Size([144, 3584])
x_dbl: torch.Size([873, 144])
delta_proj_weight @ x_dbl[:, :delta_rank].t(): torch.Size([3584, 873])
dsfdfs
after conv_state: tensor([[[ 2.1875, -0.2617,  0.4355],
         [-2.3281, -2.8125,  0.4160],
         [ 1.3438,  2.7031,  3.9844],
         ...,
         [-0.0554,  2.0625,  1.5625],
         [-0.6602, -1.5547, -0.2676],
         [-0.2539, -0.8633, -1.5156]]], device='cuda:0', dtype=torch.bfloat16,
       grad_fn=<AsStridedBackward0>)
after ssm_state: tensor([[[ 5.0354e-04, -2.1387e-01,  2.4438e-06,  ...,  1.5488e-03,
           0.0000e+00, -8.7402e-02],
         [ 4.3945e-03, -1.3965e-01,  1.2684e-04,  ..., -4.7302e-03,
           0.0000e+00, -2.5269e-02],
         [ 1.3062e-02,  5.8984e-01,  1.7405e-05,  ...,  9.3994e-03,
           0.0000e+00,  5.0049e-02],
         ...,
         [ 9.5749e-04,  3.2031e+00,  2.4587e-06,  ...,  1.7212e-02,
           0.0000e+00,  4.7461e-01],
         [ 2.4438e-06, -1.1621e-01,  1.4976e-06,  ..., -3.4637e-03,
           0.0000e+00, -5.8838e-02],
         [ 2.8876e-11, -9.6191e-02,  2.9754e-14,  ..., -1.7395e-03,
           0.0000e+00, -3.6865e-02]]], device='cuda:0', dtype=torch.bfloat16)
after conv from cache: tensor([[[ 2.1875, -0.2617,  0.4355],
         [-2.3281, -2.8125,  0.4160],
         [ 1.3438,  2.7031,  3.9844],
         ...,
         [-0.0554,  2.0625,  1.5625],
         [-0.6602, -1.5547, -0.2676],
         [-0.2539, -0.8633, -1.5156]]], device='cuda:0', dtype=torch.bfloat16,
       grad_fn=<AsStridedBackward0>)
after ssm_state from cache: tensor([[[[ 5.0354e-04, -2.1387e-01,  2.4438e-06,  ...,  1.5488e-03,
            0.0000e+00, -8.7402e-02]],

         [[ 4.3945e-03, -1.3965e-01,  1.2684e-04,  ..., -4.7302e-03,
            0.0000e+00, -2.5269e-02]],

         [[ 1.3062e-02,  5.8984e-01,  1.7405e-05,  ...,  9.3994e-03,
            0.0000e+00,  5.0049e-02]],

         ...,

         [[ 9.5749e-04,  3.2031e+00,  2.4587e-06,  ...,  1.7212e-02,
            0.0000e+00,  4.7461e-01]],

         [[ 2.4438e-06, -1.1621e-01,  1.4976e-06,  ..., -3.4637e-03,
            0.0000e+00, -5.8838e-02]],

         [[ 2.8876e-11, -9.6191e-02,  2.9754e-14,  ..., -1.7395e-03,
            0.0000e+00, -3.6865e-02]]]], device='cuda:0', dtype=torch.bfloat16)
here here
no back here:
init for: 30
before conv_state: tensor([[[0., 0., 0.],
         [0., 0., 0.],
         [0., 0., 0.],
         ...,
         [0., 0., 0.],
         [0., 0., 0.],
         [0., 0., 0.]]], device='cuda:0', dtype=torch.bfloat16)
before ssm_state: tensor([[[0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         ...,
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.]]], device='cuda:0',
       dtype=torch.bfloat16)
x.stride0: (873, 1, 873)
x.shape: torch.Size([1, 873, 3584])
x.stride1: (3128832, 3584, 1)
x.stride2: (3128832, 1, 3584)
conv_state1: (10752, 1, 3584)
conv_state2: (10752, 1, 3584)
conv_state3: (10752, 1, 3584)
conv_state: torch.Size([1, 3584, 3])
conv1d_out: torch.Size([1, 3584, 873])
rearrange(conv1d_out, 'b d l -> (b l) d') shape: torch.Size([873, 3584])
x_proj_weight shape: torch.Size([144, 3584])
x_dbl: torch.Size([873, 144])
delta_proj_weight @ x_dbl[:, :delta_rank].t(): torch.Size([3584, 873])
dsfdfs
after conv_state: tensor([[[-2.4688, -2.0938, -2.6719],
         [-4.1250, -6.4062, -5.4688],
         [-2.8750,  0.9492,  0.5469],
         ...,
         [ 0.1196,  0.4004, -0.0505],
         [ 0.2891, -2.4531, -2.4688],
         [ 2.4531, -1.3828,  0.1250]]], device='cuda:0', dtype=torch.bfloat16,
       grad_fn=<AsStridedBackward0>)
after ssm_state: tensor([[[ 8.6042e-16,  1.6504e-01,  6.6691e-24,  ...,  1.9165e-02,
           0.0000e+00,  3.3008e-01],
         [ 1.4544e-05, -1.1572e-01,  4.5657e-10,  ..., -4.1199e-03,
           0.0000e+00, -4.0283e-02],
         [ 2.3749e-08,  6.5430e-02,  9.4868e-19,  ...,  1.0400e-01,
           0.0000e+00,  2.3633e-01],
         ...,
         [ 2.6245e-03,  1.7188e-01,  2.8312e-06,  ...,  6.4697e-03,
           0.0000e+00,  4.6692e-03],
         [ 6.7969e-01,  1.9684e-03,  6.2109e-01,  ..., -1.7090e-03,
           2.4414e-02,  1.7334e-02],
         [ 4.0359e-12,  3.5938e-01,  2.7105e-20,  ...,  3.9307e-02,
           0.0000e+00,  6.6895e-02]]], device='cuda:0', dtype=torch.bfloat16)
after conv from cache: tensor([[[-2.4688, -2.0938, -2.6719],
         [-4.1250, -6.4062, -5.4688],
         [-2.8750,  0.9492,  0.5469],
         ...,
         [ 0.1196,  0.4004, -0.0505],
         [ 0.2891, -2.4531, -2.4688],
         [ 2.4531, -1.3828,  0.1250]]], device='cuda:0', dtype=torch.bfloat16,
       grad_fn=<AsStridedBackward0>)
after ssm_state from cache: tensor([[[[ 8.6042e-16,  1.6504e-01,  6.6691e-24,  ...,  1.9165e-02,
            0.0000e+00,  3.3008e-01]],

         [[ 1.4544e-05, -1.1572e-01,  4.5657e-10,  ..., -4.1199e-03,
            0.0000e+00, -4.0283e-02]],

         [[ 2.3749e-08,  6.5430e-02,  9.4868e-19,  ...,  1.0400e-01,
            0.0000e+00,  2.3633e-01]],

         ...,

         [[ 2.6245e-03,  1.7188e-01,  2.8312e-06,  ...,  6.4697e-03,
            0.0000e+00,  4.6692e-03]],

         [[ 6.7969e-01,  1.9684e-03,  6.2109e-01,  ..., -1.7090e-03,
            2.4414e-02,  1.7334e-02]],

         [[ 4.0359e-12,  3.5938e-01,  2.7105e-20,  ...,  3.9307e-02,
            0.0000e+00,  6.6895e-02]]]], device='cuda:0', dtype=torch.bfloat16)
here here
no back here:
init for: 31
before conv_state: tensor([[[0., 0., 0.],
         [0., 0., 0.],
         [0., 0., 0.],
         ...,
         [0., 0., 0.],
         [0., 0., 0.],
         [0., 0., 0.]]], device='cuda:0', dtype=torch.bfloat16)
before ssm_state: tensor([[[0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         ...,
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.]]], device='cuda:0',
       dtype=torch.bfloat16)
x.stride0: (873, 1, 873)
x.shape: torch.Size([1, 873, 3584])
x.stride1: (3128832, 3584, 1)
x.stride2: (3128832, 1, 3584)
conv_state1: (10752, 1, 3584)
conv_state2: (10752, 1, 3584)
conv_state3: (10752, 1, 3584)
conv_state: torch.Size([1, 3584, 3])
conv1d_out: torch.Size([1, 3584, 873])
rearrange(conv1d_out, 'b d l -> (b l) d') shape: torch.Size([873, 3584])
x_proj_weight shape: torch.Size([144, 3584])
x_dbl: torch.Size([873, 144])
delta_proj_weight @ x_dbl[:, :delta_rank].t(): torch.Size([3584, 873])
dsfdfs
after conv_state: tensor([[[ 0.6680, -0.8320,  0.2402],
         [ 0.5039,  3.8281,  2.1562],
         [ 2.3438,  4.7188,  1.1562],
         ...,
         [ 0.3555, -1.0938, -1.8281],
         [-1.4531, -1.9141, -3.6250],
         [-2.2500,  1.5312,  1.1016]]], device='cuda:0', dtype=torch.bfloat16,
       grad_fn=<AsStridedBackward0>)
after ssm_state: tensor([[[ 1.9776e-16, -5.1514e-02,  3.3773e-30,  ..., -9.4604e-03,
           0.0000e+00,  2.8198e-02],
         [ 7.9744e-09,  2.1094e-01,  7.5286e-25,  ...,  2.7618e-03,
           0.0000e+00,  5.9570e-02],
         [ 3.9290e-10, -1.3281e-01,  1.6156e-24,  ..., -2.8687e-03,
           0.0000e+00,  7.1777e-02],
         ...,
         [ 1.3123e-03,  2.4023e-01,  1.2100e-05,  ...,  1.2329e-02,
           0.0000e+00, -1.4160e-01],
         [ 5.6066e-15, -5.4443e-02,  9.3401e-28,  ..., -9.6436e-03,
           0.0000e+00,  6.1340e-03],
         [ 6.9033e-20,  1.1426e-01,  0.0000e+00,  ...,  9.5703e-02,
           0.0000e+00, -1.5625e-01]]], device='cuda:0', dtype=torch.bfloat16)
after conv from cache: tensor([[[ 0.6680, -0.8320,  0.2402],
         [ 0.5039,  3.8281,  2.1562],
         [ 2.3438,  4.7188,  1.1562],
         ...,
         [ 0.3555, -1.0938, -1.8281],
         [-1.4531, -1.9141, -3.6250],
         [-2.2500,  1.5312,  1.1016]]], device='cuda:0', dtype=torch.bfloat16,
       grad_fn=<AsStridedBackward0>)
after ssm_state from cache: tensor([[[[ 1.9776e-16, -5.1514e-02,  3.3773e-30,  ..., -9.4604e-03,
            0.0000e+00,  2.8198e-02]],

         [[ 7.9744e-09,  2.1094e-01,  7.5286e-25,  ...,  2.7618e-03,
            0.0000e+00,  5.9570e-02]],

         [[ 3.9290e-10, -1.3281e-01,  1.6156e-24,  ..., -2.8687e-03,
            0.0000e+00,  7.1777e-02]],

         ...,

         [[ 1.3123e-03,  2.4023e-01,  1.2100e-05,  ...,  1.2329e-02,
            0.0000e+00, -1.4160e-01]],

         [[ 5.6066e-15, -5.4443e-02,  9.3401e-28,  ..., -9.6436e-03,
            0.0000e+00,  6.1340e-03]],

         [[ 6.9033e-20,  1.1426e-01,  0.0000e+00,  ...,  9.5703e-02,
            0.0000e+00, -1.5625e-01]]]], device='cuda:0', dtype=torch.bfloat16)
here here
no back here:
init for: 32
before conv_state: tensor([[[0., 0., 0.],
         [0., 0., 0.],
         [0., 0., 0.],
         ...,
         [0., 0., 0.],
         [0., 0., 0.],
         [0., 0., 0.]]], device='cuda:0', dtype=torch.bfloat16)
before ssm_state: tensor([[[0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         ...,
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.]]], device='cuda:0',
       dtype=torch.bfloat16)
x.stride0: (873, 1, 873)
x.shape: torch.Size([1, 873, 3584])
x.stride1: (3128832, 3584, 1)
x.stride2: (3128832, 1, 3584)
conv_state1: (10752, 1, 3584)
conv_state2: (10752, 1, 3584)
conv_state3: (10752, 1, 3584)
conv_state: torch.Size([1, 3584, 3])
conv1d_out: torch.Size([1, 3584, 873])
rearrange(conv1d_out, 'b d l -> (b l) d') shape: torch.Size([873, 3584])
x_proj_weight shape: torch.Size([144, 3584])
x_dbl: torch.Size([873, 144])
delta_proj_weight @ x_dbl[:, :delta_rank].t(): torch.Size([3584, 873])
dsfdfs
after conv_state: tensor([[[ 0.9688,  1.3516,  2.2500],
         [ 2.6875, -0.5977, -0.9453],
         [ 1.3594, -1.9688, -2.8750],
         ...,
         [-1.0859,  1.7109, -1.1016],
         [-0.2227, -0.3867,  1.1953],
         [ 1.5156,  1.8438,  3.0156]]], device='cuda:0', dtype=torch.bfloat16,
       grad_fn=<AsStridedBackward0>)
after ssm_state: tensor([[[ 3.3936e-02,  1.2256e-01,  4.6692e-03,  ..., -1.2207e-02,
           1.3235e-22, -5.7129e-02],
         [ 4.2411e-14,  2.9102e-01,  1.3805e-28,  ..., -3.3691e-02,
           0.0000e+00, -1.0889e-01],
         [ 2.5024e-03,  1.2268e-02,  9.1270e-08,  ..., -5.0781e-02,
           0.0000e+00, -1.2256e-01],
         ...,
         [ 2.2870e-20,  2.6733e-02,  3.4080e-22,  ...,  6.1279e-02,
           0.0000e+00,  1.2793e-01],
         [ 1.0550e-09,  1.1816e-01,  7.0344e-13,  ..., -1.0840e-01,
           0.0000e+00, -3.5938e-01],
         [ 7.6294e-06, -7.8613e-02,  4.1723e-06,  ...,  1.2756e-02,
           0.0000e+00,  4.3457e-02]]], device='cuda:0', dtype=torch.bfloat16)
after conv from cache: tensor([[[ 0.9688,  1.3516,  2.2500],
         [ 2.6875, -0.5977, -0.9453],
         [ 1.3594, -1.9688, -2.8750],
         ...,
         [-1.0859,  1.7109, -1.1016],
         [-0.2227, -0.3867,  1.1953],
         [ 1.5156,  1.8438,  3.0156]]], device='cuda:0', dtype=torch.bfloat16,
       grad_fn=<AsStridedBackward0>)
after ssm_state from cache: tensor([[[[ 3.3936e-02,  1.2256e-01,  4.6692e-03,  ..., -1.2207e-02,
            1.3235e-22, -5.7129e-02]],

         [[ 4.2411e-14,  2.9102e-01,  1.3805e-28,  ..., -3.3691e-02,
            0.0000e+00, -1.0889e-01]],

         [[ 2.5024e-03,  1.2268e-02,  9.1270e-08,  ..., -5.0781e-02,
            0.0000e+00, -1.2256e-01]],

         ...,

         [[ 2.2870e-20,  2.6733e-02,  3.4080e-22,  ...,  6.1279e-02,
            0.0000e+00,  1.2793e-01]],

         [[ 1.0550e-09,  1.1816e-01,  7.0344e-13,  ..., -1.0840e-01,
            0.0000e+00, -3.5938e-01]],

         [[ 7.6294e-06, -7.8613e-02,  4.1723e-06,  ...,  1.2756e-02,
            0.0000e+00,  4.3457e-02]]]], device='cuda:0', dtype=torch.bfloat16)
here here
no back here:
init for: 33
before conv_state: tensor([[[0., 0., 0.],
         [0., 0., 0.],
         [0., 0., 0.],
         ...,
         [0., 0., 0.],
         [0., 0., 0.],
         [0., 0., 0.]]], device='cuda:0', dtype=torch.bfloat16)
before ssm_state: tensor([[[0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         ...,
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.]]], device='cuda:0',
       dtype=torch.bfloat16)
x.stride0: (873, 1, 873)
x.shape: torch.Size([1, 873, 3584])
x.stride1: (3128832, 3584, 1)
x.stride2: (3128832, 1, 3584)
conv_state1: (10752, 1, 3584)
conv_state2: (10752, 1, 3584)
conv_state3: (10752, 1, 3584)
conv_state: torch.Size([1, 3584, 3])
conv1d_out: torch.Size([1, 3584, 873])
rearrange(conv1d_out, 'b d l -> (b l) d') shape: torch.Size([873, 3584])
x_proj_weight shape: torch.Size([144, 3584])
x_dbl: torch.Size([873, 144])
delta_proj_weight @ x_dbl[:, :delta_rank].t(): torch.Size([3584, 873])
dsfdfs
after conv_state: tensor([[[-1.2188, -1.3672, -0.9805],
         [ 3.2500, -1.2422, -2.2031],
         [ 0.0253, -1.0391, -1.3047],
         ...,
         [-0.0500,  2.0781,  2.7969],
         [ 0.3672, -0.6016,  0.6836],
         [-4.7188,  1.5703,  0.2080]]], device='cuda:0', dtype=torch.bfloat16,
       grad_fn=<AsStridedBackward0>)
after ssm_state: tensor([[[ 9.0594e-14, -4.0039e-01,  3.1447e-34,  ...,  1.1035e-01,
           0.0000e+00, -3.6133e-02],
         [ 1.5571e-09, -2.0703e-01,  3.3218e-13,  ..., -4.6387e-02,
           0.0000e+00,  2.5146e-02],
         [ 4.2948e-32,  1.0889e-01,  0.0000e+00,  ..., -6.0547e-02,
           0.0000e+00,  4.7607e-02],
         ...,
         [ 3.0909e-13, -3.8818e-02,  3.7902e-31,  ...,  3.9307e-02,
           0.0000e+00, -2.1240e-02],
         [ 4.3164e-01,  5.7129e-02,  1.4160e-01,  ..., -3.5645e-02,
           2.0582e-07,  3.6011e-03],
         [ 3.5858e-03, -6.6797e-01,  1.3411e-06,  ...,  1.8848e-01,
           0.0000e+00, -8.1055e-02]]], device='cuda:0', dtype=torch.bfloat16)
after conv from cache: tensor([[[-1.2188, -1.3672, -0.9805],
         [ 3.2500, -1.2422, -2.2031],
         [ 0.0253, -1.0391, -1.3047],
         ...,
         [-0.0500,  2.0781,  2.7969],
         [ 0.3672, -0.6016,  0.6836],
         [-4.7188,  1.5703,  0.2080]]], device='cuda:0', dtype=torch.bfloat16,
       grad_fn=<AsStridedBackward0>)
after ssm_state from cache: tensor([[[[ 9.0594e-14, -4.0039e-01,  3.1447e-34,  ...,  1.1035e-01,
            0.0000e+00, -3.6133e-02]],

         [[ 1.5571e-09, -2.0703e-01,  3.3218e-13,  ..., -4.6387e-02,
            0.0000e+00,  2.5146e-02]],

         [[ 4.2948e-32,  1.0889e-01,  0.0000e+00,  ..., -6.0547e-02,
            0.0000e+00,  4.7607e-02]],

         ...,

         [[ 3.0909e-13, -3.8818e-02,  3.7902e-31,  ...,  3.9307e-02,
            0.0000e+00, -2.1240e-02]],

         [[ 4.3164e-01,  5.7129e-02,  1.4160e-01,  ..., -3.5645e-02,
            2.0582e-07,  3.6011e-03]],

         [[ 3.5858e-03, -6.6797e-01,  1.3411e-06,  ...,  1.8848e-01,
            0.0000e+00, -8.1055e-02]]]], device='cuda:0', dtype=torch.bfloat16)
here here
no back here:
init for: 34
before conv_state: tensor([[[0., 0., 0.],
         [0., 0., 0.],
         [0., 0., 0.],
         ...,
         [0., 0., 0.],
         [0., 0., 0.],
         [0., 0., 0.]]], device='cuda:0', dtype=torch.bfloat16)
before ssm_state: tensor([[[0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         ...,
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.]]], device='cuda:0',
       dtype=torch.bfloat16)
x.stride0: (873, 1, 873)
x.shape: torch.Size([1, 873, 3584])
x.stride1: (3128832, 3584, 1)
x.stride2: (3128832, 1, 3584)
conv_state1: (10752, 1, 3584)
conv_state2: (10752, 1, 3584)
conv_state3: (10752, 1, 3584)
conv_state: torch.Size([1, 3584, 3])
conv1d_out: torch.Size([1, 3584, 873])
rearrange(conv1d_out, 'b d l -> (b l) d') shape: torch.Size([873, 3584])
x_proj_weight shape: torch.Size([144, 3584])
x_dbl: torch.Size([873, 144])
delta_proj_weight @ x_dbl[:, :delta_rank].t(): torch.Size([3584, 873])
dsfdfs
after conv_state: tensor([[[-2.1562, -0.6680,  0.0698],
         [ 0.7344, -1.0469, -0.2363],
         [-2.2188,  0.0140, -1.8516],
         ...,
         [-1.4062,  1.9844,  1.8984],
         [-0.2793,  4.8438,  4.9375],
         [-1.8281, -0.1016,  2.0938]]], device='cuda:0', dtype=torch.bfloat16,
       grad_fn=<AsStridedBackward0>)
after ssm_state: tensor([[[ 3.9685e-35, -6.0791e-02,  0.0000e+00,  ..., -1.2817e-02,
           0.0000e+00,  4.2969e-02],
         [ 1.2369e-10,  2.1250e+00,  5.8163e-32,  ...,  2.2461e-02,
           0.0000e+00, -4.5898e-02],
         [ 2.5466e-09,  6.1768e-02,  4.7976e-23,  ..., -1.8921e-03,
           0.0000e+00, -6.4850e-04],
         ...,
         [ 3.5970e-36,  5.2344e-01,  0.0000e+00,  ...,  1.4258e-01,
           0.0000e+00, -2.8516e-01],
         [ 5.0923e-24, -2.2507e-04,  0.0000e+00,  ..., -2.2583e-02,
           0.0000e+00,  4.7852e-02],
         [ 1.9895e-11, -1.8799e-02,  9.1073e-18,  ..., -3.5095e-03,
           0.0000e+00,  8.3618e-03]]], device='cuda:0', dtype=torch.bfloat16)
after conv from cache: tensor([[[-2.1562, -0.6680,  0.0698],
         [ 0.7344, -1.0469, -0.2363],
         [-2.2188,  0.0140, -1.8516],
         ...,
         [-1.4062,  1.9844,  1.8984],
         [-0.2793,  4.8438,  4.9375],
         [-1.8281, -0.1016,  2.0938]]], device='cuda:0', dtype=torch.bfloat16,
       grad_fn=<AsStridedBackward0>)
after ssm_state from cache: tensor([[[[ 3.9685e-35, -6.0791e-02,  0.0000e+00,  ..., -1.2817e-02,
            0.0000e+00,  4.2969e-02]],

         [[ 1.2369e-10,  2.1250e+00,  5.8163e-32,  ...,  2.2461e-02,
            0.0000e+00, -4.5898e-02]],

         [[ 2.5466e-09,  6.1768e-02,  4.7976e-23,  ..., -1.8921e-03,
            0.0000e+00, -6.4850e-04]],

         ...,

         [[ 3.5970e-36,  5.2344e-01,  0.0000e+00,  ...,  1.4258e-01,
            0.0000e+00, -2.8516e-01]],

         [[ 5.0923e-24, -2.2507e-04,  0.0000e+00,  ..., -2.2583e-02,
            0.0000e+00,  4.7852e-02]],

         [[ 1.9895e-11, -1.8799e-02,  9.1073e-18,  ..., -3.5095e-03,
            0.0000e+00,  8.3618e-03]]]], device='cuda:0', dtype=torch.bfloat16)
here here
no back here:
init for: 35
before conv_state: tensor([[[0., 0., 0.],
         [0., 0., 0.],
         [0., 0., 0.],
         ...,
         [0., 0., 0.],
         [0., 0., 0.],
         [0., 0., 0.]]], device='cuda:0', dtype=torch.bfloat16)
before ssm_state: tensor([[[0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         ...,
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.]]], device='cuda:0',
       dtype=torch.bfloat16)
x.stride0: (873, 1, 873)
x.shape: torch.Size([1, 873, 3584])
x.stride1: (3128832, 3584, 1)
x.stride2: (3128832, 1, 3584)
conv_state1: (10752, 1, 3584)
conv_state2: (10752, 1, 3584)
conv_state3: (10752, 1, 3584)
conv_state: torch.Size([1, 3584, 3])
conv1d_out: torch.Size([1, 3584, 873])
rearrange(conv1d_out, 'b d l -> (b l) d') shape: torch.Size([873, 3584])
x_proj_weight shape: torch.Size([144, 3584])
x_dbl: torch.Size([873, 144])
delta_proj_weight @ x_dbl[:, :delta_rank].t(): torch.Size([3584, 873])
dsfdfs
after conv_state: tensor([[[-2.4062, -0.1133,  0.5039],
         [ 0.8594,  3.2188,  0.8086],
         [-1.6875,  0.6953,  1.2578],
         ...,
         [ 2.7812,  2.5000,  4.0938],
         [-1.5312, -0.6016, -2.1406],
         [-0.6602, -4.0000, -2.6875]]], device='cuda:0', dtype=torch.bfloat16,
       grad_fn=<AsStridedBackward0>)
after ssm_state: tensor([[[ 2.8046e-33,  1.8555e-02,  0.0000e+00,  ...,  3.6377e-02,
           0.0000e+00, -1.9409e-02],
         [ 2.6245e-02, -2.2583e-03,  3.1090e-04,  ...,  1.3367e-02,
           8.8984e-28,  3.8385e-05],
         [ 2.3293e-21,  1.1414e-02,  0.0000e+00,  ...,  5.2734e-02,
           0.0000e+00,  2.4902e-02],
         ...,
         [ 1.4710e-15,  2.3438e-02,  6.5000e-33,  ...,  3.7842e-02,
           0.0000e+00, -2.4261e-03],
         [ 1.2079e-12,  7.2327e-03,  5.2459e-29,  ..., -3.2715e-02,
           0.0000e+00,  1.0803e-02],
         [ 1.3770e-01,  1.8387e-03,  7.7637e-02,  ...,  5.4321e-03,
           1.3915e-10, -3.7384e-03]]], device='cuda:0', dtype=torch.bfloat16)
after conv from cache: tensor([[[-2.4062, -0.1133,  0.5039],
         [ 0.8594,  3.2188,  0.8086],
         [-1.6875,  0.6953,  1.2578],
         ...,
         [ 2.7812,  2.5000,  4.0938],
         [-1.5312, -0.6016, -2.1406],
         [-0.6602, -4.0000, -2.6875]]], device='cuda:0', dtype=torch.bfloat16,
       grad_fn=<AsStridedBackward0>)
after ssm_state from cache: tensor([[[[ 2.8046e-33,  1.8555e-02,  0.0000e+00,  ...,  3.6377e-02,
            0.0000e+00, -1.9409e-02]],

         [[ 2.6245e-02, -2.2583e-03,  3.1090e-04,  ...,  1.3367e-02,
            8.8984e-28,  3.8385e-05]],

         [[ 2.3293e-21,  1.1414e-02,  0.0000e+00,  ...,  5.2734e-02,
            0.0000e+00,  2.4902e-02]],

         ...,

         [[ 1.4710e-15,  2.3438e-02,  6.5000e-33,  ...,  3.7842e-02,
            0.0000e+00, -2.4261e-03]],

         [[ 1.2079e-12,  7.2327e-03,  5.2459e-29,  ..., -3.2715e-02,
            0.0000e+00,  1.0803e-02]],

         [[ 1.3770e-01,  1.8387e-03,  7.7637e-02,  ...,  5.4321e-03,
            1.3915e-10, -3.7384e-03]]]], device='cuda:0', dtype=torch.bfloat16)
here here
no back here:
init for: 36
before conv_state: tensor([[[0., 0., 0.],
         [0., 0., 0.],
         [0., 0., 0.],
         ...,
         [0., 0., 0.],
         [0., 0., 0.],
         [0., 0., 0.]]], device='cuda:0', dtype=torch.bfloat16)
before ssm_state: tensor([[[0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         ...,
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.]]], device='cuda:0',
       dtype=torch.bfloat16)
x.stride0: (873, 1, 873)
x.shape: torch.Size([1, 873, 3584])
x.stride1: (3128832, 3584, 1)
x.stride2: (3128832, 1, 3584)
conv_state1: (10752, 1, 3584)
conv_state2: (10752, 1, 3584)
conv_state3: (10752, 1, 3584)
conv_state: torch.Size([1, 3584, 3])
conv1d_out: torch.Size([1, 3584, 873])
rearrange(conv1d_out, 'b d l -> (b l) d') shape: torch.Size([873, 3584])
x_proj_weight shape: torch.Size([144, 3584])
x_dbl: torch.Size([873, 144])
delta_proj_weight @ x_dbl[:, :delta_rank].t(): torch.Size([3584, 873])
dsfdfs
after conv_state: tensor([[[ 0.3457, -0.6406,  1.7109],
         [-1.8672, -0.6406,  0.7344],
         [-1.5547, -0.9805, -0.5312],
         ...,
         [-2.9062, -1.9844, -1.3828],
         [-0.0654,  3.3594,  3.7812],
         [ 1.0625,  3.8281,  6.3438]]], device='cuda:0', dtype=torch.bfloat16,
       grad_fn=<AsStridedBackward0>)
after ssm_state: tensor([[[ 1.6211e-01, -5.0049e-02,  1.5869e-02,  ...,  8.7280e-03,
           2.7358e-08,  2.6001e-02],
         [ 6.5234e-01,  2.2827e-02,  4.7266e-01,  ..., -7.9956e-03,
           6.0730e-03,  1.5717e-03],
         [ 9.9609e-01,  6.6757e-04,  9.9219e-01,  ..., -5.4932e-04,
           9.5312e-01,  1.0834e-03],
         ...,
         [ 7.6172e-01,  4.3457e-02,  6.6016e-01,  ..., -7.0190e-03,
           2.7344e-01,  4.1504e-03],
         [ 1.0000e+00, -8.2970e-05,  1.0000e+00,  ..., -3.2663e-05,
           9.9609e-01, -2.5940e-04],
         [ 9.9219e-01,  8.8501e-04,  9.8828e-01,  ...,  1.9073e-04,
           9.0234e-01,  2.6398e-03]]], device='cuda:0', dtype=torch.bfloat16)
after conv from cache: tensor([[[ 0.3457, -0.6406,  1.7109],
         [-1.8672, -0.6406,  0.7344],
         [-1.5547, -0.9805, -0.5312],
         ...,
         [-2.9062, -1.9844, -1.3828],
         [-0.0654,  3.3594,  3.7812],
         [ 1.0625,  3.8281,  6.3438]]], device='cuda:0', dtype=torch.bfloat16,
       grad_fn=<AsStridedBackward0>)
after ssm_state from cache: tensor([[[[ 1.6211e-01, -5.0049e-02,  1.5869e-02,  ...,  8.7280e-03,
            2.7358e-08,  2.6001e-02]],

         [[ 6.5234e-01,  2.2827e-02,  4.7266e-01,  ..., -7.9956e-03,
            6.0730e-03,  1.5717e-03]],

         [[ 9.9609e-01,  6.6757e-04,  9.9219e-01,  ..., -5.4932e-04,
            9.5312e-01,  1.0834e-03]],

         ...,

         [[ 7.6172e-01,  4.3457e-02,  6.6016e-01,  ..., -7.0190e-03,
            2.7344e-01,  4.1504e-03]],

         [[ 1.0000e+00, -8.2970e-05,  1.0000e+00,  ..., -3.2663e-05,
            9.9609e-01, -2.5940e-04]],

         [[ 9.9219e-01,  8.8501e-04,  9.8828e-01,  ...,  1.9073e-04,
            9.0234e-01,  2.6398e-03]]]], device='cuda:0', dtype=torch.bfloat16)
here here
no back here:
init for: 37
before conv_state: tensor([[[0., 0., 0.],
         [0., 0., 0.],
         [0., 0., 0.],
         ...,
         [0., 0., 0.],
         [0., 0., 0.],
         [0., 0., 0.]]], device='cuda:0', dtype=torch.bfloat16)
before ssm_state: tensor([[[0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         ...,
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.]]], device='cuda:0',
       dtype=torch.bfloat16)
x.stride0: (873, 1, 873)
x.shape: torch.Size([1, 873, 3584])
x.stride1: (3128832, 3584, 1)
x.stride2: (3128832, 1, 3584)
conv_state1: (10752, 1, 3584)
conv_state2: (10752, 1, 3584)
conv_state3: (10752, 1, 3584)
conv_state: torch.Size([1, 3584, 3])
conv1d_out: torch.Size([1, 3584, 873])
rearrange(conv1d_out, 'b d l -> (b l) d') shape: torch.Size([873, 3584])
x_proj_weight shape: torch.Size([144, 3584])
x_dbl: torch.Size([873, 144])
delta_proj_weight @ x_dbl[:, :delta_rank].t(): torch.Size([3584, 873])
dsfdfs
after conv_state: tensor([[[-0.0297,  2.3750,  1.7891],
         [ 0.4590,  1.4844,  1.5938],
         [ 1.9531, -0.6016, -2.8438],
         ...,
         [ 0.9375, -0.7891, -3.5625],
         [ 0.7266,  1.1484, -0.5508],
         [-0.9648,  0.0466,  0.8008]]], device='cuda:0', dtype=torch.bfloat16,
       grad_fn=<AsStridedBackward0>)
after ssm_state: tensor([[[ 7.1491e-30, -7.9102e-02,  0.0000e+00,  ..., -1.3123e-02,
           0.0000e+00,  5.5847e-03],
         [ 4.7032e-08,  3.3447e-02,  6.6746e-19,  ..., -3.9307e-02,
           0.0000e+00,  2.8931e-02],
         [ 1.1514e-21,  3.1006e-02,  0.0000e+00,  ...,  3.0396e-02,
           0.0000e+00,  3.1738e-03],
         ...,
         [ 1.6309e-01,  2.0703e-01,  3.0151e-02,  ..., -1.2024e-02,
           3.2117e-12,  6.9275e-03],
         [ 3.0075e-30,  1.0449e-01,  0.0000e+00,  ..., -9.6191e-02,
           0.0000e+00, -2.1118e-02],
         [ 4.4936e-08, -1.1426e-01,  3.1752e-14,  ...,  1.8311e-02,
           0.0000e+00, -1.2283e-03]]], device='cuda:0', dtype=torch.bfloat16)
after conv from cache: tensor([[[-0.0297,  2.3750,  1.7891],
         [ 0.4590,  1.4844,  1.5938],
         [ 1.9531, -0.6016, -2.8438],
         ...,
         [ 0.9375, -0.7891, -3.5625],
         [ 0.7266,  1.1484, -0.5508],
         [-0.9648,  0.0466,  0.8008]]], device='cuda:0', dtype=torch.bfloat16,
       grad_fn=<AsStridedBackward0>)
after ssm_state from cache: tensor([[[[ 7.1491e-30, -7.9102e-02,  0.0000e+00,  ..., -1.3123e-02,
            0.0000e+00,  5.5847e-03]],

         [[ 4.7032e-08,  3.3447e-02,  6.6746e-19,  ..., -3.9307e-02,
            0.0000e+00,  2.8931e-02]],

         [[ 1.1514e-21,  3.1006e-02,  0.0000e+00,  ...,  3.0396e-02,
            0.0000e+00,  3.1738e-03]],

         ...,

         [[ 1.6309e-01,  2.0703e-01,  3.0151e-02,  ..., -1.2024e-02,
            3.2117e-12,  6.9275e-03]],

         [[ 3.0075e-30,  1.0449e-01,  0.0000e+00,  ..., -9.6191e-02,
            0.0000e+00, -2.1118e-02]],

         [[ 4.4936e-08, -1.1426e-01,  3.1752e-14,  ...,  1.8311e-02,
            0.0000e+00, -1.2283e-03]]]], device='cuda:0', dtype=torch.bfloat16)
here here
no back here:
init for: 38
before conv_state: tensor([[[0., 0., 0.],
         [0., 0., 0.],
         [0., 0., 0.],
         ...,
         [0., 0., 0.],
         [0., 0., 0.],
         [0., 0., 0.]]], device='cuda:0', dtype=torch.bfloat16)
before ssm_state: tensor([[[0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         ...,
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.]]], device='cuda:0',
       dtype=torch.bfloat16)
x.stride0: (873, 1, 873)
x.shape: torch.Size([1, 873, 3584])
x.stride1: (3128832, 3584, 1)
x.stride2: (3128832, 1, 3584)
conv_state1: (10752, 1, 3584)
conv_state2: (10752, 1, 3584)
conv_state3: (10752, 1, 3584)
conv_state: torch.Size([1, 3584, 3])
conv1d_out: torch.Size([1, 3584, 873])
rearrange(conv1d_out, 'b d l -> (b l) d') shape: torch.Size([873, 3584])
x_proj_weight shape: torch.Size([144, 3584])
x_dbl: torch.Size([873, 144])
delta_proj_weight @ x_dbl[:, :delta_rank].t(): torch.Size([3584, 873])
dsfdfs
after conv_state: tensor([[[-2.8125, -2.2031,  0.4023],
         [ 1.3984,  1.4297, -4.0000],
         [ 3.0625, -1.0547,  2.4531],
         ...,
         [ 1.9453, -0.9375, -0.2695],
         [ 1.4844,  3.4219,  5.2188],
         [ 1.6328,  3.8906,  4.4688]]], device='cuda:0', dtype=torch.bfloat16,
       grad_fn=<AsStridedBackward0>)
after ssm_state: tensor([[[ 2.7930e-01, -3.8605e-03,  1.2891e-01,  ..., -4.5471e-03,
           1.0245e-07,  1.1978e-03],
         [ 4.6194e-06, -8.5449e-02,  2.9132e-13,  ...,  2.6123e-02,
           0.0000e+00, -1.1108e-02],
         [ 2.2736e-03, -3.8086e-01,  1.2696e-05,  ...,  3.5889e-02,
           5.4073e-37,  5.4321e-03],
         ...,
         [ 3.3111e-12, -4.3945e-01,  5.5671e-35,  ...,  1.2695e-01,
           0.0000e+00, -2.3047e-01],
         [ 1.2817e-02, -1.8945e-01,  4.0054e-05,  ...,  9.9487e-03,
           3.7363e-32,  2.3499e-03],
         [ 5.3406e-05, -1.3550e-02,  2.3874e-12,  ...,  3.3691e-02,
           0.0000e+00, -9.5215e-03]]], device='cuda:0', dtype=torch.bfloat16)
after conv from cache: tensor([[[-2.8125, -2.2031,  0.4023],
         [ 1.3984,  1.4297, -4.0000],
         [ 3.0625, -1.0547,  2.4531],
         ...,
         [ 1.9453, -0.9375, -0.2695],
         [ 1.4844,  3.4219,  5.2188],
         [ 1.6328,  3.8906,  4.4688]]], device='cuda:0', dtype=torch.bfloat16,
       grad_fn=<AsStridedBackward0>)
after ssm_state from cache: tensor([[[[ 2.7930e-01, -3.8605e-03,  1.2891e-01,  ..., -4.5471e-03,
            1.0245e-07,  1.1978e-03]],

         [[ 4.6194e-06, -8.5449e-02,  2.9132e-13,  ...,  2.6123e-02,
            0.0000e+00, -1.1108e-02]],

         [[ 2.2736e-03, -3.8086e-01,  1.2696e-05,  ...,  3.5889e-02,
            5.4073e-37,  5.4321e-03]],

         ...,

         [[ 3.3111e-12, -4.3945e-01,  5.5671e-35,  ...,  1.2695e-01,
            0.0000e+00, -2.3047e-01]],

         [[ 1.2817e-02, -1.8945e-01,  4.0054e-05,  ...,  9.9487e-03,
            3.7363e-32,  2.3499e-03]],

         [[ 5.3406e-05, -1.3550e-02,  2.3874e-12,  ...,  3.3691e-02,
            0.0000e+00, -9.5215e-03]]]], device='cuda:0', dtype=torch.bfloat16)
here here
no back here:
init for: 39
before conv_state: tensor([[[0., 0., 0.],
         [0., 0., 0.],
         [0., 0., 0.],
         ...,
         [0., 0., 0.],
         [0., 0., 0.],
         [0., 0., 0.]]], device='cuda:0', dtype=torch.bfloat16)
before ssm_state: tensor([[[0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         ...,
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.]]], device='cuda:0',
       dtype=torch.bfloat16)
x.stride0: (873, 1, 873)
x.shape: torch.Size([1, 873, 3584])
x.stride1: (3128832, 3584, 1)
x.stride2: (3128832, 1, 3584)
conv_state1: (10752, 1, 3584)
conv_state2: (10752, 1, 3584)
conv_state3: (10752, 1, 3584)
conv_state: torch.Size([1, 3584, 3])
conv1d_out: torch.Size([1, 3584, 873])
rearrange(conv1d_out, 'b d l -> (b l) d') shape: torch.Size([873, 3584])
x_proj_weight shape: torch.Size([144, 3584])
x_dbl: torch.Size([873, 144])
delta_proj_weight @ x_dbl[:, :delta_rank].t(): torch.Size([3584, 873])
dsfdfs
after conv_state: tensor([[[-1.3750, -2.4062,  1.3594],
         [-0.4375,  1.7188,  2.3594],
         [ 2.6094, -1.6250,  2.3125],
         ...,
         [-2.4531,  0.3789, -0.9336],
         [ 1.4453,  1.4297,  0.6875],
         [ 2.2969,  0.4102,  1.0781]]], device='cuda:0', dtype=torch.bfloat16,
       grad_fn=<AsStridedBackward0>)
after ssm_state: tensor([[[ 2.7657e-04, -2.9053e-02,  1.3784e-06,  ...,  7.3242e-04,
           0.0000e+00, -2.1973e-02],
         [ 2.8320e-01,  6.1340e-03,  6.2988e-02,  ..., -5.1270e-03,
           3.5507e-09,  8.0566e-03],
         [ 4.1809e-03,  1.1719e-02,  2.0303e-07,  ..., -1.1902e-02,
           8.1250e-35, -6.1951e-03],
         ...,
         [ 4.9477e-10,  1.1169e-02,  3.4994e-13,  ...,  5.8899e-03,
           0.0000e+00,  7.4768e-03],
         [ 5.9204e-03,  1.2573e-02,  2.3842e-07,  ...,  1.7456e-02,
           3.7392e-28,  7.3624e-04],
         [ 8.1539e-05,  5.3711e-02,  3.2187e-06,  ...,  5.6152e-02,
           0.0000e+00, -1.3550e-02]]], device='cuda:0', dtype=torch.bfloat16)
after conv from cache: tensor([[[-1.3750, -2.4062,  1.3594],
         [-0.4375,  1.7188,  2.3594],
         [ 2.6094, -1.6250,  2.3125],
         ...,
         [-2.4531,  0.3789, -0.9336],
         [ 1.4453,  1.4297,  0.6875],
         [ 2.2969,  0.4102,  1.0781]]], device='cuda:0', dtype=torch.bfloat16,
       grad_fn=<AsStridedBackward0>)
after ssm_state from cache: tensor([[[[ 2.7657e-04, -2.9053e-02,  1.3784e-06,  ...,  7.3242e-04,
            0.0000e+00, -2.1973e-02]],

         [[ 2.8320e-01,  6.1340e-03,  6.2988e-02,  ..., -5.1270e-03,
            3.5507e-09,  8.0566e-03]],

         [[ 4.1809e-03,  1.1719e-02,  2.0303e-07,  ..., -1.1902e-02,
            8.1250e-35, -6.1951e-03]],

         ...,

         [[ 4.9477e-10,  1.1169e-02,  3.4994e-13,  ...,  5.8899e-03,
            0.0000e+00,  7.4768e-03]],

         [[ 5.9204e-03,  1.2573e-02,  2.3842e-07,  ...,  1.7456e-02,
            3.7392e-28,  7.3624e-04]],

         [[ 8.1539e-05,  5.3711e-02,  3.2187e-06,  ...,  5.6152e-02,
            0.0000e+00, -1.3550e-02]]]], device='cuda:0', dtype=torch.bfloat16)
here here
no back here:
init for: 40
before conv_state: tensor([[[0., 0., 0.],
         [0., 0., 0.],
         [0., 0., 0.],
         ...,
         [0., 0., 0.],
         [0., 0., 0.],
         [0., 0., 0.]]], device='cuda:0', dtype=torch.bfloat16)
before ssm_state: tensor([[[0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         ...,
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.]]], device='cuda:0',
       dtype=torch.bfloat16)
x.stride0: (873, 1, 873)
x.shape: torch.Size([1, 873, 3584])
x.stride1: (3128832, 3584, 1)
x.stride2: (3128832, 1, 3584)
conv_state1: (10752, 1, 3584)
conv_state2: (10752, 1, 3584)
conv_state3: (10752, 1, 3584)
conv_state: torch.Size([1, 3584, 3])
conv1d_out: torch.Size([1, 3584, 873])
rearrange(conv1d_out, 'b d l -> (b l) d') shape: torch.Size([873, 3584])
x_proj_weight shape: torch.Size([144, 3584])
x_dbl: torch.Size([873, 144])
delta_proj_weight @ x_dbl[:, :delta_rank].t(): torch.Size([3584, 873])
dsfdfs
after conv_state: tensor([[[-1.6797,  1.2891,  3.3438],
         [ 0.7305, -2.9219, -0.1992],
         [ 2.3281, -0.3047,  2.2812],
         ...,
         [-0.7812, -2.8906, -1.0625],
         [ 3.2188,  3.2500,  4.5625],
         [-5.2812, -1.1328, -1.9609]]], device='cuda:0', dtype=torch.bfloat16,
       grad_fn=<AsStridedBackward0>)
after ssm_state: tensor([[[ 4.1602e-01, -2.0264e-02,  1.5430e-01,  ...,  3.9978e-03,
           1.9968e-06, -2.7313e-03],
         [ 7.1668e-10, -3.7109e-02,  1.2089e-17,  ..., -1.8463e-03,
           0.0000e+00, -2.2705e-02],
         [ 5.9686e-12, -1.5234e-01,  2.4670e-20,  ..., -5.7129e-02,
           0.0000e+00,  3.6377e-02],
         ...,
         [ 2.8125e-01, -3.4180e-02,  5.2246e-02,  ..., -5.1270e-03,
           1.2282e-08,  6.0425e-03],
         [ 1.1749e-03, -9.0790e-04,  6.1035e-05,  ..., -7.3547e-03,
           5.9142e-38, -1.3550e-02],
         [ 9.4922e-01,  1.1215e-03,  8.7109e-01,  ...,  2.2278e-03,
           3.9062e-01, -1.8997e-03]]], device='cuda:0', dtype=torch.bfloat16)
after conv from cache: tensor([[[-1.6797,  1.2891,  3.3438],
         [ 0.7305, -2.9219, -0.1992],
         [ 2.3281, -0.3047,  2.2812],
         ...,
         [-0.7812, -2.8906, -1.0625],
         [ 3.2188,  3.2500,  4.5625],
         [-5.2812, -1.1328, -1.9609]]], device='cuda:0', dtype=torch.bfloat16,
       grad_fn=<AsStridedBackward0>)
after ssm_state from cache: tensor([[[[ 4.1602e-01, -2.0264e-02,  1.5430e-01,  ...,  3.9978e-03,
            1.9968e-06, -2.7313e-03]],

         [[ 7.1668e-10, -3.7109e-02,  1.2089e-17,  ..., -1.8463e-03,
            0.0000e+00, -2.2705e-02]],

         [[ 5.9686e-12, -1.5234e-01,  2.4670e-20,  ..., -5.7129e-02,
            0.0000e+00,  3.6377e-02]],

         ...,

         [[ 2.8125e-01, -3.4180e-02,  5.2246e-02,  ..., -5.1270e-03,
            1.2282e-08,  6.0425e-03]],

         [[ 1.1749e-03, -9.0790e-04,  6.1035e-05,  ..., -7.3547e-03,
            5.9142e-38, -1.3550e-02]],

         [[ 9.4922e-01,  1.1215e-03,  8.7109e-01,  ...,  2.2278e-03,
            3.9062e-01, -1.8997e-03]]]], device='cuda:0', dtype=torch.bfloat16)
here here
no back here:
init for: 41
before conv_state: tensor([[[0., 0., 0.],
         [0., 0., 0.],
         [0., 0., 0.],
         ...,
         [0., 0., 0.],
         [0., 0., 0.],
         [0., 0., 0.]]], device='cuda:0', dtype=torch.bfloat16)
before ssm_state: tensor([[[0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         ...,
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.]]], device='cuda:0',
       dtype=torch.bfloat16)
x.stride0: (873, 1, 873)
x.shape: torch.Size([1, 873, 3584])
x.stride1: (3128832, 3584, 1)
x.stride2: (3128832, 1, 3584)
conv_state1: (10752, 1, 3584)
conv_state2: (10752, 1, 3584)
conv_state3: (10752, 1, 3584)
conv_state: torch.Size([1, 3584, 3])
conv1d_out: torch.Size([1, 3584, 873])
rearrange(conv1d_out, 'b d l -> (b l) d') shape: torch.Size([873, 3584])
x_proj_weight shape: torch.Size([144, 3584])
x_dbl: torch.Size([873, 144])
delta_proj_weight @ x_dbl[:, :delta_rank].t(): torch.Size([3584, 873])
dsfdfs
after conv_state: tensor([[[ 0.9648,  1.0469,  1.7109],
         [-0.7891,  0.2676,  3.9531],
         [ 1.6797,  1.0781, -0.4824],
         ...,
         [ 4.4688,  6.5938,  4.3125],
         [-0.2412,  4.0000,  4.5938],
         [-1.1172, -3.0312, -7.0938]]], device='cuda:0', dtype=torch.bfloat16,
       grad_fn=<AsStridedBackward0>)
after ssm_state: tensor([[[ 5.5469e-01, -2.6733e-02,  2.2852e-01,  ...,  4.7913e-03,
           3.9482e-04, -4.7852e-02],
         [ 3.3984e-01, -1.9073e-03,  1.4355e-01,  ...,  2.0294e-03,
           1.0356e-06,  1.5076e-02],
         [ 5.7422e-01, -2.2705e-02,  3.4375e-01,  ..., -2.2827e-02,
           1.4305e-05, -4.0527e-02],
         ...,
         [ 9.2285e-02, -1.2402e-01,  3.2471e-02,  ...,  5.3711e-03,
           5.7259e-19, -4.1504e-03],
         [ 6.0722e-07, -1.7853e-03,  6.3238e-13,  ..., -1.0834e-03,
           0.0000e+00, -1.1215e-03],
         [ 3.6812e-04,  1.6968e-02,  8.9058e-09,  ...,  6.9580e-03,
           0.0000e+00,  1.6235e-02]]], device='cuda:0', dtype=torch.bfloat16)
after conv from cache: tensor([[[ 0.9648,  1.0469,  1.7109],
         [-0.7891,  0.2676,  3.9531],
         [ 1.6797,  1.0781, -0.4824],
         ...,
         [ 4.4688,  6.5938,  4.3125],
         [-0.2412,  4.0000,  4.5938],
         [-1.1172, -3.0312, -7.0938]]], device='cuda:0', dtype=torch.bfloat16,
       grad_fn=<AsStridedBackward0>)
after ssm_state from cache: tensor([[[[ 5.5469e-01, -2.6733e-02,  2.2852e-01,  ...,  4.7913e-03,
            3.9482e-04, -4.7852e-02]],

         [[ 3.3984e-01, -1.9073e-03,  1.4355e-01,  ...,  2.0294e-03,
            1.0356e-06,  1.5076e-02]],

         [[ 5.7422e-01, -2.2705e-02,  3.4375e-01,  ..., -2.2827e-02,
            1.4305e-05, -4.0527e-02]],

         ...,

         [[ 9.2285e-02, -1.2402e-01,  3.2471e-02,  ...,  5.3711e-03,
            5.7259e-19, -4.1504e-03]],

         [[ 6.0722e-07, -1.7853e-03,  6.3238e-13,  ..., -1.0834e-03,
            0.0000e+00, -1.1215e-03]],

         [[ 3.6812e-04,  1.6968e-02,  8.9058e-09,  ...,  6.9580e-03,
            0.0000e+00,  1.6235e-02]]]], device='cuda:0', dtype=torch.bfloat16)
here here
no back here:
init for: 42
before conv_state: tensor([[[0., 0., 0.],
         [0., 0., 0.],
         [0., 0., 0.],
         ...,
         [0., 0., 0.],
         [0., 0., 0.],
         [0., 0., 0.]]], device='cuda:0', dtype=torch.bfloat16)
before ssm_state: tensor([[[0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         ...,
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.]]], device='cuda:0',
       dtype=torch.bfloat16)
x.stride0: (873, 1, 873)
x.shape: torch.Size([1, 873, 3584])
x.stride1: (3128832, 3584, 1)
x.stride2: (3128832, 1, 3584)
conv_state1: (10752, 1, 3584)
conv_state2: (10752, 1, 3584)
conv_state3: (10752, 1, 3584)
conv_state: torch.Size([1, 3584, 3])
conv1d_out: torch.Size([1, 3584, 873])
rearrange(conv1d_out, 'b d l -> (b l) d') shape: torch.Size([873, 3584])
x_proj_weight shape: torch.Size([144, 3584])
x_dbl: torch.Size([873, 144])
delta_proj_weight @ x_dbl[:, :delta_rank].t(): torch.Size([3584, 873])
dsfdfs
after conv_state: tensor([[[ 0.8047, -2.5469, -2.6875],
         [ 6.3438,  5.4375,  2.6250],
         [ 3.8594,  1.3047,  3.2031],
         ...,
         [-1.9297, -3.7656, -7.4375],
         [-1.1875, -0.3457, -0.9375],
         [ 2.8594,  2.8594,  0.6953]]], device='cuda:0', dtype=torch.bfloat16,
       grad_fn=<AsStridedBackward0>)
after ssm_state: tensor([[[ 1.1102e-15,  9.7656e-02,  1.8735e-29,  ...,  1.6479e-02,
           0.0000e+00, -5.1758e-02],
         [ 1.5616e-05, -9.6436e-03,  2.6048e-09,  ..., -4.1199e-03,
           0.0000e+00, -1.6846e-02],
         [ 9.0122e-05, -4.1016e-02,  6.0245e-09,  ..., -5.0659e-03,
           0.0000e+00,  2.3438e-02],
         ...,
         [ 1.0986e-01,  2.0410e-01,  2.0874e-02,  ..., -7.9346e-03,
           2.4564e-20, -3.2715e-02],
         [ 7.6294e-05, -9.2773e-02,  1.6317e-06,  ...,  3.4809e-05,
           3.2914e-35,  2.5879e-02],
         [ 6.0547e-01, -3.5248e-03,  4.3164e-01,  ...,  1.7548e-03,
           1.8921e-03,  3.7384e-03]]], device='cuda:0', dtype=torch.bfloat16)
after conv from cache: tensor([[[ 0.8047, -2.5469, -2.6875],
         [ 6.3438,  5.4375,  2.6250],
         [ 3.8594,  1.3047,  3.2031],
         ...,
         [-1.9297, -3.7656, -7.4375],
         [-1.1875, -0.3457, -0.9375],
         [ 2.8594,  2.8594,  0.6953]]], device='cuda:0', dtype=torch.bfloat16,
       grad_fn=<AsStridedBackward0>)
after ssm_state from cache: tensor([[[[ 1.1102e-15,  9.7656e-02,  1.8735e-29,  ...,  1.6479e-02,
            0.0000e+00, -5.1758e-02]],

         [[ 1.5616e-05, -9.6436e-03,  2.6048e-09,  ..., -4.1199e-03,
            0.0000e+00, -1.6846e-02]],

         [[ 9.0122e-05, -4.1016e-02,  6.0245e-09,  ..., -5.0659e-03,
            0.0000e+00,  2.3438e-02]],

         ...,

         [[ 1.0986e-01,  2.0410e-01,  2.0874e-02,  ..., -7.9346e-03,
            2.4564e-20, -3.2715e-02]],

         [[ 7.6294e-05, -9.2773e-02,  1.6317e-06,  ...,  3.4809e-05,
            3.2914e-35,  2.5879e-02]],

         [[ 6.0547e-01, -3.5248e-03,  4.3164e-01,  ...,  1.7548e-03,
            1.8921e-03,  3.7384e-03]]]], device='cuda:0', dtype=torch.bfloat16)
here here
no back here:
init for: 43
before conv_state: tensor([[[0., 0., 0.],
         [0., 0., 0.],
         [0., 0., 0.],
         ...,
         [0., 0., 0.],
         [0., 0., 0.],
         [0., 0., 0.]]], device='cuda:0', dtype=torch.bfloat16)
before ssm_state: tensor([[[0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         ...,
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.]]], device='cuda:0',
       dtype=torch.bfloat16)
x.stride0: (873, 1, 873)
x.shape: torch.Size([1, 873, 3584])
x.stride1: (3128832, 3584, 1)
x.stride2: (3128832, 1, 3584)
conv_state1: (10752, 1, 3584)
conv_state2: (10752, 1, 3584)
conv_state3: (10752, 1, 3584)
conv_state: torch.Size([1, 3584, 3])
conv1d_out: torch.Size([1, 3584, 873])
rearrange(conv1d_out, 'b d l -> (b l) d') shape: torch.Size([873, 3584])
x_proj_weight shape: torch.Size([144, 3584])
x_dbl: torch.Size([873, 144])
delta_proj_weight @ x_dbl[:, :delta_rank].t(): torch.Size([3584, 873])
dsfdfs
after conv_state: tensor([[[ 2.6250,  4.0625,  3.8906],
         [-1.0312, -3.6406, -3.8125],
         [ 0.0100, -3.9375, -1.8672],
         ...,
         [-0.5938, -3.9062, -2.7969],
         [ 0.4551, -1.8672, -0.7812],
         [-2.0781,  0.8203,  1.7109]]], device='cuda:0', dtype=torch.bfloat16,
       grad_fn=<AsStridedBackward0>)
after ssm_state: tensor([[[ 1.5162e-19, -1.9043e-01,  0.0000e+00,  ...,  5.2246e-02,
           0.0000e+00,  1.9043e-02],
         [ 2.4156e-09, -1.1902e-02,  8.4199e-13,  ..., -1.9409e-02,
           0.0000e+00,  9.4604e-03],
         [ 2.9138e-18, -2.8516e-01,  2.4154e-22,  ...,  3.1006e-02,
           0.0000e+00,  1.2573e-02],
         ...,
         [ 7.3412e-24, -1.3281e-01,  0.0000e+00,  ...,  5.2490e-03,
           0.0000e+00,  5.6763e-03],
         [ 1.0490e-05, -7.3242e-02,  4.4107e-06,  ...,  4.7852e-02,
           0.0000e+00,  1.2512e-02],
         [ 9.1735e-08,  5.4297e-01,  4.9266e-16,  ..., -1.7773e-01,
           0.0000e+00, -1.0303e-01]]], device='cuda:0', dtype=torch.bfloat16)
after conv from cache: tensor([[[ 2.6250,  4.0625,  3.8906],
         [-1.0312, -3.6406, -3.8125],
         [ 0.0100, -3.9375, -1.8672],
         ...,
         [-0.5938, -3.9062, -2.7969],
         [ 0.4551, -1.8672, -0.7812],
         [-2.0781,  0.8203,  1.7109]]], device='cuda:0', dtype=torch.bfloat16,
       grad_fn=<AsStridedBackward0>)
after ssm_state from cache: tensor([[[[ 1.5162e-19, -1.9043e-01,  0.0000e+00,  ...,  5.2246e-02,
            0.0000e+00,  1.9043e-02]],

         [[ 2.4156e-09, -1.1902e-02,  8.4199e-13,  ..., -1.9409e-02,
            0.0000e+00,  9.4604e-03]],

         [[ 2.9138e-18, -2.8516e-01,  2.4154e-22,  ...,  3.1006e-02,
            0.0000e+00,  1.2573e-02]],

         ...,

         [[ 7.3412e-24, -1.3281e-01,  0.0000e+00,  ...,  5.2490e-03,
            0.0000e+00,  5.6763e-03]],

         [[ 1.0490e-05, -7.3242e-02,  4.4107e-06,  ...,  4.7852e-02,
            0.0000e+00,  1.2512e-02]],

         [[ 9.1735e-08,  5.4297e-01,  4.9266e-16,  ..., -1.7773e-01,
            0.0000e+00, -1.0303e-01]]]], device='cuda:0', dtype=torch.bfloat16)
here here
no back here:
init for: 44
before conv_state: tensor([[[0., 0., 0.],
         [0., 0., 0.],
         [0., 0., 0.],
         ...,
         [0., 0., 0.],
         [0., 0., 0.],
         [0., 0., 0.]]], device='cuda:0', dtype=torch.bfloat16)
before ssm_state: tensor([[[0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         ...,
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.]]], device='cuda:0',
       dtype=torch.bfloat16)
x.stride0: (873, 1, 873)
x.shape: torch.Size([1, 873, 3584])
x.stride1: (3128832, 3584, 1)
x.stride2: (3128832, 1, 3584)
conv_state1: (10752, 1, 3584)
conv_state2: (10752, 1, 3584)
conv_state3: (10752, 1, 3584)
conv_state: torch.Size([1, 3584, 3])
conv1d_out: torch.Size([1, 3584, 873])
rearrange(conv1d_out, 'b d l -> (b l) d') shape: torch.Size([873, 3584])
x_proj_weight shape: torch.Size([144, 3584])
x_dbl: torch.Size([873, 144])
delta_proj_weight @ x_dbl[:, :delta_rank].t(): torch.Size([3584, 873])
dsfdfs
after conv_state: tensor([[[-0.5859, -1.2812, -0.6445],
         [ 1.1797,  4.0000,  1.3047],
         [-4.1250, -1.7266, -1.4375],
         ...,
         [ 2.3906,  0.3047, -0.3320],
         [-2.3438,  0.0688,  0.1680],
         [ 3.3594,  2.5000,  5.6250]]], device='cuda:0', dtype=torch.bfloat16,
       grad_fn=<AsStridedBackward0>)
after ssm_state: tensor([[[ 5.5879e-09, -2.6758e-01,  2.3823e-22,  ...,  1.9653e-02,
           0.0000e+00,  6.5918e-03],
         [ 0.0000e+00,  3.3936e-02,  0.0000e+00,  ..., -1.8799e-02,
           0.0000e+00, -2.3651e-03],
         [ 3.2425e-05,  9.3262e-02,  3.9836e-10,  ..., -6.5918e-02,
           0.0000e+00, -1.2756e-02],
         ...,
         [ 1.2660e-09, -2.1680e-01,  1.4162e-18,  ...,  4.1016e-01,
           0.0000e+00,  1.2109e-01],
         [ 7.1049e-05,  8.0566e-02,  3.3900e-07,  ..., -2.9175e-02,
           0.0000e+00, -1.7456e-02],
         [ 6.1340e-03, -3.3594e-01,  2.6673e-06,  ...,  6.4453e-02,
           0.0000e+00,  3.8330e-02]]], device='cuda:0', dtype=torch.bfloat16)
after conv from cache: tensor([[[-0.5859, -1.2812, -0.6445],
         [ 1.1797,  4.0000,  1.3047],
         [-4.1250, -1.7266, -1.4375],
         ...,
         [ 2.3906,  0.3047, -0.3320],
         [-2.3438,  0.0688,  0.1680],
         [ 3.3594,  2.5000,  5.6250]]], device='cuda:0', dtype=torch.bfloat16,
       grad_fn=<AsStridedBackward0>)
after ssm_state from cache: tensor([[[[ 5.5879e-09, -2.6758e-01,  2.3823e-22,  ...,  1.9653e-02,
            0.0000e+00,  6.5918e-03]],

         [[ 0.0000e+00,  3.3936e-02,  0.0000e+00,  ..., -1.8799e-02,
            0.0000e+00, -2.3651e-03]],

         [[ 3.2425e-05,  9.3262e-02,  3.9836e-10,  ..., -6.5918e-02,
            0.0000e+00, -1.2756e-02]],

         ...,

         [[ 1.2660e-09, -2.1680e-01,  1.4162e-18,  ...,  4.1016e-01,
            0.0000e+00,  1.2109e-01]],

         [[ 7.1049e-05,  8.0566e-02,  3.3900e-07,  ..., -2.9175e-02,
            0.0000e+00, -1.7456e-02]],

         [[ 6.1340e-03, -3.3594e-01,  2.6673e-06,  ...,  6.4453e-02,
            0.0000e+00,  3.8330e-02]]]], device='cuda:0', dtype=torch.bfloat16)
here here
no back here:
init for: 45
before conv_state: tensor([[[0., 0., 0.],
         [0., 0., 0.],
         [0., 0., 0.],
         ...,
         [0., 0., 0.],
         [0., 0., 0.],
         [0., 0., 0.]]], device='cuda:0', dtype=torch.bfloat16)
before ssm_state: tensor([[[0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         ...,
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.]]], device='cuda:0',
       dtype=torch.bfloat16)
x.stride0: (873, 1, 873)
x.shape: torch.Size([1, 873, 3584])
x.stride1: (3128832, 3584, 1)
x.stride2: (3128832, 1, 3584)
conv_state1: (10752, 1, 3584)
conv_state2: (10752, 1, 3584)
conv_state3: (10752, 1, 3584)
conv_state: torch.Size([1, 3584, 3])
conv1d_out: torch.Size([1, 3584, 873])
rearrange(conv1d_out, 'b d l -> (b l) d') shape: torch.Size([873, 3584])
x_proj_weight shape: torch.Size([144, 3584])
x_dbl: torch.Size([873, 144])
delta_proj_weight @ x_dbl[:, :delta_rank].t(): torch.Size([3584, 873])
dsfdfs
after conv_state: tensor([[[-2.0938, -5.0625, -2.2500],
         [-3.7188,  1.3516, -1.4922],
         [-5.2812, -5.2812, -6.7500],
         ...,
         [ 4.5625,  2.9062,  2.5469],
         [-2.4531,  0.1196,  0.1006],
         [ 2.0938,  2.8750,  5.9062]]], device='cuda:0', dtype=torch.bfloat16,
       grad_fn=<AsStridedBackward0>)
after ssm_state: tensor([[[ 2.0564e-06,  2.3346e-03,  4.0563e-10,  ...,  7.7820e-03,
           0.0000e+00,  4.8828e-03],
         [ 1.9558e-08,  4.5776e-03,  9.2835e-19,  ..., -3.9978e-03,
           0.0000e+00, -1.8311e-02],
         [ 2.3636e-17, -1.5747e-02,  1.0894e-33,  ...,  2.6093e-03,
           0.0000e+00,  9.1553e-03],
         ...,
         [ 0.0000e+00, -6.2866e-03,  0.0000e+00,  ...,  2.0294e-03,
           0.0000e+00,  4.3945e-03],
         [ 8.8692e-05, -5.9814e-03,  8.0327e-09,  ..., -2.9449e-03,
           0.0000e+00,  2.1729e-02],
         [ 9.3842e-04, -3.9673e-03,  1.7509e-06,  ..., -7.4768e-03,
           7.2587e-37,  2.3193e-03]]], device='cuda:0', dtype=torch.bfloat16)
after conv from cache: tensor([[[-2.0938, -5.0625, -2.2500],
         [-3.7188,  1.3516, -1.4922],
         [-5.2812, -5.2812, -6.7500],
         ...,
         [ 4.5625,  2.9062,  2.5469],
         [-2.4531,  0.1196,  0.1006],
         [ 2.0938,  2.8750,  5.9062]]], device='cuda:0', dtype=torch.bfloat16,
       grad_fn=<AsStridedBackward0>)
after ssm_state from cache: tensor([[[[ 2.0564e-06,  2.3346e-03,  4.0563e-10,  ...,  7.7820e-03,
            0.0000e+00,  4.8828e-03]],

         [[ 1.9558e-08,  4.5776e-03,  9.2835e-19,  ..., -3.9978e-03,
            0.0000e+00, -1.8311e-02]],

         [[ 2.3636e-17, -1.5747e-02,  1.0894e-33,  ...,  2.6093e-03,
            0.0000e+00,  9.1553e-03]],

         ...,

         [[ 0.0000e+00, -6.2866e-03,  0.0000e+00,  ...,  2.0294e-03,
            0.0000e+00,  4.3945e-03]],

         [[ 8.8692e-05, -5.9814e-03,  8.0327e-09,  ..., -2.9449e-03,
            0.0000e+00,  2.1729e-02]],

         [[ 9.3842e-04, -3.9673e-03,  1.7509e-06,  ..., -7.4768e-03,
            7.2587e-37,  2.3193e-03]]]], device='cuda:0', dtype=torch.bfloat16)
here here
no back here:
init for: 46
before conv_state: tensor([[[0., 0., 0.],
         [0., 0., 0.],
         [0., 0., 0.],
         ...,
         [0., 0., 0.],
         [0., 0., 0.],
         [0., 0., 0.]]], device='cuda:0', dtype=torch.bfloat16)
before ssm_state: tensor([[[0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         ...,
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.]]], device='cuda:0',
       dtype=torch.bfloat16)
x.stride0: (873, 1, 873)
x.shape: torch.Size([1, 873, 3584])
x.stride1: (3128832, 3584, 1)
x.stride2: (3128832, 1, 3584)
conv_state1: (10752, 1, 3584)
conv_state2: (10752, 1, 3584)
conv_state3: (10752, 1, 3584)
conv_state: torch.Size([1, 3584, 3])
conv1d_out: torch.Size([1, 3584, 873])
rearrange(conv1d_out, 'b d l -> (b l) d') shape: torch.Size([873, 3584])
x_proj_weight shape: torch.Size([144, 3584])
x_dbl: torch.Size([873, 144])
delta_proj_weight @ x_dbl[:, :delta_rank].t(): torch.Size([3584, 873])
dsfdfs
after conv_state: tensor([[[ -3.0312,  -5.3125,  -2.0625],
         [  0.1982,  -3.5000,  -3.4531],
         [  0.1167,   6.1875,   5.7500],
         ...,
         [ -2.2812,  -6.4062,  -6.6875],
         [  2.4688,   1.6875,   2.3125],
         [-11.7500, -12.5000, -12.6250]]], device='cuda:0',
       dtype=torch.bfloat16, grad_fn=<AsStridedBackward0>)
after ssm_state: tensor([[[ 0.0000e+00,  9.4604e-03,  0.0000e+00,  ..., -2.4414e-02,
           0.0000e+00, -7.5195e-02],
         [ 9.9460e-27, -4.1504e-02,  0.0000e+00,  ...,  1.0254e-02,
           0.0000e+00,  1.1292e-02],
         [ 4.3656e-10,  4.7363e-02,  1.4138e-16,  ...,  1.5625e-02,
           0.0000e+00, -1.5182e-03],
         ...,
         [ 7.4942e-10,  5.5664e-02,  2.3188e-20,  ...,  8.3008e-03,
           0.0000e+00, -9.2163e-03],
         [ 0.0000e+00, -8.1177e-03,  0.0000e+00,  ..., -2.1606e-02,
           0.0000e+00, -7.1289e-02],
         [ 4.9023e-01,  2.5392e-05,  2.4219e-01,  ..., -3.5286e-05,
           1.6570e-05, -1.8120e-04]]], device='cuda:0', dtype=torch.bfloat16)
after conv from cache: tensor([[[ -3.0312,  -5.3125,  -2.0625],
         [  0.1982,  -3.5000,  -3.4531],
         [  0.1167,   6.1875,   5.7500],
         ...,
         [ -2.2812,  -6.4062,  -6.6875],
         [  2.4688,   1.6875,   2.3125],
         [-11.7500, -12.5000, -12.6250]]], device='cuda:0',
       dtype=torch.bfloat16, grad_fn=<AsStridedBackward0>)
after ssm_state from cache: tensor([[[[ 0.0000e+00,  9.4604e-03,  0.0000e+00,  ..., -2.4414e-02,
            0.0000e+00, -7.5195e-02]],

         [[ 9.9460e-27, -4.1504e-02,  0.0000e+00,  ...,  1.0254e-02,
            0.0000e+00,  1.1292e-02]],

         [[ 4.3656e-10,  4.7363e-02,  1.4138e-16,  ...,  1.5625e-02,
            0.0000e+00, -1.5182e-03]],

         ...,

         [[ 7.4942e-10,  5.5664e-02,  2.3188e-20,  ...,  8.3008e-03,
            0.0000e+00, -9.2163e-03]],

         [[ 0.0000e+00, -8.1177e-03,  0.0000e+00,  ..., -2.1606e-02,
            0.0000e+00, -7.1289e-02]],

         [[ 4.9023e-01,  2.5392e-05,  2.4219e-01,  ..., -3.5286e-05,
            1.6570e-05, -1.8120e-04]]]], device='cuda:0', dtype=torch.bfloat16)
here here
no back here:
init for: 47
before conv_state: tensor([[[0., 0., 0.],
         [0., 0., 0.],
         [0., 0., 0.],
         ...,
         [0., 0., 0.],
         [0., 0., 0.],
         [0., 0., 0.]]], device='cuda:0', dtype=torch.bfloat16)
before ssm_state: tensor([[[0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         ...,
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.]]], device='cuda:0',
       dtype=torch.bfloat16)
x.stride0: (873, 1, 873)
x.shape: torch.Size([1, 873, 3584])
x.stride1: (3128832, 3584, 1)
x.stride2: (3128832, 1, 3584)
conv_state1: (10752, 1, 3584)
conv_state2: (10752, 1, 3584)
conv_state3: (10752, 1, 3584)
conv_state: torch.Size([1, 3584, 3])
conv1d_out: torch.Size([1, 3584, 873])
rearrange(conv1d_out, 'b d l -> (b l) d') shape: torch.Size([873, 3584])
x_proj_weight shape: torch.Size([144, 3584])
x_dbl: torch.Size([873, 144])
delta_proj_weight @ x_dbl[:, :delta_rank].t(): torch.Size([3584, 873])
dsfdfs
after conv_state: tensor([[[-8.3750, -5.8125, -7.7188],
         [ 6.1250,  4.0938,  9.7500],
         [ 1.6641,  0.9414,  3.0000],
         ...,
         [-0.0723,  2.5156, -0.8906],
         [ 6.9062,  5.3125,  3.5469],
         [ 0.8789,  2.0625,  1.2578]]], device='cuda:0', dtype=torch.bfloat16,
       grad_fn=<AsStridedBackward0>)
after ssm_state: tensor([[[ 5.9454e-25, -2.0386e-02,  0.0000e+00,  ...,  5.0659e-03,
           0.0000e+00,  4.5471e-03],
         [ 9.2969e-01, -5.5237e-03,  8.6719e-01,  ...,  7.4158e-03,
           2.3730e-01,  4.7913e-03],
         [ 0.0000e+00, -4.0283e-02,  0.0000e+00,  ...,  5.0781e-02,
           0.0000e+00,  7.4219e-02],
         ...,
         [ 4.9593e-33,  5.4688e-02,  0.0000e+00,  ..., -1.0803e-02,
           0.0000e+00, -1.5640e-03],
         [ 0.0000e+00, -7.5989e-03,  0.0000e+00,  ..., -5.6250e-01,
           0.0000e+00, -8.3594e-01],
         [ 0.0000e+00,  3.3722e-03,  0.0000e+00,  ...,  6.2256e-02,
           0.0000e+00,  4.7119e-02]]], device='cuda:0', dtype=torch.bfloat16)
after conv from cache: tensor([[[-8.3750, -5.8125, -7.7188],
         [ 6.1250,  4.0938,  9.7500],
         [ 1.6641,  0.9414,  3.0000],
         ...,
         [-0.0723,  2.5156, -0.8906],
         [ 6.9062,  5.3125,  3.5469],
         [ 0.8789,  2.0625,  1.2578]]], device='cuda:0', dtype=torch.bfloat16,
       grad_fn=<AsStridedBackward0>)
after ssm_state from cache: tensor([[[[ 5.9454e-25, -2.0386e-02,  0.0000e+00,  ...,  5.0659e-03,
            0.0000e+00,  4.5471e-03]],

         [[ 9.2969e-01, -5.5237e-03,  8.6719e-01,  ...,  7.4158e-03,
            2.3730e-01,  4.7913e-03]],

         [[ 0.0000e+00, -4.0283e-02,  0.0000e+00,  ...,  5.0781e-02,
            0.0000e+00,  7.4219e-02]],

         ...,

         [[ 4.9593e-33,  5.4688e-02,  0.0000e+00,  ..., -1.0803e-02,
            0.0000e+00, -1.5640e-03]],

         [[ 0.0000e+00, -7.5989e-03,  0.0000e+00,  ..., -5.6250e-01,
            0.0000e+00, -8.3594e-01]],

         [[ 0.0000e+00,  3.3722e-03,  0.0000e+00,  ...,  6.2256e-02,
            0.0000e+00,  4.7119e-02]]]], device='cuda:0', dtype=torch.bfloat16)
restart:restart:restart:
(tensor([[[-14.0625, -14.5000, -14.0625],
         [ 12.5625,  12.8125,  12.5625],
         [  5.0938,   2.2500,   5.0938],
         ...,
         [-16.2500, -13.5625, -16.2500],
         [ 14.8125,  12.6250,  14.8125],
         [  7.5625,   9.5625,   7.5625]]], device='cuda:0',
       dtype=torch.bfloat16, grad_fn=<AsStridedBackward0>), tensor([[[[ 6.9141e-01,  5.3406e-05,  4.9023e-01,  ..., -3.1233e-05,
            1.6327e-03, -3.3188e-04]],

         [[ 4.7740e-15,  2.0303e-07,  1.6172e-28,  ...,  9.6112e-07,
            0.0000e+00, -1.3947e-05]],

         [[ 7.1094e-01,  2.8076e-03,  5.2734e-01,  ..., -1.4496e-03,
            2.5368e-04, -5.8899e-03]],

         ...,

         [[ 0.0000e+00, -1.0132e-02,  0.0000e+00,  ...,  3.1738e-02,
            0.0000e+00, -8.8379e-02]],

         [[ 7.4726e-32, -4.1485e-05,  0.0000e+00,  ...,  1.2994e-05,
            0.0000e+00, -2.0409e-04]],

         [[ 0.0000e+00,  4.2114e-03,  0.0000e+00,  ...,  2.7832e-02,
            0.0000e+00, -7.2266e-02]]]], device='cuda:0', dtype=torch.bfloat16))
here here
no back here:
hit for state: 0
before conv_state: tensor([[[-14.0625, -14.5000, -14.0625],
         [ 12.5625,  12.8125,  12.5625],
         [  5.0938,   2.2500,   5.0938],
         ...,
         [-16.2500, -13.5625, -16.2500],
         [ 14.8125,  12.6250,  14.8125],
         [  7.5625,   9.5625,   7.5625]]], device='cuda:0',
       dtype=torch.bfloat16, grad_fn=<AsStridedBackward0>)
before ssm_state: tensor([[[ 6.9141e-01,  5.3406e-05,  4.9023e-01,  ..., -3.1233e-05,
           1.6327e-03, -3.3188e-04],
         [ 4.7740e-15,  2.0303e-07,  1.6172e-28,  ...,  9.6112e-07,
           0.0000e+00, -1.3947e-05],
         [ 7.1094e-01,  2.8076e-03,  5.2734e-01,  ..., -1.4496e-03,
           2.5368e-04, -5.8899e-03],
         ...,
         [ 0.0000e+00, -1.0132e-02,  0.0000e+00,  ...,  3.1738e-02,
           0.0000e+00, -8.8379e-02],
         [ 7.4726e-32, -4.1485e-05,  0.0000e+00,  ...,  1.2994e-05,
           0.0000e+00, -2.0409e-04],
         [ 0.0000e+00,  4.2114e-03,  0.0000e+00,  ...,  2.7832e-02,
           0.0000e+00, -7.2266e-02]]], device='cuda:0', dtype=torch.bfloat16)
x.stride0: (1, 1, 1)
x.shape: torch.Size([1, 1, 3584])
x.stride1: (7168, 3584, 1)
x.stride2: (7168, 1, 3584)
conv_state1: (10752, 1, 3584)
conv_state2: (10752, 1, 3584)
conv_state3: (10752, 1, 3584)
conv_state: torch.Size([1, 3584, 3])
conv1d_out: torch.Size([1, 3584, 1])
rearrange(conv1d_out, 'b d l -> (b l) d') shape: torch.Size([1, 3584])
x_proj_weight shape: torch.Size([144, 3584])
x_dbl: torch.Size([1, 144])
delta_proj_weight @ x_dbl[:, :delta_rank].t(): torch.Size([3584, 1])
dsfxxxxxxdfs
after conv_state: tensor([[[-14.5000, -14.0625, -16.3750],
         [ 12.8125,  12.5625,  10.5625],
         [  2.2500,   5.0938,   5.3125],
         ...,
         [-13.5625, -16.2500, -17.5000],
         [ 12.6250,  14.8125,  13.9375],
         [  9.5625,   7.5625,   8.8750]]], device='cuda:0',
       dtype=torch.bfloat16, grad_fn=<AsStridedBackward0>)
after ssm_state: tensor([[[ 4.9440e-17,  5.3406e-05,  1.9185e+21,  ..., -3.0994e-05,
          -7.5591e+11, -3.2997e-04],
         [ 2.0156e+00,  2.0210e-07,  3.9690e-15,  ...,  9.4995e-07,
          -3.0223e+24, -1.4782e-05],
         [-2.5045e-17,  2.7008e-03, -1.2806e-09,  ..., -1.0834e-03,
          -1.9806e-13, -4.3945e-03],
         ...,
         [ 8.4180e+29, -8.9722e-03, -7.8963e-33,  ...,  2.5391e-02,
          -4.3165e+21, -7.0312e-02],
         [-1.1372e+17, -3.9101e-05,  2.1601e+08,  ...,  8.5235e-06,
           3.2877e-38, -1.4019e-04],
         [ 2.3315e-15,  2.5940e-03, -1.6358e+08,  ...,  1.1780e-02,
           2.5839e+13, -3.0640e-02]]], device='cuda:0', dtype=torch.bfloat16)
after conv from cache: tensor([[[-14.5000, -14.0625, -16.3750],
         [ 12.8125,  12.5625,  10.5625],
         [  2.2500,   5.0938,   5.3125],
         ...,
         [-13.5625, -16.2500, -17.5000],
         [ 12.6250,  14.8125,  13.9375],
         [  9.5625,   7.5625,   8.8750]]], device='cuda:0',
       dtype=torch.bfloat16, grad_fn=<AsStridedBackward0>)
after ssm_state from cache: tensor([[[[ 4.9440e-17,  5.3406e-05,  1.9185e+21,  ..., -3.0994e-05,
           -7.5591e+11, -3.2997e-04]],

         [[ 2.0156e+00,  2.0210e-07,  3.9690e-15,  ...,  9.4995e-07,
           -3.0223e+24, -1.4782e-05]],

         [[-2.5045e-17,  2.7008e-03, -1.2806e-09,  ..., -1.0834e-03,
           -1.9806e-13, -4.3945e-03]],

         ...,

         [[ 8.4180e+29, -8.9722e-03, -7.8963e-33,  ...,  2.5391e-02,
           -4.3165e+21, -7.0312e-02]],

         [[-1.1372e+17, -3.9101e-05,  2.1601e+08,  ...,  8.5235e-06,
            3.2877e-38, -1.4019e-04]],

         [[ 2.3315e-15,  2.5940e-03, -1.6358e+08,  ...,  1.1780e-02,
            2.5839e+13, -3.0640e-02]]]], device='cuda:0', dtype=torch.bfloat16)
here here
no back here:
hit for state: 1
before conv_state: tensor([[[-4.5312, -4.9688, -5.0938],
         [ 7.5000,  7.0312,  7.6250],
         [ 2.0000,  1.7969,  1.7031],
         ...,
         [-6.6250, -4.6250, -6.4062],
         [-3.9062, -5.6875, -4.5312],
         [ 2.4531, -1.6250,  1.6719]]], device='cuda:0', dtype=torch.bfloat16,
       grad_fn=<AsStridedBackward0>)
before ssm_state: tensor([[[ 3.1471e-04, -1.6098e-03,  5.6997e-07,  ..., -1.0300e-03,
           0.0000e+00, -2.9297e-03],
         [ 0.0000e+00, -4.9591e-04,  0.0000e+00,  ..., -1.3580e-03,
           0.0000e+00, -7.1716e-03],
         [ 1.5280e-10, -1.5503e-02,  4.8789e-19,  ..., -9.3994e-03,
           0.0000e+00, -1.4404e-02],
         ...,
         [ 9.4250e-07, -8.3618e-03,  5.3888e-11,  ..., -2.0117e-06,
           0.0000e+00, -1.3855e-02],
         [ 1.0710e-08, -5.9814e-03,  2.7617e-15,  ..., -8.5449e-03,
           0.0000e+00, -2.8564e-02],
         [ 6.6223e-03, -1.6098e-03,  7.5340e-05,  ..., -3.4485e-03,
           1.0970e-30, -3.1006e-02]]], device='cuda:0', dtype=torch.bfloat16)
x.stride0: (1, 1, 1)
x.shape: torch.Size([1, 1, 3584])
x.stride1: (7168, 3584, 1)
x.stride2: (7168, 1, 3584)
conv_state1: (10752, 1, 3584)
conv_state2: (10752, 1, 3584)
conv_state3: (10752, 1, 3584)
conv_state: torch.Size([1, 3584, 3])
conv1d_out: torch.Size([1, 3584, 1])
rearrange(conv1d_out, 'b d l -> (b l) d') shape: torch.Size([1, 3584])
x_proj_weight shape: torch.Size([144, 3584])
x_dbl: torch.Size([1, 144])
delta_proj_weight @ x_dbl[:, :delta_rank].t(): torch.Size([3584, 1])
dsfxxxxxxdfs
after conv_state: tensor([[[-4.9688, -5.0938, -3.9062],
         [ 7.0312,  7.6250,  9.3750],
         [ 1.7969,  1.7031,  4.0312],
         ...,
         [-4.6250, -6.4062, -8.6250],
         [-5.6875, -4.5312, -7.8125],
         [-1.6250,  1.6719, -0.3242]]], device='cuda:0', dtype=torch.bfloat16,
       grad_fn=<AsStridedBackward0>)
after ssm_state: tensor([[[-6.5920e+28, -1.5869e-03, -1.0708e+29,  ..., -9.6512e-04,
          -1.0458e+31, -2.7924e-03],
         [-4.3637e+12, -4.5776e-04,  1.3867e-01,  ..., -1.0986e-03,
          -8.5856e-10, -6.4392e-03],
         [ 1.0571e-17, -1.4709e-02, -1.0952e-06,  ..., -5.9204e-03,
           2.3040e+04, -9.1553e-03],
         ...,
         [ 1.0757e-07, -8.1177e-03,  8.8477e+36,  ..., -1.8701e-06,
           7.9672e-10, -1.3977e-02],
         [ 5.9392e+05, -5.7983e-03,  5.2112e+20,  ..., -6.5918e-03,
          -4.6857e-09, -2.2217e-02],
         [-9.0597e+08, -1.2054e-03,  1.8863e+33,  ..., -2.3499e-03,
           2.2041e-38, -2.3071e-02]]], device='cuda:0', dtype=torch.bfloat16)
after conv from cache: tensor([[[-4.9688, -5.0938, -3.9062],
         [ 7.0312,  7.6250,  9.3750],
         [ 1.7969,  1.7031,  4.0312],
         ...,
         [-4.6250, -6.4062, -8.6250],
         [-5.6875, -4.5312, -7.8125],
         [-1.6250,  1.6719, -0.3242]]], device='cuda:0', dtype=torch.bfloat16,
       grad_fn=<AsStridedBackward0>)
after ssm_state from cache: tensor([[[[-6.5920e+28, -1.5869e-03, -1.0708e+29,  ..., -9.6512e-04,
           -1.0458e+31, -2.7924e-03]],

         [[-4.3637e+12, -4.5776e-04,  1.3867e-01,  ..., -1.0986e-03,
           -8.5856e-10, -6.4392e-03]],

         [[ 1.0571e-17, -1.4709e-02, -1.0952e-06,  ..., -5.9204e-03,
            2.3040e+04, -9.1553e-03]],

         ...,

         [[ 1.0757e-07, -8.1177e-03,  8.8477e+36,  ..., -1.8701e-06,
            7.9672e-10, -1.3977e-02]],

         [[ 5.9392e+05, -5.7983e-03,  5.2112e+20,  ..., -6.5918e-03,
           -4.6857e-09, -2.2217e-02]],

         [[-9.0597e+08, -1.2054e-03,  1.8863e+33,  ..., -2.3499e-03,
            2.2041e-38, -2.3071e-02]]]], device='cuda:0', dtype=torch.bfloat16)
here here
no back here:
hit for state: 2
before conv_state: tensor([[[-6.4688, -8.4375, -8.0000],
         [ 1.1484, -0.3125,  0.4902],
         [ 3.6562,  3.5781,  4.9688],
         ...,
         [-5.6562, -5.8125, -6.5625],
         [-5.2812, -5.6562, -5.3125],
         [-1.8281, -1.8203, -1.5859]]], device='cuda:0', dtype=torch.bfloat16,
       grad_fn=<AsStridedBackward0>)
before ssm_state: tensor([[[ 1.2436e-03, -9.6512e-04,  7.2122e-06,  ..., -7.7724e-05,
           4.4590e-25,  5.0354e-03],
         [ 2.3145e-01,  4.3297e-04,  4.6875e-02,  ...,  2.0027e-04,
           4.5300e-06,  7.2937e-03],
         [ 0.0000e+00, -1.0437e-02,  0.0000e+00,  ...,  1.3428e-02,
           0.0000e+00,  8.8379e-02],
         ...,
         [ 0.0000e+00, -3.4027e-03,  0.0000e+00,  ...,  1.0071e-03,
           0.0000e+00,  1.2939e-02],
         [ 7.9297e-01, -2.2583e-03,  6.2109e-01,  ...,  2.7771e-03,
           5.5847e-03,  1.8066e-02],
         [ 3.4027e-03, -1.6724e-02,  4.6253e-05,  ...,  1.2436e-03,
           2.5579e-34,  2.0874e-02]]], device='cuda:0', dtype=torch.bfloat16)
x.stride0: (1, 1, 1)
x.shape: torch.Size([1, 1, 3584])
x.stride1: (7168, 3584, 1)
x.stride2: (7168, 1, 3584)
conv_state1: (10752, 1, 3584)
conv_state2: (10752, 1, 3584)
conv_state3: (10752, 1, 3584)
conv_state: torch.Size([1, 3584, 3])
conv1d_out: torch.Size([1, 3584, 1])
rearrange(conv1d_out, 'b d l -> (b l) d') shape: torch.Size([1, 3584])
x_proj_weight shape: torch.Size([144, 3584])
x_dbl: torch.Size([1, 144])
delta_proj_weight @ x_dbl[:, :delta_rank].t(): torch.Size([3584, 1])
dsfxxxxxxdfs
after conv_state: tensor([[[-8.4375, -8.0000, -9.3125],
         [-0.3125,  0.4902,  2.5938],
         [ 3.5781,  4.9688,  5.4688],
         ...,
         [-5.8125, -6.5625, -7.4375],
         [-5.6562, -5.3125, -6.6562],
         [-1.8203, -1.5859, -2.4531]]], device='cuda:0', dtype=torch.bfloat16,
       grad_fn=<AsStridedBackward0>)
after ssm_state: tensor([[[ 3.0494e+11, -9.6130e-04, -1.7205e-21,  ..., -7.5340e-05,
           6.4097e-34,  4.9133e-03],
         [-1.5281e+27,  4.1771e-04, -1.8587e+25,  ...,  1.9360e-04,
          -4.1633e-16,  7.1411e-03],
         [-2.1730e+17, -1.0376e-02, -5.3750e+01,  ...,  1.3306e-02,
          -9.9431e+30,  8.7402e-02],
         ...,
         [ 6.8954e+36, -2.3651e-03, -3.0829e+33,  ...,  7.3624e-04,
          -2.2165e-07,  1.3733e-02],
         [ 6.9814e-22, -2.2278e-03,  1.9907e+06,  ...,  2.4719e-03,
           6.3488e+05,  1.1658e-02],
         [-5.6493e+20, -1.4465e-02,  6.9500e+01,  ...,  1.0071e-03,
          -7.7248e-05, -8.7891e-03]]], device='cuda:0', dtype=torch.bfloat16)
after conv from cache: tensor([[[-8.4375, -8.0000, -9.3125],
         [-0.3125,  0.4902,  2.5938],
         [ 3.5781,  4.9688,  5.4688],
         ...,
         [-5.8125, -6.5625, -7.4375],
         [-5.6562, -5.3125, -6.6562],
         [-1.8203, -1.5859, -2.4531]]], device='cuda:0', dtype=torch.bfloat16,
       grad_fn=<AsStridedBackward0>)
after ssm_state from cache: tensor([[[[ 3.0494e+11, -9.6130e-04, -1.7205e-21,  ..., -7.5340e-05,
            6.4097e-34,  4.9133e-03]],

         [[-1.5281e+27,  4.1771e-04, -1.8587e+25,  ...,  1.9360e-04,
           -4.1633e-16,  7.1411e-03]],

         [[-2.1730e+17, -1.0376e-02, -5.3750e+01,  ...,  1.3306e-02,
           -9.9431e+30,  8.7402e-02]],

         ...,

         [[ 6.8954e+36, -2.3651e-03, -3.0829e+33,  ...,  7.3624e-04,
           -2.2165e-07,  1.3733e-02]],

         [[ 6.9814e-22, -2.2278e-03,  1.9907e+06,  ...,  2.4719e-03,
            6.3488e+05,  1.1658e-02]],

         [[-5.6493e+20, -1.4465e-02,  6.9500e+01,  ...,  1.0071e-03,
           -7.7248e-05, -8.7891e-03]]]], device='cuda:0', dtype=torch.bfloat16)
here here
no back here:
hit for state: 3
before conv_state: tensor([[[-0.3457, -0.8320, -2.0000],
         [-4.5000, -3.0781, -3.9688],
         [ 0.8125,  1.4609,  0.0359],
         ...,
         [ 3.2656,  1.5781,  1.5156],
         [-1.9062, -2.1719, -2.0000],
         [ 1.9531,  1.9766,  2.4062]]], device='cuda:0', dtype=torch.bfloat16,
       grad_fn=<AsStridedBackward0>)
before ssm_state: tensor([[[ 4.3945e-02,  1.8120e-04,  8.4839e-03,  ...,  2.1820e-03,
           3.3339e-18, -7.8125e-03],
         [ 3.4532e-12, -1.2024e-02,  2.3527e-26,  ...,  1.2741e-03,
           0.0000e+00, -3.5248e-03],
         [ 7.0473e-18, -6.2866e-03,  3.7471e-30,  ..., -2.2278e-03,
           0.0000e+00,  1.1230e-02],
         ...,
         [ 9.5063e-16, -8.2397e-03,  1.3435e-30,  ...,  1.6708e-03,
           0.0000e+00,  5.3101e-03],
         [ 4.7740e-15, -4.6692e-03,  2.9503e-28,  ...,  1.9836e-03,
           0.0000e+00,  1.3062e-02],
         [ 9.6130e-04,  5.4932e-03,  2.4587e-06,  ..., -3.9673e-03,
           0.0000e+00,  7.9956e-03]]], device='cuda:0', dtype=torch.bfloat16)
x.stride0: (1, 1, 1)
x.shape: torch.Size([1, 1, 3584])
x.stride1: (7168, 3584, 1)
x.stride2: (7168, 1, 3584)
conv_state1: (10752, 1, 3584)
conv_state2: (10752, 1, 3584)
conv_state3: (10752, 1, 3584)
conv_state: torch.Size([1, 3584, 3])
conv1d_out: torch.Size([1, 3584, 1])
rearrange(conv1d_out, 'b d l -> (b l) d') shape: torch.Size([1, 3584])
x_proj_weight shape: torch.Size([144, 3584])
x_dbl: torch.Size([1, 144])
delta_proj_weight @ x_dbl[:, :delta_rank].t(): torch.Size([3584, 1])
dsfxxxxxxdfs
after conv_state: tensor([[[-0.8320, -2.0000, -1.3047],
         [-3.0781, -3.9688, -3.5938],
         [ 1.4609,  0.0359,  0.3984],
         ...,
         [ 1.5781,  1.5156,  4.1562],
         [-2.1719, -2.0000, -0.8203],
         [ 1.9766,  2.4062,  1.2656]]], device='cuda:0', dtype=torch.bfloat16,
       grad_fn=<AsStridedBackward0>)
after ssm_state: tensor([[[-1.3977e-02,  1.8024e-04, -2.0231e+14,  ...,  2.1362e-03,
           3.6671e+34, -7.7209e-03],
         [-1.8211e+12, -1.1658e-02,  6.5523e+22,  ...,  1.2283e-03,
          -7.9379e-30, -3.4332e-03],
         [-8.5110e-18, -6.0730e-03,  1.4236e-30,  ..., -1.6479e-03,
           1.6431e-14,  1.2451e-02],
         ...,
         [-2.0011e-20, -8.1787e-03, -9.5526e-32,  ...,  1.6556e-03,
          -2.8882e+34,  5.3406e-03],
         [-1.9140e+16, -4.5776e-03,  1.1328e+00,  ...,  1.7853e-03,
          -2.5984e+04,  1.3672e-02],
         [ 2.9011e-20,  4.8218e-03, -4.2071e+26,  ..., -3.1738e-03,
          -2.7134e+17,  8.4839e-03]]], device='cuda:0', dtype=torch.bfloat16)
after conv from cache: tensor([[[-0.8320, -2.0000, -1.3047],
         [-3.0781, -3.9688, -3.5938],
         [ 1.4609,  0.0359,  0.3984],
         ...,
         [ 1.5781,  1.5156,  4.1562],
         [-2.1719, -2.0000, -0.8203],
         [ 1.9766,  2.4062,  1.2656]]], device='cuda:0', dtype=torch.bfloat16,
       grad_fn=<AsStridedBackward0>)
after ssm_state from cache: tensor([[[[-1.3977e-02,  1.8024e-04, -2.0231e+14,  ...,  2.1362e-03,
            3.6671e+34, -7.7209e-03]],

         [[-1.8211e+12, -1.1658e-02,  6.5523e+22,  ...,  1.2283e-03,
           -7.9379e-30, -3.4332e-03]],

         [[-8.5110e-18, -6.0730e-03,  1.4236e-30,  ..., -1.6479e-03,
            1.6431e-14,  1.2451e-02]],

         ...,

         [[-2.0011e-20, -8.1787e-03, -9.5526e-32,  ...,  1.6556e-03,
           -2.8882e+34,  5.3406e-03]],

         [[-1.9140e+16, -4.5776e-03,  1.1328e+00,  ...,  1.7853e-03,
           -2.5984e+04,  1.3672e-02]],

         [[ 2.9011e-20,  4.8218e-03, -4.2071e+26,  ..., -3.1738e-03,
           -2.7134e+17,  8.4839e-03]]]], device='cuda:0', dtype=torch.bfloat16)
here here
no back here:
hit for state: 4
before conv_state: tensor([[[-2.2812, -1.6875, -3.0312],
         [ 0.9297,  0.5273,  1.0703],
         [-2.1250, -1.3125, -2.0156],
         ...,
         [ 0.3652,  0.2109,  1.7656],
         [-1.9922, -1.2578, -0.5195],
         [ 0.8320,  1.7500,  2.8906]]], device='cuda:0', dtype=torch.bfloat16,
       grad_fn=<AsStridedBackward0>)
before ssm_state: tensor([[[ 7.1850e-11,  4.6082e-03,  2.2031e-16,  ..., -9.1553e-03,
           0.0000e+00, -6.4392e-03],
         [ 4.8572e-16, -7.5684e-03,  4.2648e-30,  ...,  1.3580e-03,
           0.0000e+00, -1.3962e-03],
         [ 4.9438e-03,  1.4038e-02,  3.3617e-05,  ..., -4.2114e-03,
           6.0080e-27, -1.4343e-03],
         ...,
         [ 3.9844e-01,  1.3256e-04,  1.4062e-01,  ..., -6.5613e-04,
           5.3272e-07,  4.4250e-04],
         [ 2.5368e-04,  3.1586e-03,  6.6124e-08,  ..., -6.4697e-03,
           0.0000e+00, -5.4016e-03],
         [ 1.6499e-04,  8.3008e-03,  5.0059e-08,  ..., -1.2390e-02,
           0.0000e+00, -4.9973e-04]]], device='cuda:0', dtype=torch.bfloat16)
x.stride0: (1, 1, 1)
x.shape: torch.Size([1, 1, 3584])
x.stride1: (7168, 3584, 1)
x.stride2: (7168, 1, 3584)
conv_state1: (10752, 1, 3584)
conv_state2: (10752, 1, 3584)
conv_state3: (10752, 1, 3584)
conv_state: torch.Size([1, 3584, 3])
conv1d_out: torch.Size([1, 3584, 1])
rearrange(conv1d_out, 'b d l -> (b l) d') shape: torch.Size([1, 3584])
x_proj_weight shape: torch.Size([144, 3584])
x_dbl: torch.Size([1, 144])
delta_proj_weight @ x_dbl[:, :delta_rank].t(): torch.Size([3584, 1])
dsfxxxxxxdfs
after conv_state: tensor([[[-1.6875, -3.0312, -2.0938],
         [ 0.5273,  1.0703,  0.2354],
         [-1.3125, -2.0156, -1.8906],
         ...,
         [ 0.2109,  1.7656, -0.2812],
         [-1.2578, -0.5195, -2.3281],
         [ 1.7500,  2.8906,  2.6094]]], device='cuda:0', dtype=torch.bfloat16,
       grad_fn=<AsStridedBackward0>)
after ssm_state: tensor([[[-5.5856e+30,  4.4556e-03,  5.1274e+34,  ..., -7.6904e-03,
           3.5167e-06, -4.4861e-03],
         [ 1.1839e-23, -6.2256e-03,  4.5262e-07,  ...,  9.9945e-04,
           1.6178e-19, -1.8501e-04],
         [-5.7553e+11,  1.3428e-02, -9.1773e+20,  ..., -3.1128e-03,
           8.2207e+25, -4.3640e-03],
         ...,
         [ 1.1013e-13,  6.7234e-05,  1.3304e+14,  ..., -1.9550e-04,
           6.6588e-23, -3.3264e-03],
         [ 6.1892e+36,  3.0823e-03,  1.2268e-26,  ..., -5.4932e-03,
           4.3945e-01, -2.4719e-03],
         [-4.5002e+31,  7.1106e-03, -7.8993e-35,  ..., -9.2773e-03,
          -1.2917e-20,  1.5564e-03]]], device='cuda:0', dtype=torch.bfloat16)
after conv from cache: tensor([[[-1.6875, -3.0312, -2.0938],
         [ 0.5273,  1.0703,  0.2354],
         [-1.3125, -2.0156, -1.8906],
         ...,
         [ 0.2109,  1.7656, -0.2812],
         [-1.2578, -0.5195, -2.3281],
         [ 1.7500,  2.8906,  2.6094]]], device='cuda:0', dtype=torch.bfloat16,
       grad_fn=<AsStridedBackward0>)
after ssm_state from cache: tensor([[[[-5.5856e+30,  4.4556e-03,  5.1274e+34,  ..., -7.6904e-03,
            3.5167e-06, -4.4861e-03]],

         [[ 1.1839e-23, -6.2256e-03,  4.5262e-07,  ...,  9.9945e-04,
            1.6178e-19, -1.8501e-04]],

         [[-5.7553e+11,  1.3428e-02, -9.1773e+20,  ..., -3.1128e-03,
            8.2207e+25, -4.3640e-03]],

         ...,

         [[ 1.1013e-13,  6.7234e-05,  1.3304e+14,  ..., -1.9550e-04,
            6.6588e-23, -3.3264e-03]],

         [[ 6.1892e+36,  3.0823e-03,  1.2268e-26,  ..., -5.4932e-03,
            4.3945e-01, -2.4719e-03]],

         [[-4.5002e+31,  7.1106e-03, -7.8993e-35,  ..., -9.2773e-03,
           -1.2917e-20,  1.5564e-03]]]], device='cuda:0', dtype=torch.bfloat16)
here here
no back here:
hit for state: 5
before conv_state: tensor([[[-0.4824, -2.5625, -1.7812],
         [-2.2812, -3.2500, -0.9883],
         [-0.1240,  2.4531, -0.1777],
         ...,
         [ 0.6172,  0.4570,  0.5039],
         [-3.5781, -3.5469, -3.7969],
         [-0.0164, -2.3281,  0.8984]]], device='cuda:0', dtype=torch.bfloat16,
       grad_fn=<AsStridedBackward0>)
before ssm_state: tensor([[[ 3.2783e-07,  1.2146e-02,  2.3022e-12,  ..., -5.9204e-03,
           0.0000e+00,  1.5869e-03],
         [ 8.7500e-01,  2.0599e-03,  7.9297e-01,  ..., -4.5471e-03,
           1.7871e-01,  2.8839e-03],
         [ 1.1108e-02, -2.0142e-03,  1.8239e-05,  ..., -7.4387e-04,
           2.3603e-27,  8.9722e-03],
         ...,
         [ 4.2480e-02,  5.7678e-03,  4.8828e-03,  ...,  6.6833e-03,
           6.4798e-20,  5.1575e-03],
         [ 3.0078e-01,  1.1108e-02,  4.5166e-02,  ..., -7.2632e-03,
           6.4756e-10, -2.4109e-03],
         [ 1.1253e-04,  8.7280e-03,  1.6647e-08,  ...,  8.8501e-03,
           0.0000e+00,  1.9684e-03]]], device='cuda:0', dtype=torch.bfloat16)
x.stride0: (1, 1, 1)
x.shape: torch.Size([1, 1, 3584])
x.stride1: (7168, 3584, 1)
x.stride2: (7168, 1, 3584)
conv_state1: (10752, 1, 3584)
conv_state2: (10752, 1, 3584)
conv_state3: (10752, 1, 3584)
conv_state: torch.Size([1, 3584, 3])
conv1d_out: torch.Size([1, 3584, 1])
rearrange(conv1d_out, 'b d l -> (b l) d') shape: torch.Size([1, 3584])
x_proj_weight shape: torch.Size([144, 3584])
x_dbl: torch.Size([1, 144])
delta_proj_weight @ x_dbl[:, :delta_rank].t(): torch.Size([3584, 1])
dsfxxxxxxdfs
after conv_state: tensor([[[-2.5625, -1.7812, -0.9766],
         [-3.2500, -0.9883, -1.5312],
         [ 2.4531, -0.1777, -1.5938],
         ...,
         [ 0.4570,  0.5039,  0.0422],
         [-3.5469, -3.7969, -3.0938],
         [-2.3281,  0.8984,  1.2500]]], device='cuda:0', dtype=torch.bfloat16,
       grad_fn=<AsStridedBackward0>)
after ssm_state: tensor([[[-1.7999e-20,  1.1597e-02, -6.7791e+37,  ..., -4.3030e-03,
           2.1487e-25,  5.2795e-03],
         [-1.8048e-12,  1.4496e-03, -2.3959e+18,  ..., -2.5330e-03,
           1.1107e+25, -7.6904e-03],
         [ 1.6764e-06, -2.0142e-03,  3.5286e-04,  ..., -7.4005e-04,
           1.0259e-09,  8.9722e-03],
         ...,
         [-1.6356e+35,  5.7068e-03, -5.5611e+26,  ...,  6.5918e-03,
           3.0375e+35,  5.0964e-03],
         [ 2.1444e+20,  1.1047e-02, -2.3981e+21,  ..., -7.0496e-03,
           1.6875e+35, -1.9989e-03],
         [ 1.5769e-33,  8.4229e-03,  9.5757e-16,  ...,  8.3008e-03,
           7.1338e-12,  1.0910e-03]]], device='cuda:0', dtype=torch.bfloat16)
after conv from cache: tensor([[[-2.5625, -1.7812, -0.9766],
         [-3.2500, -0.9883, -1.5312],
         [ 2.4531, -0.1777, -1.5938],
         ...,
         [ 0.4570,  0.5039,  0.0422],
         [-3.5469, -3.7969, -3.0938],
         [-2.3281,  0.8984,  1.2500]]], device='cuda:0', dtype=torch.bfloat16,
       grad_fn=<AsStridedBackward0>)
after ssm_state from cache: tensor([[[[-1.7999e-20,  1.1597e-02, -6.7791e+37,  ..., -4.3030e-03,
            2.1487e-25,  5.2795e-03]],

         [[-1.8048e-12,  1.4496e-03, -2.3959e+18,  ..., -2.5330e-03,
            1.1107e+25, -7.6904e-03]],

         [[ 1.6764e-06, -2.0142e-03,  3.5286e-04,  ..., -7.4005e-04,
            1.0259e-09,  8.9722e-03]],

         ...,

         [[-1.6356e+35,  5.7068e-03, -5.5611e+26,  ...,  6.5918e-03,
            3.0375e+35,  5.0964e-03]],

         [[ 2.1444e+20,  1.1047e-02, -2.3981e+21,  ..., -7.0496e-03,
            1.6875e+35, -1.9989e-03]],

         [[ 1.5769e-33,  8.4229e-03,  9.5757e-16,  ...,  8.3008e-03,
            7.1338e-12,  1.0910e-03]]]], device='cuda:0', dtype=torch.bfloat16)
here here
no back here:
hit for state: 6
before conv_state: tensor([[[-2.8438, -0.4258,  0.2773],
         [ 3.2656,  2.5156,  2.3125],
         [ 1.6562, -0.6484,  1.9375],
         ...,
         [ 0.4922, -0.5859,  0.4902],
         [-0.5820, -0.5117,  0.5977],
         [ 1.4141,  0.8711, -0.0172]]], device='cuda:0', dtype=torch.bfloat16,
       grad_fn=<AsStridedBackward0>)
before ssm_state: tensor([[[ 9.1735e-08,  1.5991e-02,  7.1609e-15,  ...,  1.4771e-02,
           0.0000e+00,  9.1553e-03],
         [ 7.9364e-17, -2.0447e-03,  1.4073e-27,  ...,  6.9824e-02,
           0.0000e+00, -8.9111e-03],
         [ 2.2072e-07,  1.2329e-02,  1.8457e-15,  ..., -9.2163e-03,
           0.0000e+00, -2.2736e-03],
         ...,
         [ 2.1240e-02,  1.5259e-02,  2.9182e-04,  ..., -3.8452e-03,
           2.5436e-23, -5.4550e-04],
         [ 2.8906e-01, -2.3041e-03,  1.1279e-01,  ...,  5.0354e-03,
           1.1595e-07, -4.5204e-04],
         [ 1.7073e-21,  4.5776e-03,  0.0000e+00,  ..., -6.7871e-02,
           0.0000e+00,  1.3550e-02]]], device='cuda:0', dtype=torch.bfloat16)
x.stride0: (1, 1, 1)
x.shape: torch.Size([1, 1, 3584])
x.stride1: (7168, 3584, 1)
x.stride2: (7168, 1, 3584)
conv_state1: (10752, 1, 3584)
conv_state2: (10752, 1, 3584)
conv_state3: (10752, 1, 3584)
conv_state: torch.Size([1, 3584, 3])
conv1d_out: torch.Size([1, 3584, 1])
rearrange(conv1d_out, 'b d l -> (b l) d') shape: torch.Size([1, 3584])
x_proj_weight shape: torch.Size([144, 3584])
x_dbl: torch.Size([1, 144])
delta_proj_weight @ x_dbl[:, :delta_rank].t(): torch.Size([3584, 1])
dsfxxxxxxdfs
after conv_state: tensor([[[-0.4258,  0.2773,  2.8906],
         [ 2.5156,  2.3125,  2.3125],
         [-0.6484,  1.9375,  2.9062],
         ...,
         [-0.5859,  0.4902, -0.6602],
         [-0.5117,  0.5977,  0.2852],
         [ 0.8711, -0.0172,  0.8203]]], device='cuda:0', dtype=torch.bfloat16,
       grad_fn=<AsStridedBackward0>)
after ssm_state: tensor([[[-2.5835e-20,  1.5625e-02, -8.2670e-19,  ...,  1.2451e-02,
           2.5781e-01,  7.9346e-03],
         [-3.1471e-05, -1.7090e-03,  4.8266e-26,  ...,  4.9805e-02,
           8.1065e+16, -1.0742e-02],
         [-3.0424e+32,  1.2024e-02,  1.2925e-24,  ..., -8.1177e-03,
          -3.7315e-33, -2.0447e-03],
         ...,
         [ 1.6171e+28,  1.5015e-02,  1.1210e-34,  ..., -3.7231e-03,
           1.6645e+19, -7.7438e-04],
         [-9.7451e+30, -2.2736e-03, -6.7793e-32,  ...,  4.6997e-03,
          -6.2390e+07, -3.9291e-04],
         [ 2.1567e+27,  4.2419e-03, -2.3365e-05,  ..., -5.9570e-02,
           2.2016e+05,  1.1230e-02]]], device='cuda:0', dtype=torch.bfloat16)
after conv from cache: tensor([[[-0.4258,  0.2773,  2.8906],
         [ 2.5156,  2.3125,  2.3125],
         [-0.6484,  1.9375,  2.9062],
         ...,
         [-0.5859,  0.4902, -0.6602],
         [-0.5117,  0.5977,  0.2852],
         [ 0.8711, -0.0172,  0.8203]]], device='cuda:0', dtype=torch.bfloat16,
       grad_fn=<AsStridedBackward0>)
after ssm_state from cache: tensor([[[[-2.5835e-20,  1.5625e-02, -8.2670e-19,  ...,  1.2451e-02,
            2.5781e-01,  7.9346e-03]],

         [[-3.1471e-05, -1.7090e-03,  4.8266e-26,  ...,  4.9805e-02,
            8.1065e+16, -1.0742e-02]],

         [[-3.0424e+32,  1.2024e-02,  1.2925e-24,  ..., -8.1177e-03,
           -3.7315e-33, -2.0447e-03]],

         ...,

         [[ 1.6171e+28,  1.5015e-02,  1.1210e-34,  ..., -3.7231e-03,
            1.6645e+19, -7.7438e-04]],

         [[-9.7451e+30, -2.2736e-03, -6.7793e-32,  ...,  4.6997e-03,
           -6.2390e+07, -3.9291e-04]],

         [[ 2.1567e+27,  4.2419e-03, -2.3365e-05,  ..., -5.9570e-02,
            2.2016e+05,  1.1230e-02]]]], device='cuda:0', dtype=torch.bfloat16)
here here
no back here:
hit for state: 7
before conv_state: tensor([[[ 0.2109,  2.5156,  0.5820],
         [-2.4531, -4.5000, -3.5781],
         [-1.3281, -0.7109, -0.4941],
         ...,
         [-0.2441,  0.7305,  1.4297],
         [-0.6992,  0.0664, -1.3359],
         [ 2.5469,  1.3828,  1.3281]]], device='cuda:0', dtype=torch.bfloat16,
       grad_fn=<AsStridedBackward0>)
before ssm_state: tensor([[[ 2.6867e-21, -1.5198e-02,  0.0000e+00,  ..., -1.0300e-03,
           0.0000e+00,  9.7046e-03],
         [ 1.7812e-08,  1.3794e-02,  1.4528e-17,  ..., -1.1492e-04,
           0.0000e+00, -3.0975e-03],
         [ 1.2354e-01,  1.1353e-02,  2.8442e-02,  ..., -3.2654e-03,
           4.3769e-12, -1.4420e-03],
         ...,
         [ 2.1118e-02, -5.4626e-03,  1.6937e-03,  ..., -4.5013e-04,
           2.4880e-25,  1.9379e-03],
         [ 2.3592e-16,  4.1504e-03,  1.0203e-35,  ..., -3.0212e-03,
           0.0000e+00,  1.5869e-02],
         [ 6.0797e-06,  3.4668e-02,  1.0159e-14,  ..., -1.2970e-03,
           0.0000e+00, -1.1658e-02]]], device='cuda:0', dtype=torch.bfloat16)
x.stride0: (1, 1, 1)
x.shape: torch.Size([1, 1, 3584])
x.stride1: (7168, 3584, 1)
x.stride2: (7168, 1, 3584)
conv_state1: (10752, 1, 3584)
conv_state2: (10752, 1, 3584)
conv_state3: (10752, 1, 3584)
conv_state: torch.Size([1, 3584, 3])
conv1d_out: torch.Size([1, 3584, 1])
rearrange(conv1d_out, 'b d l -> (b l) d') shape: torch.Size([1, 3584])
x_proj_weight shape: torch.Size([144, 3584])
x_dbl: torch.Size([1, 144])
delta_proj_weight @ x_dbl[:, :delta_rank].t(): torch.Size([3584, 1])
dsfxxxxxxdfs
after conv_state: tensor([[[ 2.5156,  0.5820,  1.0625],
         [-4.5000, -3.5781, -5.1250],
         [-0.7109, -0.4941,  0.2500],
         ...,
         [ 0.7305,  1.4297, -0.3418],
         [ 0.0664, -1.3359,  1.1250],
         [ 1.3828,  1.3281,  0.5469]]], device='cuda:0', dtype=torch.bfloat16,
       grad_fn=<AsStridedBackward0>)
after ssm_state: tensor([[[-7.7919e+22, -1.4648e-02, -2.1282e-20,  ..., -8.1635e-04,
           1.2667e+09,  6.3782e-03],
         [-1.2877e+36,  1.0376e-02,  1.6882e+23,  ..., -6.7711e-05,
           3.7440e+03, -5.8594e-03],
         [ 2.9048e+17,  1.1230e-02, -6.4498e+33,  ..., -2.9755e-03,
           1.7873e+28, -1.4038e-03],
         ...,
         [-1.9703e+15, -2.7161e-03, -4.7705e-18,  ..., -9.8228e-05,
          -1.4426e+15,  6.4087e-03],
         [ 2.3394e-24,  4.1504e-03,  1.5668e+13,  ..., -3.0060e-03,
          -9.6244e-38,  1.5747e-02],
         [-6.0272e-04,  3.4424e-02, -2.1067e-24,  ..., -1.2817e-03,
          -3.6919e-28, -1.1597e-02]]], device='cuda:0', dtype=torch.bfloat16)
after conv from cache: tensor([[[ 2.5156,  0.5820,  1.0625],
         [-4.5000, -3.5781, -5.1250],
         [-0.7109, -0.4941,  0.2500],
         ...,
         [ 0.7305,  1.4297, -0.3418],
         [ 0.0664, -1.3359,  1.1250],
         [ 1.3828,  1.3281,  0.5469]]], device='cuda:0', dtype=torch.bfloat16,
       grad_fn=<AsStridedBackward0>)
after ssm_state from cache: tensor([[[[-7.7919e+22, -1.4648e-02, -2.1282e-20,  ..., -8.1635e-04,
            1.2667e+09,  6.3782e-03]],

         [[-1.2877e+36,  1.0376e-02,  1.6882e+23,  ..., -6.7711e-05,
            3.7440e+03, -5.8594e-03]],

         [[ 2.9048e+17,  1.1230e-02, -6.4498e+33,  ..., -2.9755e-03,
            1.7873e+28, -1.4038e-03]],

         ...,

         [[-1.9703e+15, -2.7161e-03, -4.7705e-18,  ..., -9.8228e-05,
           -1.4426e+15,  6.4087e-03]],

         [[ 2.3394e-24,  4.1504e-03,  1.5668e+13,  ..., -3.0060e-03,
           -9.6244e-38,  1.5747e-02]],

         [[-6.0272e-04,  3.4424e-02, -2.1067e-24,  ..., -1.2817e-03,
           -3.6919e-28, -1.1597e-02]]]], device='cuda:0', dtype=torch.bfloat16)
here here
no back here:
hit for state: 8
before conv_state: tensor([[[ 1.7812,  0.7461, -1.7344],
         [-1.4531, -1.5078,  0.0496],
         [-0.1494,  0.1338, -0.7461],
         ...,
         [ 2.7500,  5.8125,  7.2188],
         [ 2.8750,  1.5469,  2.1406],
         [ 3.4688,  3.9531,  2.4844]]], device='cuda:0', dtype=torch.bfloat16,
       grad_fn=<AsStridedBackward0>)
before ssm_state: tensor([[[ 1.2305e-01,  3.8818e-02,  1.4877e-03,  ...,  3.8605e-03,
           4.7531e-16, -1.5198e-02],
         [ 3.6865e-02,  2.1973e-03,  5.1498e-04,  ...,  7.5531e-04,
           2.1455e-24, -5.7068e-03],
         [ 8.4766e-01,  5.0964e-03,  7.0703e-01,  ...,  3.6011e-03,
           6.6895e-02,  4.4441e-04],
         ...,
         [ 2.1680e-01, -2.4048e-02,  6.7383e-02,  ..., -6.9885e-03,
           1.1156e-12, -1.2085e-02],
         [ 1.2012e-01,  2.8931e-02,  2.3315e-02,  ...,  1.0254e-02,
           4.6185e-14, -1.1047e-02],
         [ 4.6384e-11,  3.0029e-02,  8.9363e-32,  ...,  4.1809e-03,
           0.0000e+00, -7.3853e-03]]], device='cuda:0', dtype=torch.bfloat16)
x.stride0: (1, 1, 1)
x.shape: torch.Size([1, 1, 3584])
x.stride1: (7168, 3584, 1)
x.stride2: (7168, 1, 3584)
conv_state1: (10752, 1, 3584)
conv_state2: (10752, 1, 3584)
conv_state3: (10752, 1, 3584)
conv_state: torch.Size([1, 3584, 3])
conv1d_out: torch.Size([1, 3584, 1])
rearrange(conv1d_out, 'b d l -> (b l) d') shape: torch.Size([1, 3584])
x_proj_weight shape: torch.Size([144, 3584])
x_dbl: torch.Size([1, 144])
delta_proj_weight @ x_dbl[:, :delta_rank].t(): torch.Size([3584, 1])
dsfxxxxxxdfs
after conv_state: tensor([[[ 0.7461, -1.7344,  0.8125],
         [-1.5078,  0.0496, -0.5000],
         [ 0.1338, -0.7461, -2.4062],
         ...,
         [ 5.8125,  7.2188,  2.2656],
         [ 1.5469,  2.1406,  2.3906],
         [ 3.9531,  2.4844,  2.0625]]], device='cuda:0', dtype=torch.bfloat16,
       grad_fn=<AsStridedBackward0>)
after ssm_state: tensor([[[-1.0639e+26,  3.8574e-02, -1.3830e-21,  ...,  3.7842e-03,
          -5.9375e-01, -1.4893e-02],
         [ 2.3574e+25,  2.1515e-03, -1.4784e+04,  ...,  7.2861e-04,
          -1.7044e-32, -5.4626e-03],
         [-9.7422e+19,  5.0659e-03,  1.5622e+16,  ...,  3.5400e-03,
           2.5134e+20,  4.4250e-04],
         ...,
         [ 9.9712e-28, -2.1484e-02, -3.8926e-10,  ..., -5.5847e-03,
           2.6081e-38, -1.0315e-02],
         [ 2.2763e+11,  2.7710e-02,  1.8645e-11,  ...,  6.7749e-03,
           2.0872e-14, -6.7444e-03],
         [-1.1220e+08,  1.9409e-02,  5.1840e+37,  ...,  1.8158e-03,
          -9.9600e+02, -5.7983e-03]]], device='cuda:0', dtype=torch.bfloat16)
after conv from cache: tensor([[[ 0.7461, -1.7344,  0.8125],
         [-1.5078,  0.0496, -0.5000],
         [ 0.1338, -0.7461, -2.4062],
         ...,
         [ 5.8125,  7.2188,  2.2656],
         [ 1.5469,  2.1406,  2.3906],
         [ 3.9531,  2.4844,  2.0625]]], device='cuda:0', dtype=torch.bfloat16,
       grad_fn=<AsStridedBackward0>)
after ssm_state from cache: tensor([[[[-1.0639e+26,  3.8574e-02, -1.3830e-21,  ...,  3.7842e-03,
           -5.9375e-01, -1.4893e-02]],

         [[ 2.3574e+25,  2.1515e-03, -1.4784e+04,  ...,  7.2861e-04,
           -1.7044e-32, -5.4626e-03]],

         [[-9.7422e+19,  5.0659e-03,  1.5622e+16,  ...,  3.5400e-03,
            2.5134e+20,  4.4250e-04]],

         ...,

         [[ 9.9712e-28, -2.1484e-02, -3.8926e-10,  ..., -5.5847e-03,
            2.6081e-38, -1.0315e-02]],

         [[ 2.2763e+11,  2.7710e-02,  1.8645e-11,  ...,  6.7749e-03,
            2.0872e-14, -6.7444e-03]],

         [[-1.1220e+08,  1.9409e-02,  5.1840e+37,  ...,  1.8158e-03,
           -9.9600e+02, -5.7983e-03]]]], device='cuda:0', dtype=torch.bfloat16)
here here
no back here:
hit for state: 9
before conv_state: tensor([[[-2.2500, -1.4609, -1.3203],
         [ 2.4375,  0.3613, -3.1250],
         [ 1.7188,  0.2500,  2.0625],
         ...,
         [-2.8594, -2.9062, -2.0312],
         [ 3.9844,  3.7812,  3.0312],
         [ 0.7422, -0.7539, -0.8359]]], device='cuda:0', dtype=torch.bfloat16,
       grad_fn=<AsStridedBackward0>)
before ssm_state: tensor([[[ 4.1016e-01, -4.8340e-02,  2.1191e-01,  ...,  1.0300e-03,
           1.1732e-10,  7.5073e-03],
         [ 5.2734e-01,  1.2451e-02,  2.2656e-01,  ...,  9.2697e-04,
           4.9591e-04,  3.1738e-03],
         [ 0.0000e+00, -4.2419e-03,  0.0000e+00,  ...,  1.2268e-02,
           0.0000e+00,  1.8066e-02],
         ...,
         [ 8.3009e-19, -1.8921e-02,  7.0310e-24,  ..., -3.7231e-03,
           0.0000e+00, -3.3569e-03],
         [ 5.1953e-01,  1.8311e-02,  1.5820e-01,  ...,  1.3809e-03,
           6.9618e-05,  7.9956e-03],
         [ 1.8359e-01,  2.5368e-04,  3.5156e-02,  ..., -1.7881e-05,
           2.9421e-15, -2.1210e-03]]], device='cuda:0', dtype=torch.bfloat16)
x.stride0: (1, 1, 1)
x.shape: torch.Size([1, 1, 3584])
x.stride1: (7168, 3584, 1)
x.stride2: (7168, 1, 3584)
conv_state1: (10752, 1, 3584)
conv_state2: (10752, 1, 3584)
conv_state3: (10752, 1, 3584)
conv_state: torch.Size([1, 3584, 3])
conv1d_out: torch.Size([1, 3584, 1])
rearrange(conv1d_out, 'b d l -> (b l) d') shape: torch.Size([1, 3584])
x_proj_weight shape: torch.Size([144, 3584])
x_dbl: torch.Size([1, 144])
delta_proj_weight @ x_dbl[:, :delta_rank].t(): torch.Size([3584, 1])
dsfxxxxxxdfs
after conv_state: tensor([[[-1.4609, -1.3203, -2.4219],
         [ 0.3613, -3.1250,  1.9766],
         [ 0.2500,  2.0625,  1.3047],
         ...,
         [-2.9062, -2.0312, -3.8906],
         [ 3.7812,  3.0312,  0.7773],
         [-0.7539, -0.8359,  0.0310]]], device='cuda:0', dtype=torch.bfloat16,
       grad_fn=<AsStridedBackward0>)
after ssm_state: tensor([[[ 3.4332e-05, -4.8340e-02,  3.0398e-05,  ...,  1.0300e-03,
          -6.1482e-10,  7.4768e-03],
         [-6.6425e+32,  1.2390e-02,  5.9069e-37,  ...,  9.2316e-04,
           5.3932e-22,  3.1738e-03],
         [-2.4545e+17, -4.2114e-03, -1.2534e+06,  ...,  1.2146e-02,
           3.9790e-12,  1.7944e-02],
         ...,
         [ 2.9582e-31, -1.8921e-02, -2.6585e+37,  ..., -3.7079e-03,
           4.5000e+02, -3.3264e-03],
         [-1.3351e-03,  1.8188e-02, -6.3702e-07,  ...,  1.3428e-03,
          -1.7021e-35,  7.1106e-03],
         [-3.3458e-20,  2.4605e-04, -5.6546e-27,  ..., -1.7047e-05,
          -4.7821e-24, -8.7357e-04]]], device='cuda:0', dtype=torch.bfloat16)
after conv from cache: tensor([[[-1.4609, -1.3203, -2.4219],
         [ 0.3613, -3.1250,  1.9766],
         [ 0.2500,  2.0625,  1.3047],
         ...,
         [-2.9062, -2.0312, -3.8906],
         [ 3.7812,  3.0312,  0.7773],
         [-0.7539, -0.8359,  0.0310]]], device='cuda:0', dtype=torch.bfloat16,
       grad_fn=<AsStridedBackward0>)
after ssm_state from cache: tensor([[[[ 3.4332e-05, -4.8340e-02,  3.0398e-05,  ...,  1.0300e-03,
           -6.1482e-10,  7.4768e-03]],

         [[-6.6425e+32,  1.2390e-02,  5.9069e-37,  ...,  9.2316e-04,
            5.3932e-22,  3.1738e-03]],

         [[-2.4545e+17, -4.2114e-03, -1.2534e+06,  ...,  1.2146e-02,
            3.9790e-12,  1.7944e-02]],

         ...,

         [[ 2.9582e-31, -1.8921e-02, -2.6585e+37,  ..., -3.7079e-03,
            4.5000e+02, -3.3264e-03]],

         [[-1.3351e-03,  1.8188e-02, -6.3702e-07,  ...,  1.3428e-03,
           -1.7021e-35,  7.1106e-03]],

         [[-3.3458e-20,  2.4605e-04, -5.6546e-27,  ..., -1.7047e-05,
           -4.7821e-24, -8.7357e-04]]]], device='cuda:0', dtype=torch.bfloat16)
here here
no back here:
hit for state: 10
before conv_state: tensor([[[-0.8242,  0.1338, -0.3535],
         [ 0.6992,  0.3848,  3.1406],
         [-1.9219, -2.6250, -1.2734],
         ...,
         [-1.2656, -0.2178,  0.1084],
         [ 0.8320, -0.9609, -0.4980],
         [ 0.9922,  0.0243,  1.5625]]], device='cuda:0', dtype=torch.bfloat16,
       grad_fn=<AsStridedBackward0>)
before ssm_state: tensor([[[ 8.5216e-08, -1.9287e-02,  6.1846e-10,  ..., -3.1128e-03,
           0.0000e+00,  1.5869e-02],
         [ 8.0404e-36,  1.4648e-02,  0.0000e+00,  ...,  7.3853e-03,
           0.0000e+00,  5.3406e-03],
         [ 2.0703e-01,  2.8839e-03,  4.8828e-02,  ..., -4.8218e-03,
           4.7294e-10,  1.2329e-02],
         ...,
         [ 2.4707e-01,  9.6436e-03,  8.8867e-02,  ...,  2.3365e-04,
           1.0803e-07,  6.3782e-03],
         [ 0.0000e+00, -3.7384e-03,  0.0000e+00,  ...,  1.0071e-03,
           0.0000e+00, -1.7456e-02],
         [ 3.2043e-03, -1.4282e-02,  6.7428e-07,  ..., -3.8300e-03,
           0.0000e+00,  5.6763e-03]]], device='cuda:0', dtype=torch.bfloat16)
x.stride0: (1, 1, 1)
x.shape: torch.Size([1, 1, 3584])
x.stride1: (7168, 3584, 1)
x.stride2: (7168, 1, 3584)
conv_state1: (10752, 1, 3584)
conv_state2: (10752, 1, 3584)
conv_state3: (10752, 1, 3584)
conv_state: torch.Size([1, 3584, 3])
conv1d_out: torch.Size([1, 3584, 1])
rearrange(conv1d_out, 'b d l -> (b l) d') shape: torch.Size([1, 3584])
x_proj_weight shape: torch.Size([144, 3584])
x_dbl: torch.Size([1, 144])
delta_proj_weight @ x_dbl[:, :delta_rank].t(): torch.Size([3584, 1])
dsfxxxxxxdfs
after conv_state: tensor([[[ 0.1338, -0.3535, -0.7773],
         [ 0.3848,  3.1406,  0.6680],
         [-2.6250, -1.2734, -2.1406],
         ...,
         [-0.2178,  0.1084, -1.9062],
         [-0.9609, -0.4980,  0.2100],
         [ 0.0243,  1.5625,  1.7344]]], device='cuda:0', dtype=torch.bfloat16,
       grad_fn=<AsStridedBackward0>)
after ssm_state: tensor([[[ 6.0148e+37, -1.8921e-02,  7.7112e-36,  ..., -2.8839e-03,
           1.3824e-10,  1.4221e-02],
         [-8.4799e+28,  1.3245e-02,  2.4884e-09,  ...,  6.5918e-03,
           1.6972e-33,  1.1047e-02],
         [ 5.7440e+29,  2.5940e-03,  9.2691e-30,  ..., -2.5330e-03,
          -7.0315e+29,  2.3804e-03],
         ...,
         [ 4.8600e+02,  9.4604e-03,  9.0320e+30,  ...,  2.2507e-04,
          -3.6931e-19,  7.5684e-03],
         [-1.0941e+26, -3.5706e-03, -4.3066e+16,  ...,  6.9427e-04,
          -1.1216e+22, -9.1553e-03],
         [ 1.1098e+24, -1.0437e-02,  4.9478e+13,  ..., -1.9836e-03,
           1.1267e-32, -3.4668e-02]]], device='cuda:0', dtype=torch.bfloat16)
after conv from cache: tensor([[[ 0.1338, -0.3535, -0.7773],
         [ 0.3848,  3.1406,  0.6680],
         [-2.6250, -1.2734, -2.1406],
         ...,
         [-0.2178,  0.1084, -1.9062],
         [-0.9609, -0.4980,  0.2100],
         [ 0.0243,  1.5625,  1.7344]]], device='cuda:0', dtype=torch.bfloat16,
       grad_fn=<AsStridedBackward0>)
after ssm_state from cache: tensor([[[[ 6.0148e+37, -1.8921e-02,  7.7112e-36,  ..., -2.8839e-03,
            1.3824e-10,  1.4221e-02]],

         [[-8.4799e+28,  1.3245e-02,  2.4884e-09,  ...,  6.5918e-03,
            1.6972e-33,  1.1047e-02]],

         [[ 5.7440e+29,  2.5940e-03,  9.2691e-30,  ..., -2.5330e-03,
           -7.0315e+29,  2.3804e-03]],

         ...,

         [[ 4.8600e+02,  9.4604e-03,  9.0320e+30,  ...,  2.2507e-04,
           -3.6931e-19,  7.5684e-03]],

         [[-1.0941e+26, -3.5706e-03, -4.3066e+16,  ...,  6.9427e-04,
           -1.1216e+22, -9.1553e-03]],

         [[ 1.1098e+24, -1.0437e-02,  4.9478e+13,  ..., -1.9836e-03,
            1.1267e-32, -3.4668e-02]]]], device='cuda:0', dtype=torch.bfloat16)
here here
no back here:
hit for state: 11
before conv_state: tensor([[[-3.3438, -5.2500, -3.7812],
         [ 1.5391,  1.5312,  1.0000],
         [-0.4707, -2.4844,  0.4531],
         ...,
         [-2.5156, -2.0000, -1.9062],
         [ 0.0126,  0.1982,  0.1279],
         [ 1.3438,  2.7812,  3.8281]]], device='cuda:0', dtype=torch.bfloat16,
       grad_fn=<AsStridedBackward0>)
before ssm_state: tensor([[[ 0.0000e+00, -7.9102e-02,  0.0000e+00,  ..., -1.3062e-02,
           0.0000e+00,  3.0518e-02],
         [ 3.6955e-06,  2.1973e-02,  4.1471e-18,  ..., -4.1992e-02,
           0.0000e+00, -4.4556e-03],
         [ 2.5586e-01,  1.0303e-01,  8.0078e-02,  ..., -4.0283e-02,
           5.8208e-08,  3.6621e-03],
         ...,
         [ 3.6523e-01, -3.3984e-01,  9.6680e-02,  ...,  3.4912e-02,
           7.4579e-11,  1.9043e-02],
         [ 6.1002e-08,  2.3535e-01,  3.4694e-16,  ...,  3.3203e-02,
           0.0000e+00,  1.9775e-02],
         [ 8.8501e-03,  2.5177e-03,  3.0734e-08,  ...,  1.0757e-03,
           6.1137e-30,  9.4604e-04]]], device='cuda:0', dtype=torch.bfloat16)
x.stride0: (1, 1, 1)
x.shape: torch.Size([1, 1, 3584])
x.stride1: (7168, 3584, 1)
x.stride2: (7168, 1, 3584)
conv_state1: (10752, 1, 3584)
conv_state2: (10752, 1, 3584)
conv_state3: (10752, 1, 3584)
conv_state: torch.Size([1, 3584, 3])
conv1d_out: torch.Size([1, 3584, 1])
rearrange(conv1d_out, 'b d l -> (b l) d') shape: torch.Size([1, 3584])
x_proj_weight shape: torch.Size([144, 3584])
x_dbl: torch.Size([1, 144])
delta_proj_weight @ x_dbl[:, :delta_rank].t(): torch.Size([3584, 1])
dsfxxxxxxdfs
after conv_state: tensor([[[-5.2500, -3.7812, -2.4844],
         [ 1.5312,  1.0000,  2.3281],
         [-2.4844,  0.4531, -1.4766],
         ...,
         [-2.0000, -1.9062, -1.0391],
         [ 0.1982,  0.1279,  0.6992],
         [ 2.7812,  3.8281,  0.7656]]], device='cuda:0', dtype=torch.bfloat16,
       grad_fn=<AsStridedBackward0>)
after ssm_state: tensor([[[-2.0163e-23, -6.8359e-02,  3.2985e+14,  ..., -6.9580e-03,
           5.2167e+07,  1.1902e-02],
         [-7.4962e-20,  1.3123e-02,  1.2059e+07,  ..., -1.3611e-02,
          -1.6058e-12, -6.5002e-03],
         [-1.4746e+36,  1.0156e-01, -1.4355e+17,  ..., -3.7109e-02,
          -8.3077e+35,  2.6855e-03],
         ...,
         [ 1.8455e+09, -3.1250e-01,  1.1141e-10,  ...,  2.9785e-02,
          -7.1942e+20,  1.5320e-02],
         [-2.3779e+18,  2.3438e-01, -2.0794e+27,  ...,  3.2715e-02,
          -2.6770e+28,  1.9287e-02],
         [-1.7025e-19,  2.4872e-03, -1.0361e-08,  ...,  1.0529e-03,
           2.1354e+28,  6.9427e-04]]], device='cuda:0', dtype=torch.bfloat16)
after conv from cache: tensor([[[-5.2500, -3.7812, -2.4844],
         [ 1.5312,  1.0000,  2.3281],
         [-2.4844,  0.4531, -1.4766],
         ...,
         [-2.0000, -1.9062, -1.0391],
         [ 0.1982,  0.1279,  0.6992],
         [ 2.7812,  3.8281,  0.7656]]], device='cuda:0', dtype=torch.bfloat16,
       grad_fn=<AsStridedBackward0>)
after ssm_state from cache: tensor([[[[-2.0163e-23, -6.8359e-02,  3.2985e+14,  ..., -6.9580e-03,
            5.2167e+07,  1.1902e-02]],

         [[-7.4962e-20,  1.3123e-02,  1.2059e+07,  ..., -1.3611e-02,
           -1.6058e-12, -6.5002e-03]],

         [[-1.4746e+36,  1.0156e-01, -1.4355e+17,  ..., -3.7109e-02,
           -8.3077e+35,  2.6855e-03]],

         ...,

         [[ 1.8455e+09, -3.1250e-01,  1.1141e-10,  ...,  2.9785e-02,
           -7.1942e+20,  1.5320e-02]],

         [[-2.3779e+18,  2.3438e-01, -2.0794e+27,  ...,  3.2715e-02,
           -2.6770e+28,  1.9287e-02]],

         [[-1.7025e-19,  2.4872e-03, -1.0361e-08,  ...,  1.0529e-03,
            2.1354e+28,  6.9427e-04]]]], device='cuda:0', dtype=torch.bfloat16)
here here
no back here:
hit for state: 12
before conv_state: tensor([[[ 1.3672,  0.6523, -0.1177],
         [-1.3438, -0.6328, -2.6562],
         [-2.2031,  0.1123, -1.6875],
         ...,
         [ 0.1641,  0.5742, -0.3730],
         [-2.0156, -0.3594, -1.5781],
         [ 0.7422, -0.4375, -1.3828]]], device='cuda:0', dtype=torch.bfloat16,
       grad_fn=<AsStridedBackward0>)
before ssm_state: tensor([[[ 7.2266e-01, -9.4604e-03,  6.0156e-01,  ..., -4.8065e-04,
           1.5991e-02,  2.1851e-02],
         [ 1.9531e-01,  2.6001e-02,  6.6528e-03,  ..., -2.1973e-03,
           9.2371e-14, -1.5564e-02],
         [ 5.9674e-15, -2.5024e-02,  2.7263e-26,  ...,  2.6398e-03,
           0.0000e+00,  1.6113e-02],
         ...,
         [ 1.9409e-02,  3.5156e-02,  6.1989e-06,  ..., -1.8677e-02,
           6.7526e-28,  6.2256e-02],
         [ 5.9686e-12, -2.1118e-02,  1.4188e-20,  ...,  6.3171e-03,
           0.0000e+00, -1.9775e-02],
         [ 1.1864e-27, -9.0027e-04,  0.0000e+00,  ..., -1.2817e-03,
           0.0000e+00,  1.5503e-02]]], device='cuda:0', dtype=torch.bfloat16)
x.stride0: (1, 1, 1)
x.shape: torch.Size([1, 1, 3584])
x.stride1: (7168, 3584, 1)
x.stride2: (7168, 1, 3584)
conv_state1: (10752, 1, 3584)
conv_state2: (10752, 1, 3584)
conv_state3: (10752, 1, 3584)
conv_state: torch.Size([1, 3584, 3])
conv1d_out: torch.Size([1, 3584, 1])
rearrange(conv1d_out, 'b d l -> (b l) d') shape: torch.Size([1, 3584])
x_proj_weight shape: torch.Size([144, 3584])
x_dbl: torch.Size([1, 144])
delta_proj_weight @ x_dbl[:, :delta_rank].t(): torch.Size([3584, 1])
dsfxxxxxxdfs
after conv_state: tensor([[[ 0.6523, -0.1177,  2.5000],
         [-0.6328, -2.6562, -1.5156],
         [ 0.1123, -1.6875, -2.8594],
         ...,
         [ 0.5742, -0.3730,  0.1514],
         [-0.3594, -1.5781, -1.6797],
         [-0.4375, -1.3828, -0.5234]]], device='cuda:0', dtype=torch.bfloat16,
       grad_fn=<AsStridedBackward0>)
after ssm_state: tensor([[[ 3.2924e-10, -9.4604e-03, -2.2128e+28,  ..., -4.7874e-04,
          -6.0398e+08,  2.1729e-02],
         [-4.3671e-27,  2.5879e-02, -9.0332e-02,  ..., -2.1820e-03,
          -1.9228e-29, -1.5442e-02],
         [-3.1738e-03, -2.4902e-02,  4.0323e-13,  ...,  2.5787e-03,
           1.6927e-36,  1.5503e-02],
         ...,
         [ 1.9070e+12,  2.4902e-02,  7.3499e+18,  ..., -7.9346e-03,
           2.1076e-04, -4.9744e-03],
         [ 6.1035e-03, -1.9287e-02, -6.1951e-03,  ...,  3.5248e-03,
           1.4704e-18, -1.6235e-02],
         [-3.1038e+09, -4.0627e-04,  1.2124e+07,  ..., -4.0627e-04,
          -8.9439e-24, -2.2217e-02]]], device='cuda:0', dtype=torch.bfloat16)
after conv from cache: tensor([[[ 0.6523, -0.1177,  2.5000],
         [-0.6328, -2.6562, -1.5156],
         [ 0.1123, -1.6875, -2.8594],
         ...,
         [ 0.5742, -0.3730,  0.1514],
         [-0.3594, -1.5781, -1.6797],
         [-0.4375, -1.3828, -0.5234]]], device='cuda:0', dtype=torch.bfloat16,
       grad_fn=<AsStridedBackward0>)
after ssm_state from cache: tensor([[[[ 3.2924e-10, -9.4604e-03, -2.2128e+28,  ..., -4.7874e-04,
           -6.0398e+08,  2.1729e-02]],

         [[-4.3671e-27,  2.5879e-02, -9.0332e-02,  ..., -2.1820e-03,
           -1.9228e-29, -1.5442e-02]],

         [[-3.1738e-03, -2.4902e-02,  4.0323e-13,  ...,  2.5787e-03,
            1.6927e-36,  1.5503e-02]],

         ...,

         [[ 1.9070e+12,  2.4902e-02,  7.3499e+18,  ..., -7.9346e-03,
            2.1076e-04, -4.9744e-03]],

         [[ 6.1035e-03, -1.9287e-02, -6.1951e-03,  ...,  3.5248e-03,
            1.4704e-18, -1.6235e-02]],

         [[-3.1038e+09, -4.0627e-04,  1.2124e+07,  ..., -4.0627e-04,
           -8.9439e-24, -2.2217e-02]]]], device='cuda:0', dtype=torch.bfloat16)
here here
no back here:
hit for state: 13
before conv_state: tensor([[[ 0.7109,  0.0347, -1.8203],
         [ 0.7891, -1.3672,  0.3379],
         [ 3.5312,  4.5312,  2.5781],
         ...,
         [ 0.3164,  3.2031,  2.0938],
         [ 0.9766,  3.0781,  2.0938],
         [-0.9883, -1.2969, -0.4961]]], device='cuda:0', dtype=torch.bfloat16,
       grad_fn=<AsStridedBackward0>)
before ssm_state: tensor([[[ 2.0508e-01,  7.0801e-03,  2.8320e-02,  ..., -3.2501e-03,
           1.7735e-10, -6.2180e-04],
         [ 3.6925e-10, -6.5231e-04,  6.2797e-16,  ...,  4.2236e-02,
           0.0000e+00,  2.5391e-02],
         [ 9.3937e-05,  5.7373e-02,  9.5497e-11,  ..., -6.6528e-03,
           0.0000e+00, -5.8289e-03],
         ...,
         [ 3.6430e-04, -2.6703e-04,  2.2054e-06,  ..., -6.3782e-03,
           0.0000e+00, -8.8501e-03],
         [ 4.9225e-27, -8.9722e-03,  0.0000e+00,  ..., -1.1475e-02,
           0.0000e+00,  2.5879e-02],
         [ 4.6839e-11,  8.2031e-02,  1.1858e-18,  ..., -7.5073e-03,
           0.0000e+00,  8.0566e-03]]], device='cuda:0', dtype=torch.bfloat16)
x.stride0: (1, 1, 1)
x.shape: torch.Size([1, 1, 3584])
x.stride1: (7168, 3584, 1)
x.stride2: (7168, 1, 3584)
conv_state1: (10752, 1, 3584)
conv_state2: (10752, 1, 3584)
conv_state3: (10752, 1, 3584)
conv_state: torch.Size([1, 3584, 3])
conv1d_out: torch.Size([1, 3584, 1])
rearrange(conv1d_out, 'b d l -> (b l) d') shape: torch.Size([1, 3584])
x_proj_weight shape: torch.Size([144, 3584])
x_dbl: torch.Size([1, 144])
delta_proj_weight @ x_dbl[:, :delta_rank].t(): torch.Size([3584, 1])
dsfxxxxxxdfs
after conv_state: tensor([[[ 0.0347, -1.8203,  1.1406],
         [-1.3672,  0.3379,  1.8672],
         [ 4.5312,  2.5781,  1.6328],
         ...,
         [ 3.2031,  2.0938,  1.2188],
         [ 3.0781,  2.0938, -0.6055],
         [-1.2969, -0.4961, -3.5000]]], device='cuda:0', dtype=torch.bfloat16,
       grad_fn=<AsStridedBackward0>)
after ssm_state: tensor([[[ 1.8829e-13,  7.0801e-03, -8.4400e+02,  ..., -3.2349e-03,
           3.5735e-36, -6.2943e-04],
         [-1.0107e-01, -6.4850e-04, -3.2285e+30,  ...,  4.1992e-02,
          -2.2283e-10,  2.5146e-02],
         [-4.6736e-23,  5.6152e-02, -3.5575e-20,  ..., -5.9509e-03,
          -3.1273e+19, -4.2419e-03],
         ...,
         [-2.5477e-22, -1.9550e-04, -1.3592e-37,  ..., -3.6163e-03,
          -3.7250e+01, -5.3406e-03],
         [-6.6223e-03, -8.8501e-03, -4.3600e+02,  ..., -1.0925e-02,
           3.2142e-38,  2.4658e-02],
         [-9.3387e+19,  7.6660e-02, -6.9470e-26,  ..., -7.0190e-03,
          -1.1746e-22,  7.0496e-03]]], device='cuda:0', dtype=torch.bfloat16)
after conv from cache: tensor([[[ 0.0347, -1.8203,  1.1406],
         [-1.3672,  0.3379,  1.8672],
         [ 4.5312,  2.5781,  1.6328],
         ...,
         [ 3.2031,  2.0938,  1.2188],
         [ 3.0781,  2.0938, -0.6055],
         [-1.2969, -0.4961, -3.5000]]], device='cuda:0', dtype=torch.bfloat16,
       grad_fn=<AsStridedBackward0>)
after ssm_state from cache: tensor([[[[ 1.8829e-13,  7.0801e-03, -8.4400e+02,  ..., -3.2349e-03,
            3.5735e-36, -6.2943e-04]],

         [[-1.0107e-01, -6.4850e-04, -3.2285e+30,  ...,  4.1992e-02,
           -2.2283e-10,  2.5146e-02]],

         [[-4.6736e-23,  5.6152e-02, -3.5575e-20,  ..., -5.9509e-03,
           -3.1273e+19, -4.2419e-03]],

         ...,

         [[-2.5477e-22, -1.9550e-04, -1.3592e-37,  ..., -3.6163e-03,
           -3.7250e+01, -5.3406e-03]],

         [[-6.6223e-03, -8.8501e-03, -4.3600e+02,  ..., -1.0925e-02,
            3.2142e-38,  2.4658e-02]],

         [[-9.3387e+19,  7.6660e-02, -6.9470e-26,  ..., -7.0190e-03,
           -1.1746e-22,  7.0496e-03]]]], device='cuda:0', dtype=torch.bfloat16)
here here
no back here:
hit for state: 14
before conv_state: tensor([[[-0.2559, -2.5000, -2.1406],
         [ 0.8047,  0.3633,  1.5703],
         [-1.3594,  0.4941, -0.0270],
         ...,
         [-1.1562, -2.8750, -1.9062],
         [-3.2188, -0.4395, -0.9141],
         [-3.6406, -4.5625, -3.2500]]], device='cuda:0', dtype=torch.bfloat16,
       grad_fn=<AsStridedBackward0>)
before ssm_state: tensor([[[ 1.2765e-38,  1.1328e-01,  0.0000e+00,  ...,  2.8442e-02,
           0.0000e+00,  3.2227e-02],
         [ 1.2747e-08, -3.6133e-02,  1.6805e-17,  ..., -2.2125e-03,
           0.0000e+00,  5.9509e-04],
         [ 1.1520e-03, -4.0588e-03,  3.8999e-09,  ...,  5.4550e-04,
           0.0000e+00,  9.5367e-04],
         ...,
         [ 1.8359e-01,  1.3611e-02,  2.2070e-01,  ...,  3.4332e-04,
           1.4494e-08,  9.7752e-05],
         [ 2.5543e-21,  2.0898e-01,  0.0000e+00,  ...,  8.9722e-03,
           0.0000e+00,  9.5215e-03],
         [ 6.9922e-01,  7.4768e-03,  5.9375e-01,  ..., -1.8539e-03,
           1.8799e-02,  2.9182e-04]]], device='cuda:0', dtype=torch.bfloat16)
x.stride0: (1, 1, 1)
x.shape: torch.Size([1, 1, 3584])
x.stride1: (7168, 3584, 1)
x.stride2: (7168, 1, 3584)
conv_state1: (10752, 1, 3584)
conv_state2: (10752, 1, 3584)
conv_state3: (10752, 1, 3584)
conv_state: torch.Size([1, 3584, 3])
conv1d_out: torch.Size([1, 3584, 1])
rearrange(conv1d_out, 'b d l -> (b l) d') shape: torch.Size([1, 3584])
x_proj_weight shape: torch.Size([144, 3584])
x_dbl: torch.Size([1, 144])
delta_proj_weight @ x_dbl[:, :delta_rank].t(): torch.Size([3584, 1])
dsfxxxxxxdfs
after conv_state: tensor([[[-2.5000, -2.1406,  0.1807],
         [ 0.3633,  1.5703,  0.6367],
         [ 0.4941, -0.0270, -0.0879],
         ...,
         [-2.8750, -1.9062, -0.9219],
         [-0.4395, -0.9141,  0.2695],
         [-4.5625, -3.2500, -0.1162]]], device='cuda:0', dtype=torch.bfloat16,
       grad_fn=<AsStridedBackward0>)
after ssm_state: tensor([[[ 1.1488e+35,  8.8867e-02,  5.5955e-14,  ...,  5.0049e-03,
          -3.8221e+28, -1.5991e-02],
         [-1.9013e-15, -9.7656e-03, -3.7966e-25,  ..., -1.5163e-04,
           8.8083e+20,  2.2461e-02],
         [-2.5042e-26, -3.9978e-03, -8.0469e+24,  ...,  4.8637e-04,
          -5.2480e+03,  9.8419e-04],
         ...,
         [ 4.0800e+02,  1.2695e-02,  7.8643e+05,  ...,  3.0327e-04,
           1.4250e+01,  2.2888e-03],
         [-6.6709e-37,  2.0312e-01,  4.0183e-30,  ...,  7.6599e-03,
          -1.6022e-03,  5.6763e-03],
         [-6.4640e+03,  6.1951e-03,  2.0556e-16,  ..., -1.2894e-03,
           3.7000e+01,  2.7161e-03]]], device='cuda:0', dtype=torch.bfloat16)
after conv from cache: tensor([[[-2.5000, -2.1406,  0.1807],
         [ 0.3633,  1.5703,  0.6367],
         [ 0.4941, -0.0270, -0.0879],
         ...,
         [-2.8750, -1.9062, -0.9219],
         [-0.4395, -0.9141,  0.2695],
         [-4.5625, -3.2500, -0.1162]]], device='cuda:0', dtype=torch.bfloat16,
       grad_fn=<AsStridedBackward0>)
after ssm_state from cache: tensor([[[[ 1.1488e+35,  8.8867e-02,  5.5955e-14,  ...,  5.0049e-03,
           -3.8221e+28, -1.5991e-02]],

         [[-1.9013e-15, -9.7656e-03, -3.7966e-25,  ..., -1.5163e-04,
            8.8083e+20,  2.2461e-02]],

         [[-2.5042e-26, -3.9978e-03, -8.0469e+24,  ...,  4.8637e-04,
           -5.2480e+03,  9.8419e-04]],

         ...,

         [[ 4.0800e+02,  1.2695e-02,  7.8643e+05,  ...,  3.0327e-04,
            1.4250e+01,  2.2888e-03]],

         [[-6.6709e-37,  2.0312e-01,  4.0183e-30,  ...,  7.6599e-03,
           -1.6022e-03,  5.6763e-03]],

         [[-6.4640e+03,  6.1951e-03,  2.0556e-16,  ..., -1.2894e-03,
            3.7000e+01,  2.7161e-03]]]], device='cuda:0', dtype=torch.bfloat16)
here here
no back here:
hit for state: 15
before conv_state: tensor([[[ 2.2344, -3.0469, -1.4062],
         [ 1.9688,  1.6953,  2.2969],
         [-0.2969,  1.3516,  0.8242],
         ...,
         [ 1.1953, -0.2539,  1.2891],
         [-3.1875, -1.4844, -3.2031],
         [-2.5781, -2.7188, -0.8398]]], device='cuda:0', dtype=torch.bfloat16,
       grad_fn=<AsStridedBackward0>)
before ssm_state: tensor([[[ 1.2054e-03,  1.5137e-01,  5.1880e-04,  ...,  1.4771e-02,
           0.0000e+00, -1.0193e-02],
         [ 4.0039e-01,  8.4961e-02,  8.6914e-02,  ..., -1.2146e-02,
           2.8126e-07, -6.2561e-03],
         [ 2.1191e-01,  9.3262e-02,  3.1982e-02,  ..., -1.0437e-02,
           8.3134e-13, -2.2888e-03],
         ...,
         [ 1.2351e-15, -4.4189e-02,  2.8620e-22,  ..., -1.1292e-02,
           0.0000e+00, -1.4496e-03],
         [ 3.3617e-21,  8.2031e-02,  0.0000e+00,  ..., -4.6387e-02,
           0.0000e+00,  2.8931e-02],
         [ 8.3267e-17,  3.8818e-02,  1.0768e-35,  ..., -3.0884e-02,
           0.0000e+00,  1.5625e-02]]], device='cuda:0', dtype=torch.bfloat16)
x.stride0: (1, 1, 1)
x.shape: torch.Size([1, 1, 3584])
x.stride1: (7168, 3584, 1)
x.stride2: (7168, 1, 3584)
conv_state1: (10752, 1, 3584)
conv_state2: (10752, 1, 3584)
conv_state3: (10752, 1, 3584)
conv_state: torch.Size([1, 3584, 3])
conv1d_out: torch.Size([1, 3584, 1])
rearrange(conv1d_out, 'b d l -> (b l) d') shape: torch.Size([1, 3584])
x_proj_weight shape: torch.Size([144, 3584])
x_dbl: torch.Size([1, 144])
delta_proj_weight @ x_dbl[:, :delta_rank].t(): torch.Size([3584, 1])
dsfxxxxxxdfs
after conv_state: tensor([[[-3.0469, -1.4062,  0.0364],
         [ 1.6953,  2.2969, -0.3828],
         [ 1.3516,  0.8242, -4.5312],
         ...,
         [-0.2539,  1.2891,  3.8438],
         [-1.4844, -3.2031, -1.0938],
         [-2.7188, -0.8398, -3.8750]]], device='cuda:0', dtype=torch.bfloat16,
       grad_fn=<AsStridedBackward0>)
after ssm_state: tensor([[[ 1.3769e-05,  1.5039e-01,  1.6971e+22,  ...,  1.3733e-02,
           9.6430e-27, -9.2773e-03],
         [-1.4051e+18,  8.1543e-02, -4.0796e+06,  ..., -9.6436e-03,
          -2.7689e+06, -5.1270e-03],
         [ 1.7397e-36,  9.3262e-02,  2.5580e+18,  ..., -1.0315e-02,
          -7.1944e-28, -2.1515e-03],
         ...,
         [ 1.1250e+01, -4.3213e-02,  2.7740e-11,  ..., -1.0620e-02,
           1.5874e-34, -1.3657e-03],
         [ 3.4459e-08,  7.5684e-02,  6.2466e-05,  ..., -3.0640e-02,
          -6.1655e+25,  2.2583e-02],
         [ 9.7857e+13,  2.3804e-02, -2.3426e-26,  ..., -1.1353e-02,
          -2.6867e-14,  3.0670e-03]]], device='cuda:0', dtype=torch.bfloat16)
after conv from cache: tensor([[[-3.0469, -1.4062,  0.0364],
         [ 1.6953,  2.2969, -0.3828],
         [ 1.3516,  0.8242, -4.5312],
         ...,
         [-0.2539,  1.2891,  3.8438],
         [-1.4844, -3.2031, -1.0938],
         [-2.7188, -0.8398, -3.8750]]], device='cuda:0', dtype=torch.bfloat16,
       grad_fn=<AsStridedBackward0>)
after ssm_state from cache: tensor([[[[ 1.3769e-05,  1.5039e-01,  1.6971e+22,  ...,  1.3733e-02,
            9.6430e-27, -9.2773e-03]],

         [[-1.4051e+18,  8.1543e-02, -4.0796e+06,  ..., -9.6436e-03,
           -2.7689e+06, -5.1270e-03]],

         [[ 1.7397e-36,  9.3262e-02,  2.5580e+18,  ..., -1.0315e-02,
           -7.1944e-28, -2.1515e-03]],

         ...,

         [[ 1.1250e+01, -4.3213e-02,  2.7740e-11,  ..., -1.0620e-02,
            1.5874e-34, -1.3657e-03]],

         [[ 3.4459e-08,  7.5684e-02,  6.2466e-05,  ..., -3.0640e-02,
           -6.1655e+25,  2.2583e-02]],

         [[ 9.7857e+13,  2.3804e-02, -2.3426e-26,  ..., -1.1353e-02,
           -2.6867e-14,  3.0670e-03]]]], device='cuda:0', dtype=torch.bfloat16)
here here
no back here:
hit for state: 16
before conv_state: tensor([[[ 1.1719,  3.2656,  3.4844],
         [ 0.3555, -1.2891, -0.2295],
         [ 1.3594, -0.5547, -0.5469],
         ...,
         [ 2.0312, -1.2969,  0.4492],
         [ 2.9219,  2.1719,  2.2031],
         [-1.5547,  1.5156, -0.2598]]], device='cuda:0', dtype=torch.bfloat16,
       grad_fn=<AsStridedBackward0>)
before ssm_state: tensor([[[ 1.5497e-05,  6.2988e-02,  8.9813e-12,  ...,  1.8555e-02,
           0.0000e+00,  5.5908e-02],
         [ 4.1723e-05,  8.5449e-02,  1.5154e-21,  ...,  1.1536e-02,
           0.0000e+00,  1.5442e-02],
         [ 2.0117e-01,  3.5645e-02,  4.1748e-02,  ..., -4.6082e-03,
           1.6189e-10,  5.2643e-04],
         ...,
         [ 2.3828e-01,  7.3242e-02,  1.8921e-02,  ...,  5.9204e-03,
           2.1714e-11,  1.2146e-02],
         [ 1.2684e-04,  2.0386e-02,  3.8445e-06,  ...,  1.3000e-02,
           0.0000e+00,  4.2480e-02],
         [ 4.2419e-03, -5.8838e-02,  1.1325e-05,  ..., -1.3428e-02,
           9.0519e-32, -6.4453e-02]]], device='cuda:0', dtype=torch.bfloat16)
x.stride0: (1, 1, 1)
x.shape: torch.Size([1, 1, 3584])
x.stride1: (7168, 3584, 1)
x.stride2: (7168, 1, 3584)
conv_state1: (10752, 1, 3584)
conv_state2: (10752, 1, 3584)
conv_state3: (10752, 1, 3584)
conv_state: torch.Size([1, 3584, 3])
conv1d_out: torch.Size([1, 3584, 1])
rearrange(conv1d_out, 'b d l -> (b l) d') shape: torch.Size([1, 3584])
x_proj_weight shape: torch.Size([144, 3584])
x_dbl: torch.Size([1, 144])
delta_proj_weight @ x_dbl[:, :delta_rank].t(): torch.Size([3584, 1])
dsfxxxxxxdfs
after conv_state: tensor([[[ 3.2656,  3.4844, -3.3750],
         [-1.2891, -0.2295,  0.4805],
         [-0.5547, -0.5469,  0.8945],
         ...,
         [-1.2969,  0.4492, -3.9062],
         [ 2.1719,  2.2031,  0.4590],
         [ 1.5156, -0.2598,  1.9453]]], device='cuda:0', dtype=torch.bfloat16,
       grad_fn=<AsStridedBackward0>)
after ssm_state: tensor([[[-3.3094e+25,  6.2012e-02,  6.1409e-09,  ...,  1.7700e-02,
           2.1817e-34,  4.1260e-02],
         [ 1.1415e-22,  7.9590e-02,  2.7022e+16,  ...,  8.5449e-03,
          -2.2959e-38, -4.0771e-02],
         [ 2.0266e-05,  3.4180e-02,  1.0334e-19,  ..., -3.2654e-03,
           7.8160e-14,  1.0452e-03],
         ...,
         [ 7.9101e+33,  6.1035e-02, -1.9163e-32,  ...,  3.9978e-03,
           4.6530e-31,  2.2339e-02],
         [ 4.1693e+15,  2.0142e-02,  7.7200e+02,  ...,  1.1230e-02,
           3.2933e-32,  3.9307e-02],
         [ 1.3632e+04, -4.6631e-02, -6.3438e+00,  ..., -6.9580e-03,
           3.5243e-11, -2.2217e-02]]], device='cuda:0', dtype=torch.bfloat16)
after conv from cache: tensor([[[ 3.2656,  3.4844, -3.3750],
         [-1.2891, -0.2295,  0.4805],
         [-0.5547, -0.5469,  0.8945],
         ...,
         [-1.2969,  0.4492, -3.9062],
         [ 2.1719,  2.2031,  0.4590],
         [ 1.5156, -0.2598,  1.9453]]], device='cuda:0', dtype=torch.bfloat16,
       grad_fn=<AsStridedBackward0>)
after ssm_state from cache: tensor([[[[-3.3094e+25,  6.2012e-02,  6.1409e-09,  ...,  1.7700e-02,
            2.1817e-34,  4.1260e-02]],

         [[ 1.1415e-22,  7.9590e-02,  2.7022e+16,  ...,  8.5449e-03,
           -2.2959e-38, -4.0771e-02]],

         [[ 2.0266e-05,  3.4180e-02,  1.0334e-19,  ..., -3.2654e-03,
            7.8160e-14,  1.0452e-03]],

         ...,

         [[ 7.9101e+33,  6.1035e-02, -1.9163e-32,  ...,  3.9978e-03,
            4.6530e-31,  2.2339e-02]],

         [[ 4.1693e+15,  2.0142e-02,  7.7200e+02,  ...,  1.1230e-02,
            3.2933e-32,  3.9307e-02]],

         [[ 1.3632e+04, -4.6631e-02, -6.3438e+00,  ..., -6.9580e-03,
            3.5243e-11, -2.2217e-02]]]], device='cuda:0', dtype=torch.bfloat16)
here here
no back here:
hit for state: 17
before conv_state: tensor([[[ 0.3281,  1.6641,  1.7969],
         [ 1.3984, -0.8516, -0.9844],
         [-0.2490, -0.4219, -0.5742],
         ...,
         [-1.2344,  1.0625,  1.5547],
         [ 1.1719, -1.9219, -3.1719],
         [-4.0312, -5.3750, -3.7188]]], device='cuda:0', dtype=torch.bfloat16,
       grad_fn=<AsStridedBackward0>)
before ssm_state: tensor([[[ 6.9470e-26, -9.8633e-02,  3.8654e-28,  ...,  7.4463e-03,
           0.0000e+00,  1.4114e-03],
         [ 7.8613e-02,  1.1719e-01,  1.0071e-02,  ...,  2.4780e-02,
           6.8160e-22, -1.9379e-03],
         [ 1.2988e-01,  1.1963e-02,  6.7383e-02,  ...,  7.7209e-03,
           2.2192e-10, -5.7602e-04],
         ...,
         [ 4.7266e-01,  1.3123e-02,  2.0215e-01,  ...,  1.9379e-03,
           8.8289e-07,  6.8054e-03],
         [ 4.2677e-05, -1.8652e-01,  2.8308e-11,  ...,  1.5106e-03,
           0.0000e+00, -4.2114e-03],
         [ 3.8672e-01,  1.4526e-02,  2.3145e-01,  ...,  1.2329e-02,
           3.3677e-06,  2.0752e-03]]], device='cuda:0', dtype=torch.bfloat16)
x.stride0: (1, 1, 1)
x.shape: torch.Size([1, 1, 3584])
x.stride1: (7168, 3584, 1)
x.stride2: (7168, 1, 3584)
conv_state1: (10752, 1, 3584)
conv_state2: (10752, 1, 3584)
conv_state3: (10752, 1, 3584)
conv_state: torch.Size([1, 3584, 3])
conv1d_out: torch.Size([1, 3584, 1])
rearrange(conv1d_out, 'b d l -> (b l) d') shape: torch.Size([1, 3584])
x_proj_weight shape: torch.Size([144, 3584])
x_dbl: torch.Size([1, 144])
delta_proj_weight @ x_dbl[:, :delta_rank].t(): torch.Size([3584, 1])
dsfxxxxxxdfs
after conv_state: tensor([[[ 1.6641,  1.7969,  0.8672],
         [-0.8516, -0.9844, -0.1982],
         [-0.4219, -0.5742, -1.7656],
         ...,
         [ 1.0625,  1.5547,  0.8555],
         [-1.9219, -3.1719,  0.1582],
         [-5.3750, -3.7188, -3.6250]]], device='cuda:0', dtype=torch.bfloat16,
       grad_fn=<AsStridedBackward0>)
after ssm_state: tensor([[[-2.0424e-37, -9.3750e-02,  3.9262e-36,  ...,  5.9204e-03,
           2.4028e-07,  1.8799e-02],
         [ 2.3175e-18,  7.8613e-02, -1.8104e-31,  ...,  1.6113e-02,
          -1.5016e+09, -3.7079e-03],
         [ 4.6529e-25,  1.1902e-02, -3.2909e+22,  ...,  7.4158e-03,
           3.7764e-26, -1.2024e-02],
         ...,
         [-4.4913e+35,  1.3000e-02, -4.9068e+21,  ...,  1.9226e-03,
           1.1948e+24,  6.7139e-03],
         [ 6.2593e-34, -1.8262e-01,  2.9057e-06,  ...,  1.2817e-03,
           1.7908e-38,  1.1063e-03],
         [-5.4481e-18,  1.2329e-02, -1.4123e+36,  ...,  9.3994e-03,
          -1.5465e+32,  9.3460e-04]]], device='cuda:0', dtype=torch.bfloat16)
after conv from cache: tensor([[[ 1.6641,  1.7969,  0.8672],
         [-0.8516, -0.9844, -0.1982],
         [-0.4219, -0.5742, -1.7656],
         ...,
         [ 1.0625,  1.5547,  0.8555],
         [-1.9219, -3.1719,  0.1582],
         [-5.3750, -3.7188, -3.6250]]], device='cuda:0', dtype=torch.bfloat16,
       grad_fn=<AsStridedBackward0>)
after ssm_state from cache: tensor([[[[-2.0424e-37, -9.3750e-02,  3.9262e-36,  ...,  5.9204e-03,
            2.4028e-07,  1.8799e-02]],

         [[ 2.3175e-18,  7.8613e-02, -1.8104e-31,  ...,  1.6113e-02,
           -1.5016e+09, -3.7079e-03]],

         [[ 4.6529e-25,  1.1902e-02, -3.2909e+22,  ...,  7.4158e-03,
            3.7764e-26, -1.2024e-02]],

         ...,

         [[-4.4913e+35,  1.3000e-02, -4.9068e+21,  ...,  1.9226e-03,
            1.1948e+24,  6.7139e-03]],

         [[ 6.2593e-34, -1.8262e-01,  2.9057e-06,  ...,  1.2817e-03,
            1.7908e-38,  1.1063e-03]],

         [[-5.4481e-18,  1.2329e-02, -1.4123e+36,  ...,  9.3994e-03,
           -1.5465e+32,  9.3460e-04]]]], device='cuda:0', dtype=torch.bfloat16)
here here
no back here:
hit for state: 18
before conv_state: tensor([[[-1.7344,  0.0850,  0.8008],
         [-0.8867, -1.2812, -1.4297],
         [ 2.0000,  1.5625,  1.7422],
         ...,
         [ 0.7578,  1.6953,  0.7578],
         [ 2.2031, -0.3789, -2.0156],
         [ 1.9375,  1.8438,  0.7305]]], device='cuda:0', dtype=torch.bfloat16,
       grad_fn=<AsStridedBackward0>)
before ssm_state: tensor([[[ 1.3551e-10, -4.7363e-02,  1.7205e-21,  ...,  3.6621e-02,
           0.0000e+00, -2.6978e-02],
         [ 4.0018e-10, -1.3770e-01,  3.1225e-17,  ..., -2.5635e-02,
           0.0000e+00, -5.7983e-04],
         [ 9.3675e-17,  8.9844e-02,  1.3131e-23,  ...,  8.6975e-04,
           0.0000e+00,  4.1199e-03],
         ...,
         [ 1.9455e-04,  1.5076e-02,  6.7221e-17,  ...,  1.1719e-02,
           0.0000e+00, -1.8188e-02],
         [ 1.4062e-01,  1.9141e-01,  3.6133e-02,  ...,  2.6733e-02,
           1.8106e-17,  4.2419e-03],
         [ 7.8217e-11,  3.5156e-01,  5.8420e-24,  ..., -5.6763e-03,
           0.0000e+00,  1.9165e-02]]], device='cuda:0', dtype=torch.bfloat16)
x.stride0: (1, 1, 1)
x.shape: torch.Size([1, 1, 3584])
x.stride1: (7168, 3584, 1)
x.stride2: (7168, 1, 3584)
conv_state1: (10752, 1, 3584)
conv_state2: (10752, 1, 3584)
conv_state3: (10752, 1, 3584)
conv_state: torch.Size([1, 3584, 3])
conv1d_out: torch.Size([1, 3584, 1])
rearrange(conv1d_out, 'b d l -> (b l) d') shape: torch.Size([1, 3584])
x_proj_weight shape: torch.Size([144, 3584])
x_dbl: torch.Size([1, 144])
delta_proj_weight @ x_dbl[:, :delta_rank].t(): torch.Size([3584, 1])
dsfxxxxxxdfs
after conv_state: tensor([[[ 0.0850,  0.8008, -1.2578],
         [-1.2812, -1.4297, -0.3574],
         [ 1.5625,  1.7422,  1.7812],
         ...,
         [ 1.6953,  0.7578,  0.0344],
         [-0.3789, -2.0156,  2.5938],
         [ 1.8438,  0.7305,  1.2031]]], device='cuda:0', dtype=torch.bfloat16,
       grad_fn=<AsStridedBackward0>)
after ssm_state: tensor([[[-1.5569e+22, -4.5898e-02, -1.0334e-28,  ...,  3.2471e-02,
           1.5227e+29, -2.6611e-02],
         [ 8.1527e-21, -1.2500e-01, -4.7601e+24,  ..., -1.8555e-02,
          -8.6784e-05, -4.2114e-03],
         [-2.1375e+15,  8.2520e-02,  1.8588e-37,  ...,  5.1498e-04,
          -1.1387e+06,  2.1362e-03],
         ...,
         [-2.3273e+30,  1.4404e-02,  2.0561e+36,  ...,  1.0193e-02,
           1.1596e+11, -1.7822e-02],
         [-9.2909e+13,  1.9043e-01,  6.2749e+31,  ...,  2.5879e-02,
           5.1808e+10,  4.9744e-03],
         [-3.0511e+13,  3.4375e-01, -2.7317e-20,  ..., -5.4016e-03,
           5.3683e+18,  1.9531e-02]]], device='cuda:0', dtype=torch.bfloat16)
after conv from cache: tensor([[[ 0.0850,  0.8008, -1.2578],
         [-1.2812, -1.4297, -0.3574],
         [ 1.5625,  1.7422,  1.7812],
         ...,
         [ 1.6953,  0.7578,  0.0344],
         [-0.3789, -2.0156,  2.5938],
         [ 1.8438,  0.7305,  1.2031]]], device='cuda:0', dtype=torch.bfloat16,
       grad_fn=<AsStridedBackward0>)
after ssm_state from cache: tensor([[[[-1.5569e+22, -4.5898e-02, -1.0334e-28,  ...,  3.2471e-02,
            1.5227e+29, -2.6611e-02]],

         [[ 8.1527e-21, -1.2500e-01, -4.7601e+24,  ..., -1.8555e-02,
           -8.6784e-05, -4.2114e-03]],

         [[-2.1375e+15,  8.2520e-02,  1.8588e-37,  ...,  5.1498e-04,
           -1.1387e+06,  2.1362e-03]],

         ...,

         [[-2.3273e+30,  1.4404e-02,  2.0561e+36,  ...,  1.0193e-02,
            1.1596e+11, -1.7822e-02]],

         [[-9.2909e+13,  1.9043e-01,  6.2749e+31,  ...,  2.5879e-02,
            5.1808e+10,  4.9744e-03]],

         [[-3.0511e+13,  3.4375e-01, -2.7317e-20,  ..., -5.4016e-03,
            5.3683e+18,  1.9531e-02]]]], device='cuda:0', dtype=torch.bfloat16)
here here
no back here:
hit for state: 19
before conv_state: tensor([[[-6.9427e-04, -2.0829e+16, -2.3335e+21],
         [ 1.9824e-01, -4.4531e-01,  3.7695e-01],
         [-1.7952e+09,  7.1157e+17, -1.8455e+08],
         ...,
         [-1.5859e+00, -2.4844e+00, -1.2109e+00],
         [ 3.8623e-27,  1.2820e+21, -7.8160e-14],
         [-4.2578e-01, -2.0938e+00, -8.7109e-01]]], device='cuda:0',
       dtype=torch.bfloat16, grad_fn=<AsStridedBackward0>)
before ssm_state: tensor([[[ 3.3447e-02, -1.6113e-01,  4.3392e-05,  ..., -1.0840e-01,
           1.3706e-29, -1.0254e-01],
         [ 9.9652e-08, -2.0874e-02,  5.8549e-12,  ..., -1.7166e-04,
           0.0000e+00,  5.8594e-02],
         [ 3.5667e-04, -3.0823e-03,  6.9122e-11,  ..., -2.7275e-04,
           0.0000e+00, -4.5654e-02],
         ...,
         [ 4.6387e-02, -2.3047e-01,  8.0109e-05,  ..., -6.2561e-03,
           1.3127e-30,  4.3213e-02],
         [ 1.4343e-02,  8.1543e-02,  3.1710e-05,  ...,  9.4238e-02,
           4.9593e-33,  1.2158e-01],
         [ 3.6508e-07, -6.8359e-02,  3.8369e-12,  ...,  1.2939e-02,
           0.0000e+00,  1.5381e-02]]], device='cuda:0', dtype=torch.bfloat16)
x.stride0: (1, 1, 1)
x.shape: torch.Size([1, 1, 3584])
x.stride1: (7168, 3584, 1)
x.stride2: (7168, 1, 3584)
conv_state1: (10752, 1, 3584)
conv_state2: (10752, 1, 3584)
conv_state3: (10752, 1, 3584)
conv_state: torch.Size([1, 3584, 3])
conv1d_out: torch.Size([1, 3584, 1])
rearrange(conv1d_out, 'b d l -> (b l) d') shape: torch.Size([1, 3584])
x_proj_weight shape: torch.Size([144, 3584])
x_dbl: torch.Size([1, 144])
delta_proj_weight @ x_dbl[:, :delta_rank].t(): torch.Size([3584, 1])
dsfxxxxxxdfs
after conv_state: tensor([[[-2.0829e+16, -2.3335e+21,  3.4062e+00],
         [-4.4531e-01,  3.7695e-01,  6.8750e-01],
         [ 7.1157e+17, -1.8455e+08, -1.8984e+00],
         ...,
         [-2.4844e+00, -1.2109e+00, -4.2812e+00],
         [ 1.2820e+21, -7.8160e-14, -1.8906e+00],
         [-2.0938e+00, -8.7109e-01, -3.1875e+00]]], device='cuda:0',
       dtype=torch.bfloat16, grad_fn=<AsStridedBackward0>)
after ssm_state: tensor([[[nan, nan, nan,  ..., nan, nan, nan],
         [nan, nan, nan,  ..., nan, nan, nan],
         [nan, nan, nan,  ..., nan, nan, nan],
         ...,
         [nan, nan, nan,  ..., nan, nan, nan],
         [nan, nan, nan,  ..., nan, nan, nan],
         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:0',
       dtype=torch.bfloat16)
after conv from cache: tensor([[[-2.0829e+16, -2.3335e+21,  3.4062e+00],
         [-4.4531e-01,  3.7695e-01,  6.8750e-01],
         [ 7.1157e+17, -1.8455e+08, -1.8984e+00],
         ...,
         [-2.4844e+00, -1.2109e+00, -4.2812e+00],
         [ 1.2820e+21, -7.8160e-14, -1.8906e+00],
         [-2.0938e+00, -8.7109e-01, -3.1875e+00]]], device='cuda:0',
       dtype=torch.bfloat16, grad_fn=<AsStridedBackward0>)
after ssm_state from cache: tensor([[[[nan, nan, nan,  ..., nan, nan, nan]],

         [[nan, nan, nan,  ..., nan, nan, nan]],

         [[nan, nan, nan,  ..., nan, nan, nan]],

         ...,

         [[nan, nan, nan,  ..., nan, nan, nan]],

         [[nan, nan, nan,  ..., nan, nan, nan]],

         [[nan, nan, nan,  ..., nan, nan, nan]]]], device='cuda:0',
       dtype=torch.bfloat16)
here here
no back here:
hit for state: 20
before conv_state: tensor([[[ 8.7880e-21, -6.0230e-15,  1.8440e-07],
         [-1.8828e+00, -1.8516e+00, -1.8457e-01],
         [-2.0000e+01, -1.7826e+07,  6.6039e+21],
         ...,
         [ 2.2812e+00,  2.9375e+00,  2.2852e-01],
         [-1.2372e+33, -9.1761e+16, -9.1016e+08],
         [-7.5684e-02,  9.1016e-01,  7.6904e-03]]], device='cuda:0',
       dtype=torch.bfloat16, grad_fn=<AsStridedBackward0>)
before ssm_state: tensor([[[ 9.3994e-03, -2.1777e-01,  2.4261e-03,  ..., -2.5635e-02,
           4.2756e-32,  4.9316e-02],
         [ 4.7607e-03,  3.1006e-02,  1.3962e-03,  ...,  4.8218e-03,
           1.9956e-23, -3.7354e-02],
         [ 5.6396e-02, -2.2754e-01,  7.6294e-04,  ...,  3.4180e-03,
           5.8161e-26,  3.1738e-02],
         ...,
         [ 1.0834e-03, -9.3750e-01,  2.2203e-06,  ..., -6.0059e-02,
           0.0000e+00,  3.7842e-02],
         [ 6.9824e-02, -3.2227e-01,  8.4229e-03,  ..., -8.7280e-03,
           7.2298e-26,  1.4526e-02],
         [ 6.3330e-08,  8.3496e-02,  3.4925e-09,  ..., -9.0332e-03,
           0.0000e+00, -2.8687e-02]]], device='cuda:0', dtype=torch.bfloat16)
x.stride0: (1, 1, 1)
x.shape: torch.Size([1, 1, 3584])
x.stride1: (7168, 3584, 1)
x.stride2: (7168, 1, 3584)
conv_state1: (10752, 1, 3584)
conv_state2: (10752, 1, 3584)
conv_state3: (10752, 1, 3584)
conv_state: torch.Size([1, 3584, 3])
conv1d_out: torch.Size([1, 3584, 1])
rearrange(conv1d_out, 'b d l -> (b l) d') shape: torch.Size([1, 3584])
x_proj_weight shape: torch.Size([144, 3584])
x_dbl: torch.Size([1, 144])
delta_proj_weight @ x_dbl[:, :delta_rank].t(): torch.Size([3584, 1])
dsfxxxxxxdfs
after conv_state: tensor([[[-6.0230e-15,  1.8440e-07,         nan],
         [-1.8516e+00, -1.8457e-01,         nan],
         [-1.7826e+07,  6.6039e+21,         nan],
         ...,
         [ 2.9375e+00,  2.2852e-01,         nan],
         [-9.1761e+16, -9.1016e+08,         nan],
         [ 9.1016e-01,  7.6904e-03,         nan]]], device='cuda:0',
       dtype=torch.bfloat16, grad_fn=<AsStridedBackward0>)
after ssm_state: tensor([[[nan, nan, nan,  ..., nan, nan, nan],
         [nan, nan, nan,  ..., nan, nan, nan],
         [nan, nan, nan,  ..., nan, nan, nan],
         ...,
         [nan, nan, nan,  ..., nan, nan, nan],
         [nan, nan, nan,  ..., nan, nan, nan],
         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:0',
       dtype=torch.bfloat16)
after conv from cache: tensor([[[-6.0230e-15,  1.8440e-07,         nan],
         [-1.8516e+00, -1.8457e-01,         nan],
         [-1.7826e+07,  6.6039e+21,         nan],
         ...,
         [ 2.9375e+00,  2.2852e-01,         nan],
         [-9.1761e+16, -9.1016e+08,         nan],
         [ 9.1016e-01,  7.6904e-03,         nan]]], device='cuda:0',
       dtype=torch.bfloat16, grad_fn=<AsStridedBackward0>)
after ssm_state from cache: tensor([[[[nan, nan, nan,  ..., nan, nan, nan]],

         [[nan, nan, nan,  ..., nan, nan, nan]],

         [[nan, nan, nan,  ..., nan, nan, nan]],

         ...,

         [[nan, nan, nan,  ..., nan, nan, nan]],

         [[nan, nan, nan,  ..., nan, nan, nan]],

         [[nan, nan, nan,  ..., nan, nan, nan]]]], device='cuda:0',
       dtype=torch.bfloat16)
here here
no back here:
hit for state: 21
before conv_state: tensor([[[ 1.6211e-01,  2.8672e+05, -3.1499e-21],
         [ 7.4219e-01, -2.6719e+00, -4.8633e-01],
         [-1.7928e-08, -5.2897e-38, -1.9806e-13],
         ...,
         [ 9.4531e-01,  2.2812e+00,  3.3750e+00],
         [-2.9559e-12, -3.7778e-19,  2.3745e-36],
         [ 2.2188e+00,  3.0312e+00,  3.6562e+00]]], device='cuda:0',
       dtype=torch.bfloat16, grad_fn=<AsStridedBackward0>)
before ssm_state: tensor([[[ 1.9531e-01, -6.2988e-02,  4.0283e-02,  ...,  2.2217e-02,
           5.0591e-12, -2.3071e-02],
         [ 9.4369e-16,  2.6131e-04,  2.9477e-19,  ..., -7.5195e-02,
           0.0000e+00,  4.3213e-02],
         [ 4.1602e-01, -3.2227e-01,  2.8320e-01,  ..., -4.6387e-02,
           1.7171e-09,  8.5449e-02],
         ...,
         [ 8.8379e-02, -5.4016e-03,  2.1606e-02,  ..., -6.7383e-02,
           8.3489e-13,  1.9434e-01],
         [ 4.9805e-01, -6.1646e-03,  2.2461e-01,  ..., -4.5776e-03,
           1.6466e-06,  2.6978e-02],
         [ 2.9492e-01, -6.3965e-02,  7.5195e-02,  ..., -1.2512e-03,
           6.1846e-11,  1.3916e-02]]], device='cuda:0', dtype=torch.bfloat16)
x.stride0: (1, 1, 1)
x.shape: torch.Size([1, 1, 3584])
x.stride1: (7168, 3584, 1)
x.stride2: (7168, 1, 3584)
conv_state1: (10752, 1, 3584)
conv_state2: (10752, 1, 3584)
conv_state3: (10752, 1, 3584)
conv_state: torch.Size([1, 3584, 3])
conv1d_out: torch.Size([1, 3584, 1])
rearrange(conv1d_out, 'b d l -> (b l) d') shape: torch.Size([1, 3584])
x_proj_weight shape: torch.Size([144, 3584])
x_dbl: torch.Size([1, 144])
delta_proj_weight @ x_dbl[:, :delta_rank].t(): torch.Size([3584, 1])
dsfxxxxxxdfs
after conv_state: tensor([[[ 2.8672e+05, -3.1499e-21,         nan],
         [-2.6719e+00, -4.8633e-01,         nan],
         [-5.2897e-38, -1.9806e-13,         nan],
         ...,
         [ 2.2812e+00,  3.3750e+00,         nan],
         [-3.7778e-19,  2.3745e-36,         nan],
         [ 3.0312e+00,  3.6562e+00,         nan]]], device='cuda:0',
       dtype=torch.bfloat16, grad_fn=<AsStridedBackward0>)
after ssm_state: tensor([[[nan, nan, nan,  ..., nan, nan, nan],
         [nan, nan, nan,  ..., nan, nan, nan],
         [nan, nan, nan,  ..., nan, nan, nan],
         ...,
         [nan, nan, nan,  ..., nan, nan, nan],
         [nan, nan, nan,  ..., nan, nan, nan],
         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:0',
       dtype=torch.bfloat16)
after conv from cache: tensor([[[ 2.8672e+05, -3.1499e-21,         nan],
         [-2.6719e+00, -4.8633e-01,         nan],
         [-5.2897e-38, -1.9806e-13,         nan],
         ...,
         [ 2.2812e+00,  3.3750e+00,         nan],
         [-3.7778e-19,  2.3745e-36,         nan],
         [ 3.0312e+00,  3.6562e+00,         nan]]], device='cuda:0',
       dtype=torch.bfloat16, grad_fn=<AsStridedBackward0>)
after ssm_state from cache: tensor([[[[nan, nan, nan,  ..., nan, nan, nan]],

         [[nan, nan, nan,  ..., nan, nan, nan]],

         [[nan, nan, nan,  ..., nan, nan, nan]],

         ...,

         [[nan, nan, nan,  ..., nan, nan, nan]],

         [[nan, nan, nan,  ..., nan, nan, nan]],

         [[nan, nan, nan,  ..., nan, nan, nan]]]], device='cuda:0',
       dtype=torch.bfloat16)
here here
no back here:
hit for state: 22
before conv_state: tensor([[[ 2.4531,  1.1094,  2.3125],
         [-0.1064,  1.0156,  2.6719],
         [ 2.0312,  0.1484,  1.0156],
         ...,
         [ 1.5625,  1.0234,  0.4512],
         [-0.9336, -0.7383,  0.8438],
         [ 0.6797, -2.5156, -0.1709]]], device='cuda:0', dtype=torch.bfloat16,
       grad_fn=<AsStridedBackward0>)
before ssm_state: tensor([[[ 2.2799e-06, -1.5156e+00,  5.8316e-23,  ...,  1.1670e-01,
           0.0000e+00,  2.6758e-01],
         [ 4.9593e-08,  1.8457e-01,  9.4502e-13,  ..., -2.6489e-02,
           0.0000e+00, -7.1289e-02],
         [ 3.1494e-02,  2.1875e-01,  1.5640e-03,  ..., -3.0365e-03,
           9.6780e-23, -8.4473e-02],
         ...,
         [ 4.6082e-03, -1.6968e-02,  1.3411e-07,  ..., -4.4556e-03,
           0.0000e+00, -5.3711e-02],
         [ 1.7462e-09,  8.2520e-02,  9.4147e-14,  ..., -6.3171e-03,
           0.0000e+00, -4.8096e-02],
         [ 6.9351e-21,  7.3242e-02,  1.4068e-34,  ...,  6.3171e-03,
           0.0000e+00,  6.6223e-03]]], device='cuda:0', dtype=torch.bfloat16)
x.stride0: (1, 1, 1)
x.shape: torch.Size([1, 1, 3584])
x.stride1: (7168, 3584, 1)
x.stride2: (7168, 1, 3584)
conv_state1: (10752, 1, 3584)
conv_state2: (10752, 1, 3584)
conv_state3: (10752, 1, 3584)
conv_state: torch.Size([1, 3584, 3])
conv1d_out: torch.Size([1, 3584, 1])
rearrange(conv1d_out, 'b d l -> (b l) d') shape: torch.Size([1, 3584])
x_proj_weight shape: torch.Size([144, 3584])
x_dbl: torch.Size([1, 144])
delta_proj_weight @ x_dbl[:, :delta_rank].t(): torch.Size([3584, 1])
dsfxxxxxxdfs
after conv_state: tensor([[[ 1.1094,  2.3125,     nan],
         [ 1.0156,  2.6719,     nan],
         [ 0.1484,  1.0156,     nan],
         ...,
         [ 1.0234,  0.4512,     nan],
         [-0.7383,  0.8438,     nan],
         [-2.5156, -0.1709,     nan]]], device='cuda:0', dtype=torch.bfloat16,
       grad_fn=<AsStridedBackward0>)
after ssm_state: tensor([[[nan, nan, nan,  ..., nan, nan, nan],
         [nan, nan, nan,  ..., nan, nan, nan],
         [nan, nan, nan,  ..., nan, nan, nan],
         ...,
         [nan, nan, nan,  ..., nan, nan, nan],
         [nan, nan, nan,  ..., nan, nan, nan],
         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:0',
       dtype=torch.bfloat16)
after conv from cache: tensor([[[ 1.1094,  2.3125,     nan],
         [ 1.0156,  2.6719,     nan],
         [ 0.1484,  1.0156,     nan],
         ...,
         [ 1.0234,  0.4512,     nan],
         [-0.7383,  0.8438,     nan],
         [-2.5156, -0.1709,     nan]]], device='cuda:0', dtype=torch.bfloat16,
       grad_fn=<AsStridedBackward0>)
after ssm_state from cache: tensor([[[[nan, nan, nan,  ..., nan, nan, nan]],

         [[nan, nan, nan,  ..., nan, nan, nan]],

         [[nan, nan, nan,  ..., nan, nan, nan]],

         ...,

         [[nan, nan, nan,  ..., nan, nan, nan]],

         [[nan, nan, nan,  ..., nan, nan, nan]],

         [[nan, nan, nan,  ..., nan, nan, nan]]]], device='cuda:0',
       dtype=torch.bfloat16)
here here
no back here:
hit for state: 23
before conv_state: tensor([[[nan, nan, nan],
         [nan, nan, nan],
         [nan, nan, nan],
         ...,
         [nan, nan, nan],
         [nan, nan, nan],
         [nan, nan, nan]]], device='cuda:0', dtype=torch.bfloat16,
       grad_fn=<AsStridedBackward0>)
before ssm_state: tensor([[[ 7.2266e-01,  3.4424e-02,  6.6797e-01,  ...,  2.4048e-02,
           4.6387e-03,  1.2268e-02],
         [ 2.1815e-05,  1.5259e-02,  6.2209e-10,  ...,  1.5564e-03,
           0.0000e+00,  3.2471e-02],
         [ 1.0014e-05, -3.8086e-01,  6.7085e-19,  ..., -6.0059e-02,
           0.0000e+00,  5.3406e-03],
         ...,
         [ 3.9648e-01, -9.3750e-02,  3.1055e-01,  ..., -4.0039e-02,
           8.6054e-07, -4.9072e-02],
         [ 5.5078e-01,  3.6621e-02,  3.5156e-01,  ...,  1.8433e-02,
           1.5793e-03,  2.7344e-02],
         [ 5.3516e-01, -1.3123e-02,  1.6113e-01,  ..., -1.2756e-02,
           1.6117e-04, -5.8350e-02]]], device='cuda:0', dtype=torch.bfloat16)
x.stride0: (1, 1, 1)
x.shape: torch.Size([1, 1, 3584])
x.stride1: (7168, 3584, 1)
x.stride2: (7168, 1, 3584)
conv_state1: (10752, 1, 3584)
conv_state2: (10752, 1, 3584)
conv_state3: (10752, 1, 3584)
conv_state: torch.Size([1, 3584, 3])
conv1d_out: torch.Size([1, 3584, 1])
rearrange(conv1d_out, 'b d l -> (b l) d') shape: torch.Size([1, 3584])
x_proj_weight shape: torch.Size([144, 3584])
x_dbl: torch.Size([1, 144])
delta_proj_weight @ x_dbl[:, :delta_rank].t(): torch.Size([3584, 1])
dsfxxxxxxdfs
after conv_state: tensor([[[nan, nan, nan],
         [nan, nan, nan],
         [nan, nan, nan],
         ...,
         [nan, nan, nan],
         [nan, nan, nan],
         [nan, nan, nan]]], device='cuda:0', dtype=torch.bfloat16,
       grad_fn=<AsStridedBackward0>)
after ssm_state: tensor([[[nan, nan, nan,  ..., nan, nan, nan],
         [nan, nan, nan,  ..., nan, nan, nan],
         [nan, nan, nan,  ..., nan, nan, nan],
         ...,
         [nan, nan, nan,  ..., nan, nan, nan],
         [nan, nan, nan,  ..., nan, nan, nan],
         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:0',
       dtype=torch.bfloat16)
after conv from cache: tensor([[[nan, nan, nan],
         [nan, nan, nan],
         [nan, nan, nan],
         ...,
         [nan, nan, nan],
         [nan, nan, nan],
         [nan, nan, nan]]], device='cuda:0', dtype=torch.bfloat16,
       grad_fn=<AsStridedBackward0>)
after ssm_state from cache: tensor([[[[nan, nan, nan,  ..., nan, nan, nan]],

         [[nan, nan, nan,  ..., nan, nan, nan]],

         [[nan, nan, nan,  ..., nan, nan, nan]],

         ...,

         [[nan, nan, nan,  ..., nan, nan, nan]],

         [[nan, nan, nan,  ..., nan, nan, nan]],

         [[nan, nan, nan,  ..., nan, nan, nan]]]], device='cuda:0',
       dtype=torch.bfloat16)
here here
no back here:
hit for state: 24
before conv_state: tensor([[[nan, nan, nan],
         [nan, nan, nan],
         [nan, nan, nan],
         ...,
         [nan, nan, nan],
         [nan, nan, nan],
         [nan, nan, nan]]], device='cuda:0', dtype=torch.bfloat16,
       grad_fn=<AsStridedBackward0>)
before ssm_state: tensor([[[ 6.4697e-03,  9.1797e-01,  1.2387e-07,  ..., -1.4648e-02,
           0.0000e+00,  6.5918e-02],
         [ 3.7812e-07, -3.1982e-02,  6.2585e-06,  ..., -1.6235e-02,
           0.0000e+00, -3.3264e-03],
         [ 3.2336e-06, -2.9419e-02,  2.1048e-07,  ...,  1.5076e-02,
           0.0000e+00, -8.8379e-02],
         ...,
         [ 2.2888e-04,  1.0645e-01,  9.4878e-09,  ...,  3.7354e-02,
           0.0000e+00,  1.4404e-02],
         [ 1.3924e-04, -2.2339e-02,  6.5613e-04,  ...,  3.8910e-03,
           0.0000e+00, -2.8687e-02],
         [ 2.6093e-03, -2.3682e-02,  7.4878e-07,  ...,  1.9531e-02,
           0.0000e+00, -6.1523e-02]]], device='cuda:0', dtype=torch.bfloat16)
x.stride0: (1, 1, 1)
x.shape: torch.Size([1, 1, 3584])
x.stride1: (7168, 3584, 1)
x.stride2: (7168, 1, 3584)
conv_state1: (10752, 1, 3584)
conv_state2: (10752, 1, 3584)
conv_state3: (10752, 1, 3584)
conv_state: torch.Size([1, 3584, 3])
conv1d_out: torch.Size([1, 3584, 1])
rearrange(conv1d_out, 'b d l -> (b l) d') shape: torch.Size([1, 3584])
x_proj_weight shape: torch.Size([144, 3584])
x_dbl: torch.Size([1, 144])
delta_proj_weight @ x_dbl[:, :delta_rank].t(): torch.Size([3584, 1])
dsfxxxxxxdfs
after conv_state: tensor([[[nan, nan, nan],
         [nan, nan, nan],
         [nan, nan, nan],
         ...,
         [nan, nan, nan],
         [nan, nan, nan],
         [nan, nan, nan]]], device='cuda:0', dtype=torch.bfloat16,
       grad_fn=<AsStridedBackward0>)
after ssm_state: tensor([[[nan, nan, nan,  ..., nan, nan, nan],
         [nan, nan, nan,  ..., nan, nan, nan],
         [nan, nan, nan,  ..., nan, nan, nan],
         ...,
         [nan, nan, nan,  ..., nan, nan, nan],
         [nan, nan, nan,  ..., nan, nan, nan],
         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:0',
       dtype=torch.bfloat16)
after conv from cache: tensor([[[nan, nan, nan],
         [nan, nan, nan],
         [nan, nan, nan],
         ...,
         [nan, nan, nan],
         [nan, nan, nan],
         [nan, nan, nan]]], device='cuda:0', dtype=torch.bfloat16,
       grad_fn=<AsStridedBackward0>)
after ssm_state from cache: tensor([[[[nan, nan, nan,  ..., nan, nan, nan]],

         [[nan, nan, nan,  ..., nan, nan, nan]],

         [[nan, nan, nan,  ..., nan, nan, nan]],

         ...,

         [[nan, nan, nan,  ..., nan, nan, nan]],

         [[nan, nan, nan,  ..., nan, nan, nan]],

         [[nan, nan, nan,  ..., nan, nan, nan]]]], device='cuda:0',
       dtype=torch.bfloat16)
here here
no back here:
hit for state: 25
before conv_state: tensor([[[nan, nan, nan],
         [nan, nan, nan],
         [nan, nan, nan],
         ...,
         [nan, nan, nan],
         [nan, nan, nan],
         [nan, nan, nan]]], device='cuda:0', dtype=torch.bfloat16,
       grad_fn=<AsStridedBackward0>)
before ssm_state: tensor([[[ 9.7656e-03,  6.1035e-02,  4.3631e-05,  ..., -7.9102e-02,
           0.0000e+00,  9.0408e-04],
         [ 4.0820e-01, -1.5411e-03,  1.7090e-01,  ...,  8.4229e-03,
           2.1756e-06,  5.6641e-02],
         [ 2.2445e-07, -1.0840e-01,  3.2187e-19,  ...,  2.2827e-02,
           0.0000e+00, -4.3701e-02],
         ...,
         [ 4.8438e-01, -3.4912e-02,  2.2266e-01,  ..., -3.7766e-04,
           6.5804e-05, -3.9551e-02],
         [ 2.6562e-01, -1.5039e-01,  4.1504e-02,  ...,  1.9169e-04,
           7.2760e-10, -3.2806e-03],
         [ 1.1276e-17, -3.5742e-01,  0.0000e+00,  ...,  2.3193e-03,
           0.0000e+00, -2.0020e-01]]], device='cuda:0', dtype=torch.bfloat16)
x.stride0: (1, 1, 1)
x.shape: torch.Size([1, 1, 3584])
x.stride1: (7168, 3584, 1)
x.stride2: (7168, 1, 3584)
conv_state1: (10752, 1, 3584)
conv_state2: (10752, 1, 3584)
conv_state3: (10752, 1, 3584)
conv_state: torch.Size([1, 3584, 3])
conv1d_out: torch.Size([1, 3584, 1])
rearrange(conv1d_out, 'b d l -> (b l) d') shape: torch.Size([1, 3584])
x_proj_weight shape: torch.Size([144, 3584])
x_dbl: torch.Size([1, 144])
delta_proj_weight @ x_dbl[:, :delta_rank].t(): torch.Size([3584, 1])
dsfxxxxxxdfs
after conv_state: tensor([[[nan, nan, nan],
         [nan, nan, nan],
         [nan, nan, nan],
         ...,
         [nan, nan, nan],
         [nan, nan, nan],
         [nan, nan, nan]]], device='cuda:0', dtype=torch.bfloat16,
       grad_fn=<AsStridedBackward0>)
after ssm_state: tensor([[[nan, nan, nan,  ..., nan, nan, nan],
         [nan, nan, nan,  ..., nan, nan, nan],
         [nan, nan, nan,  ..., nan, nan, nan],
         ...,
         [nan, nan, nan,  ..., nan, nan, nan],
         [nan, nan, nan,  ..., nan, nan, nan],
         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:0',
       dtype=torch.bfloat16)
after conv from cache: tensor([[[nan, nan, nan],
         [nan, nan, nan],
         [nan, nan, nan],
         ...,
         [nan, nan, nan],
         [nan, nan, nan],
         [nan, nan, nan]]], device='cuda:0', dtype=torch.bfloat16,
       grad_fn=<AsStridedBackward0>)
after ssm_state from cache: tensor([[[[nan, nan, nan,  ..., nan, nan, nan]],

         [[nan, nan, nan,  ..., nan, nan, nan]],

         [[nan, nan, nan,  ..., nan, nan, nan]],

         ...,

         [[nan, nan, nan,  ..., nan, nan, nan]],

         [[nan, nan, nan,  ..., nan, nan, nan]],

         [[nan, nan, nan,  ..., nan, nan, nan]]]], device='cuda:0',
       dtype=torch.bfloat16)
here here
no back here:
hit for state: 26
before conv_state: tensor([[[nan, nan, nan],
         [nan, nan, nan],
         [nan, nan, nan],
         ...,
         [nan, nan, nan],
         [nan, nan, nan],
         [nan, nan, nan]]], device='cuda:0', dtype=torch.bfloat16,
       grad_fn=<AsStridedBackward0>)
before ssm_state: tensor([[[ 2.1362e-02, -6.7749e-03,  1.5497e-05,  ...,  4.2236e-02,
           1.4670e-35, -1.4572e-03],
         [ 1.1115e-24, -8.8882e-04,  0.0000e+00,  ..., -9.1309e-02,
           0.0000e+00,  2.6550e-03],
         [ 3.5577e-07,  7.0312e-02,  2.1788e-15,  ...,  5.3406e-03,
           0.0000e+00,  1.6357e-02],
         ...,
         [ 4.0430e-01, -1.5259e-02,  1.4648e-01,  ..., -2.3804e-03,
           2.6584e-05,  4.1580e-04],
         [ 7.6172e-01, -2.0386e-02,  5.8594e-01,  ..., -2.7710e-02,
           2.7466e-02, -7.7820e-03],
         [ 3.9413e-15, -6.7383e-02,  1.5267e-25,  ..., -6.3477e-02,
           0.0000e+00, -1.4709e-02]]], device='cuda:0', dtype=torch.bfloat16)
x.stride0: (1, 1, 1)
x.shape: torch.Size([1, 1, 3584])
x.stride1: (7168, 3584, 1)
x.stride2: (7168, 1, 3584)
conv_state1: (10752, 1, 3584)
conv_state2: (10752, 1, 3584)
conv_state3: (10752, 1, 3584)
conv_state: torch.Size([1, 3584, 3])
conv1d_out: torch.Size([1, 3584, 1])
rearrange(conv1d_out, 'b d l -> (b l) d') shape: torch.Size([1, 3584])
x_proj_weight shape: torch.Size([144, 3584])
x_dbl: torch.Size([1, 144])
delta_proj_weight @ x_dbl[:, :delta_rank].t(): torch.Size([3584, 1])
dsfxxxxxxdfs
after conv_state: tensor([[[nan, nan, nan],
         [nan, nan, nan],
         [nan, nan, nan],
         ...,
         [nan, nan, nan],
         [nan, nan, nan],
         [nan, nan, nan]]], device='cuda:0', dtype=torch.bfloat16,
       grad_fn=<AsStridedBackward0>)
after ssm_state: tensor([[[nan, nan, nan,  ..., nan, nan, nan],
         [nan, nan, nan,  ..., nan, nan, nan],
         [nan, nan, nan,  ..., nan, nan, nan],
         ...,
         [nan, nan, nan,  ..., nan, nan, nan],
         [nan, nan, nan,  ..., nan, nan, nan],
         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:0',
       dtype=torch.bfloat16)
after conv from cache: tensor([[[nan, nan, nan],
         [nan, nan, nan],
         [nan, nan, nan],
         ...,
         [nan, nan, nan],
         [nan, nan, nan],
         [nan, nan, nan]]], device='cuda:0', dtype=torch.bfloat16,
       grad_fn=<AsStridedBackward0>)
after ssm_state from cache: tensor([[[[nan, nan, nan,  ..., nan, nan, nan]],

         [[nan, nan, nan,  ..., nan, nan, nan]],

         [[nan, nan, nan,  ..., nan, nan, nan]],

         ...,

         [[nan, nan, nan,  ..., nan, nan, nan]],

         [[nan, nan, nan,  ..., nan, nan, nan]],

         [[nan, nan, nan,  ..., nan, nan, nan]]]], device='cuda:0',
       dtype=torch.bfloat16)
here here
no back here:
hit for state: 27
before conv_state: tensor([[[-0.6719, -0.7734, -1.7812],
         [-1.4062, -3.0781, -3.3594],
         [-0.7734, -1.4609, -1.2812],
         ...,
         [ 1.9688,  1.1562, -0.1147],
         [ 2.3438,  3.2812,  4.0312],
         [ 0.3262,  0.0527,  0.5586]]], device='cuda:0', dtype=torch.bfloat16,
       grad_fn=<AsStridedBackward0>)
before ssm_state: tensor([[[ 6.0201e-06,  9.3750e-02,  1.4097e-10,  ..., -1.6602e-02,
           0.0000e+00, -4.6143e-02],
         [ 3.6603e-27,  6.6895e-02,  0.0000e+00,  ...,  1.6724e-02,
           0.0000e+00,  6.0059e-02],
         [ 9.7656e-02, -5.8203e-01,  5.7678e-03,  ..., -1.2512e-02,
           1.3678e-13, -2.4707e-01],
         ...,
         [ 4.9174e-07, -2.0508e-01,  3.4284e-13,  ...,  2.6001e-02,
           0.0000e+00, -2.2583e-02],
         [ 1.9989e-03,  1.0059e-01,  9.0003e-06,  ...,  2.9602e-03,
           4.5943e-27,  7.0312e-02],
         [ 7.5602e-12, -6.7444e-03,  7.1498e-14,  ...,  9.5215e-03,
           0.0000e+00, -8.4229e-03]]], device='cuda:0', dtype=torch.bfloat16)
x.stride0: (1, 1, 1)
x.shape: torch.Size([1, 1, 3584])
x.stride1: (7168, 3584, 1)
x.stride2: (7168, 1, 3584)
conv_state1: (10752, 1, 3584)
conv_state2: (10752, 1, 3584)
conv_state3: (10752, 1, 3584)
conv_state: torch.Size([1, 3584, 3])
conv1d_out: torch.Size([1, 3584, 1])
rearrange(conv1d_out, 'b d l -> (b l) d') shape: torch.Size([1, 3584])
x_proj_weight shape: torch.Size([144, 3584])
x_dbl: torch.Size([1, 144])
delta_proj_weight @ x_dbl[:, :delta_rank].t(): torch.Size([3584, 1])
dsfxxxxxxdfs
after conv_state: tensor([[[-0.7734, -1.7812,     nan],
         [-3.0781, -3.3594,     nan],
         [-1.4609, -1.2812,     nan],
         ...,
         [ 1.1562, -0.1147,     nan],
         [ 3.2812,  4.0312,     nan],
         [ 0.0527,  0.5586,     nan]]], device='cuda:0', dtype=torch.bfloat16,
       grad_fn=<AsStridedBackward0>)
after ssm_state: tensor([[[nan, nan, nan,  ..., nan, nan, nan],
         [nan, nan, nan,  ..., nan, nan, nan],
         [nan, nan, nan,  ..., nan, nan, nan],
         ...,
         [nan, nan, nan,  ..., nan, nan, nan],
         [nan, nan, nan,  ..., nan, nan, nan],
         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:0',
       dtype=torch.bfloat16)
after conv from cache: tensor([[[-0.7734, -1.7812,     nan],
         [-3.0781, -3.3594,     nan],
         [-1.4609, -1.2812,     nan],
         ...,
         [ 1.1562, -0.1147,     nan],
         [ 3.2812,  4.0312,     nan],
         [ 0.0527,  0.5586,     nan]]], device='cuda:0', dtype=torch.bfloat16,
       grad_fn=<AsStridedBackward0>)
after ssm_state from cache: tensor([[[[nan, nan, nan,  ..., nan, nan, nan]],

         [[nan, nan, nan,  ..., nan, nan, nan]],

         [[nan, nan, nan,  ..., nan, nan, nan]],

         ...,

         [[nan, nan, nan,  ..., nan, nan, nan]],

         [[nan, nan, nan,  ..., nan, nan, nan]],

         [[nan, nan, nan,  ..., nan, nan, nan]]]], device='cuda:0',
       dtype=torch.bfloat16)
here here
no back here:
hit for state: 28
before conv_state: tensor([[[-0.0752,  1.6797,  0.7148],
         [-1.3359, -2.1562, -1.3203],
         [ 2.7812,  0.4219,  0.6211],
         ...,
         [-1.9375, -1.5391, -0.7734],
         [ 1.7891,  2.4062,  0.3125],
         [-0.7891, -0.6758, -0.0957]]], device='cuda:0', dtype=torch.bfloat16,
       grad_fn=<AsStridedBackward0>)
before ssm_state: tensor([[[ 1.3065e-04, -6.7871e-02,  5.4538e-06,  ..., -1.3916e-02,
           0.0000e+00,  3.4912e-02],
         [ 1.6644e-10, -8.3984e-02,  8.4655e-16,  ..., -1.2085e-02,
           0.0000e+00,  1.5747e-02],
         [ 5.2296e-11,  7.5195e-02,  5.3871e-19,  ..., -1.8433e-02,
           0.0000e+00,  2.9602e-03],
         ...,
         [ 2.1838e-22,  7.5195e-02,  6.8409e-31,  ..., -4.8256e-04,
           0.0000e+00, -2.7954e-02],
         [ 4.5402e-09, -6.5234e-01,  2.4869e-13,  ...,  1.7700e-02,
           0.0000e+00,  1.5137e-02],
         [ 2.0664e-09,  1.4062e-01,  2.0783e-13,  ...,  6.6528e-03,
           0.0000e+00, -8.0078e-02]]], device='cuda:0', dtype=torch.bfloat16)
x.stride0: (1, 1, 1)
x.shape: torch.Size([1, 1, 3584])
x.stride1: (7168, 3584, 1)
x.stride2: (7168, 1, 3584)
conv_state1: (10752, 1, 3584)
conv_state2: (10752, 1, 3584)
conv_state3: (10752, 1, 3584)
conv_state: torch.Size([1, 3584, 3])
conv1d_out: torch.Size([1, 3584, 1])
rearrange(conv1d_out, 'b d l -> (b l) d') shape: torch.Size([1, 3584])
x_proj_weight shape: torch.Size([144, 3584])
x_dbl: torch.Size([1, 144])
delta_proj_weight @ x_dbl[:, :delta_rank].t(): torch.Size([3584, 1])
dsfxxxxxxdfs
after conv_state: tensor([[[ 1.6797,  0.7148,     nan],
         [-2.1562, -1.3203,     nan],
         [ 0.4219,  0.6211,     nan],
         ...,
         [-1.5391, -0.7734,     nan],
         [ 2.4062,  0.3125,     nan],
         [-0.6758, -0.0957,     nan]]], device='cuda:0', dtype=torch.bfloat16,
       grad_fn=<AsStridedBackward0>)
after ssm_state: tensor([[[nan, nan, nan,  ..., nan, nan, nan],
         [nan, nan, nan,  ..., nan, nan, nan],
         [nan, nan, nan,  ..., nan, nan, nan],
         ...,
         [nan, nan, nan,  ..., nan, nan, nan],
         [nan, nan, nan,  ..., nan, nan, nan],
         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:0',
       dtype=torch.bfloat16)
after conv from cache: tensor([[[ 1.6797,  0.7148,     nan],
         [-2.1562, -1.3203,     nan],
         [ 0.4219,  0.6211,     nan],
         ...,
         [-1.5391, -0.7734,     nan],
         [ 2.4062,  0.3125,     nan],
         [-0.6758, -0.0957,     nan]]], device='cuda:0', dtype=torch.bfloat16,
       grad_fn=<AsStridedBackward0>)
after ssm_state from cache: tensor([[[[nan, nan, nan,  ..., nan, nan, nan]],

         [[nan, nan, nan,  ..., nan, nan, nan]],

         [[nan, nan, nan,  ..., nan, nan, nan]],

         ...,

         [[nan, nan, nan,  ..., nan, nan, nan]],

         [[nan, nan, nan,  ..., nan, nan, nan]],

         [[nan, nan, nan,  ..., nan, nan, nan]]]], device='cuda:0',
       dtype=torch.bfloat16)
here here
no back here:
hit for state: 29
before conv_state: tensor([[[ 2.1875, -0.2617,  0.4355],
         [-2.3281, -2.8125,  0.4160],
         [ 1.3438,  2.7031,  3.9844],
         ...,
         [-0.0554,  2.0625,  1.5625],
         [-0.6602, -1.5547, -0.2676],
         [-0.2539, -0.8633, -1.5156]]], device='cuda:0', dtype=torch.bfloat16,
       grad_fn=<AsStridedBackward0>)
before ssm_state: tensor([[[ 5.0354e-04, -2.1387e-01,  2.4438e-06,  ...,  1.5488e-03,
           0.0000e+00, -8.7402e-02],
         [ 4.3945e-03, -1.3965e-01,  1.2684e-04,  ..., -4.7302e-03,
           0.0000e+00, -2.5269e-02],
         [ 1.3062e-02,  5.8984e-01,  1.7405e-05,  ...,  9.3994e-03,
           0.0000e+00,  5.0049e-02],
         ...,
         [ 9.5749e-04,  3.2031e+00,  2.4587e-06,  ...,  1.7212e-02,
           0.0000e+00,  4.7461e-01],
         [ 2.4438e-06, -1.1621e-01,  1.4976e-06,  ..., -3.4637e-03,
           0.0000e+00, -5.8838e-02],
         [ 2.8876e-11, -9.6191e-02,  2.9754e-14,  ..., -1.7395e-03,
           0.0000e+00, -3.6865e-02]]], device='cuda:0', dtype=torch.bfloat16)
x.stride0: (1, 1, 1)
x.shape: torch.Size([1, 1, 3584])
x.stride1: (7168, 3584, 1)
x.stride2: (7168, 1, 3584)
conv_state1: (10752, 1, 3584)
conv_state2: (10752, 1, 3584)
conv_state3: (10752, 1, 3584)
conv_state: torch.Size([1, 3584, 3])
conv1d_out: torch.Size([1, 3584, 1])
rearrange(conv1d_out, 'b d l -> (b l) d') shape: torch.Size([1, 3584])
x_proj_weight shape: torch.Size([144, 3584])
x_dbl: torch.Size([1, 144])
delta_proj_weight @ x_dbl[:, :delta_rank].t(): torch.Size([3584, 1])
dsfxxxxxxdfs
after conv_state: tensor([[[-0.2617,  0.4355,     nan],
         [-2.8125,  0.4160,     nan],
         [ 2.7031,  3.9844,     nan],
         ...,
         [ 2.0625,  1.5625,     nan],
         [-1.5547, -0.2676,     nan],
         [-0.8633, -1.5156,     nan]]], device='cuda:0', dtype=torch.bfloat16,
       grad_fn=<AsStridedBackward0>)
after ssm_state: tensor([[[nan, nan, nan,  ..., nan, nan, nan],
         [nan, nan, nan,  ..., nan, nan, nan],
         [nan, nan, nan,  ..., nan, nan, nan],
         ...,
         [nan, nan, nan,  ..., nan, nan, nan],
         [nan, nan, nan,  ..., nan, nan, nan],
         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:0',
       dtype=torch.bfloat16)
after conv from cache: tensor([[[-0.2617,  0.4355,     nan],
         [-2.8125,  0.4160,     nan],
         [ 2.7031,  3.9844,     nan],
         ...,
         [ 2.0625,  1.5625,     nan],
         [-1.5547, -0.2676,     nan],
         [-0.8633, -1.5156,     nan]]], device='cuda:0', dtype=torch.bfloat16,
       grad_fn=<AsStridedBackward0>)
after ssm_state from cache: tensor([[[[nan, nan, nan,  ..., nan, nan, nan]],

         [[nan, nan, nan,  ..., nan, nan, nan]],

         [[nan, nan, nan,  ..., nan, nan, nan]],

         ...,

         [[nan, nan, nan,  ..., nan, nan, nan]],

         [[nan, nan, nan,  ..., nan, nan, nan]],

         [[nan, nan, nan,  ..., nan, nan, nan]]]], device='cuda:0',
       dtype=torch.bfloat16)
here here
no back here:
hit for state: 30
before conv_state: tensor([[[-2.4688, -2.0938, -2.6719],
         [-4.1250, -6.4062, -5.4688],
         [-2.8750,  0.9492,  0.5469],
         ...,
         [ 0.1196,  0.4004, -0.0505],
         [ 0.2891, -2.4531, -2.4688],
         [ 2.4531, -1.3828,  0.1250]]], device='cuda:0', dtype=torch.bfloat16,
       grad_fn=<AsStridedBackward0>)
before ssm_state: tensor([[[ 8.6042e-16,  1.6504e-01,  6.6691e-24,  ...,  1.9165e-02,
           0.0000e+00,  3.3008e-01],
         [ 1.4544e-05, -1.1572e-01,  4.5657e-10,  ..., -4.1199e-03,
           0.0000e+00, -4.0283e-02],
         [ 2.3749e-08,  6.5430e-02,  9.4868e-19,  ...,  1.0400e-01,
           0.0000e+00,  2.3633e-01],
         ...,
         [ 2.6245e-03,  1.7188e-01,  2.8312e-06,  ...,  6.4697e-03,
           0.0000e+00,  4.6692e-03],
         [ 6.7969e-01,  1.9684e-03,  6.2109e-01,  ..., -1.7090e-03,
           2.4414e-02,  1.7334e-02],
         [ 4.0359e-12,  3.5938e-01,  2.7105e-20,  ...,  3.9307e-02,
           0.0000e+00,  6.6895e-02]]], device='cuda:0', dtype=torch.bfloat16)
x.stride0: (1, 1, 1)
x.shape: torch.Size([1, 1, 3584])
x.stride1: (7168, 3584, 1)
x.stride2: (7168, 1, 3584)
conv_state1: (10752, 1, 3584)
conv_state2: (10752, 1, 3584)
conv_state3: (10752, 1, 3584)
conv_state: torch.Size([1, 3584, 3])
conv1d_out: torch.Size([1, 3584, 1])
rearrange(conv1d_out, 'b d l -> (b l) d') shape: torch.Size([1, 3584])
x_proj_weight shape: torch.Size([144, 3584])
x_dbl: torch.Size([1, 144])
delta_proj_weight @ x_dbl[:, :delta_rank].t(): torch.Size([3584, 1])
dsfxxxxxxdfs
after conv_state: tensor([[[-2.0938, -2.6719,     nan],
         [-6.4062, -5.4688,     nan],
         [ 0.9492,  0.5469,     nan],
         ...,
         [ 0.4004, -0.0505,     nan],
         [-2.4531, -2.4688,     nan],
         [-1.3828,  0.1250,     nan]]], device='cuda:0', dtype=torch.bfloat16,
       grad_fn=<AsStridedBackward0>)
after ssm_state: tensor([[[nan, nan, nan,  ..., nan, nan, nan],
         [nan, nan, nan,  ..., nan, nan, nan],
         [nan, nan, nan,  ..., nan, nan, nan],
         ...,
         [nan, nan, nan,  ..., nan, nan, nan],
         [nan, nan, nan,  ..., nan, nan, nan],
         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:0',
       dtype=torch.bfloat16)
after conv from cache: tensor([[[-2.0938, -2.6719,     nan],
         [-6.4062, -5.4688,     nan],
         [ 0.9492,  0.5469,     nan],
         ...,
         [ 0.4004, -0.0505,     nan],
         [-2.4531, -2.4688,     nan],
         [-1.3828,  0.1250,     nan]]], device='cuda:0', dtype=torch.bfloat16,
       grad_fn=<AsStridedBackward0>)
after ssm_state from cache: tensor([[[[nan, nan, nan,  ..., nan, nan, nan]],

         [[nan, nan, nan,  ..., nan, nan, nan]],

         [[nan, nan, nan,  ..., nan, nan, nan]],

         ...,

         [[nan, nan, nan,  ..., nan, nan, nan]],

         [[nan, nan, nan,  ..., nan, nan, nan]],

         [[nan, nan, nan,  ..., nan, nan, nan]]]], device='cuda:0',
       dtype=torch.bfloat16)
here here
no back here:
hit for state: 31
before conv_state: tensor([[[ 0.6680, -0.8320,  0.2402],
         [ 0.5039,  3.8281,  2.1562],
         [ 2.3438,  4.7188,  1.1562],
         ...,
         [ 0.3555, -1.0938, -1.8281],
         [-1.4531, -1.9141, -3.6250],
         [-2.2500,  1.5312,  1.1016]]], device='cuda:0', dtype=torch.bfloat16,
       grad_fn=<AsStridedBackward0>)
before ssm_state: tensor([[[ 1.9776e-16, -5.1514e-02,  3.3773e-30,  ..., -9.4604e-03,
           0.0000e+00,  2.8198e-02],
         [ 7.9744e-09,  2.1094e-01,  7.5286e-25,  ...,  2.7618e-03,
           0.0000e+00,  5.9570e-02],
         [ 3.9290e-10, -1.3281e-01,  1.6156e-24,  ..., -2.8687e-03,
           0.0000e+00,  7.1777e-02],
         ...,
         [ 1.3123e-03,  2.4023e-01,  1.2100e-05,  ...,  1.2329e-02,
           0.0000e+00, -1.4160e-01],
         [ 5.6066e-15, -5.4443e-02,  9.3401e-28,  ..., -9.6436e-03,
           0.0000e+00,  6.1340e-03],
         [ 6.9033e-20,  1.1426e-01,  0.0000e+00,  ...,  9.5703e-02,
           0.0000e+00, -1.5625e-01]]], device='cuda:0', dtype=torch.bfloat16)
x.stride0: (1, 1, 1)
x.shape: torch.Size([1, 1, 3584])
x.stride1: (7168, 3584, 1)
x.stride2: (7168, 1, 3584)
conv_state1: (10752, 1, 3584)
conv_state2: (10752, 1, 3584)
conv_state3: (10752, 1, 3584)
conv_state: torch.Size([1, 3584, 3])
conv1d_out: torch.Size([1, 3584, 1])
rearrange(conv1d_out, 'b d l -> (b l) d') shape: torch.Size([1, 3584])
x_proj_weight shape: torch.Size([144, 3584])
x_dbl: torch.Size([1, 144])
delta_proj_weight @ x_dbl[:, :delta_rank].t(): torch.Size([3584, 1])
dsfxxxxxxdfs
after conv_state: tensor([[[-0.8320,  0.2402,     nan],
         [ 3.8281,  2.1562,     nan],
         [ 4.7188,  1.1562,     nan],
         ...,
         [-1.0938, -1.8281,     nan],
         [-1.9141, -3.6250,     nan],
         [ 1.5312,  1.1016,     nan]]], device='cuda:0', dtype=torch.bfloat16,
       grad_fn=<AsStridedBackward0>)
after ssm_state: tensor([[[nan, nan, nan,  ..., nan, nan, nan],
         [nan, nan, nan,  ..., nan, nan, nan],
         [nan, nan, nan,  ..., nan, nan, nan],
         ...,
         [nan, nan, nan,  ..., nan, nan, nan],
         [nan, nan, nan,  ..., nan, nan, nan],
         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:0',
       dtype=torch.bfloat16)
after conv from cache: tensor([[[-0.8320,  0.2402,     nan],
         [ 3.8281,  2.1562,     nan],
         [ 4.7188,  1.1562,     nan],
         ...,
         [-1.0938, -1.8281,     nan],
         [-1.9141, -3.6250,     nan],
         [ 1.5312,  1.1016,     nan]]], device='cuda:0', dtype=torch.bfloat16,
       grad_fn=<AsStridedBackward0>)
after ssm_state from cache: tensor([[[[nan, nan, nan,  ..., nan, nan, nan]],

         [[nan, nan, nan,  ..., nan, nan, nan]],

         [[nan, nan, nan,  ..., nan, nan, nan]],

         ...,

         [[nan, nan, nan,  ..., nan, nan, nan]],

         [[nan, nan, nan,  ..., nan, nan, nan]],

         [[nan, nan, nan,  ..., nan, nan, nan]]]], device='cuda:0',
       dtype=torch.bfloat16)
here here
no back here:
hit for state: 32
before conv_state: tensor([[[ 0.9688,  1.3516,  2.2500],
         [ 2.6875, -0.5977, -0.9453],
         [ 1.3594, -1.9688, -2.8750],
         ...,
         [-1.0859,  1.7109, -1.1016],
         [-0.2227, -0.3867,  1.1953],
         [ 1.5156,  1.8438,  3.0156]]], device='cuda:0', dtype=torch.bfloat16,
       grad_fn=<AsStridedBackward0>)
before ssm_state: tensor([[[ 3.3936e-02,  1.2256e-01,  4.6692e-03,  ..., -1.2207e-02,
           1.3235e-22, -5.7129e-02],
         [ 4.2411e-14,  2.9102e-01,  1.3805e-28,  ..., -3.3691e-02,
           0.0000e+00, -1.0889e-01],
         [ 2.5024e-03,  1.2268e-02,  9.1270e-08,  ..., -5.0781e-02,
           0.0000e+00, -1.2256e-01],
         ...,
         [ 2.2870e-20,  2.6733e-02,  3.4080e-22,  ...,  6.1279e-02,
           0.0000e+00,  1.2793e-01],
         [ 1.0550e-09,  1.1816e-01,  7.0344e-13,  ..., -1.0840e-01,
           0.0000e+00, -3.5938e-01],
         [ 7.6294e-06, -7.8613e-02,  4.1723e-06,  ...,  1.2756e-02,
           0.0000e+00,  4.3457e-02]]], device='cuda:0', dtype=torch.bfloat16)
x.stride0: (1, 1, 1)
x.shape: torch.Size([1, 1, 3584])
x.stride1: (7168, 3584, 1)
x.stride2: (7168, 1, 3584)
conv_state1: (10752, 1, 3584)
conv_state2: (10752, 1, 3584)
conv_state3: (10752, 1, 3584)
conv_state: torch.Size([1, 3584, 3])
conv1d_out: torch.Size([1, 3584, 1])
rearrange(conv1d_out, 'b d l -> (b l) d') shape: torch.Size([1, 3584])
x_proj_weight shape: torch.Size([144, 3584])
x_dbl: torch.Size([1, 144])
delta_proj_weight @ x_dbl[:, :delta_rank].t(): torch.Size([3584, 1])
dsfxxxxxxdfs
after conv_state: tensor([[[ 1.3516,  2.2500,     nan],
         [-0.5977, -0.9453,     nan],
         [-1.9688, -2.8750,     nan],
         ...,
         [ 1.7109, -1.1016,     nan],
         [-0.3867,  1.1953,     nan],
         [ 1.8438,  3.0156,     nan]]], device='cuda:0', dtype=torch.bfloat16,
       grad_fn=<AsStridedBackward0>)
after ssm_state: tensor([[[nan, nan, nan,  ..., nan, nan, nan],
         [nan, nan, nan,  ..., nan, nan, nan],
         [nan, nan, nan,  ..., nan, nan, nan],
         ...,
         [nan, nan, nan,  ..., nan, nan, nan],
         [nan, nan, nan,  ..., nan, nan, nan],
         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:0',
       dtype=torch.bfloat16)
after conv from cache: tensor([[[ 1.3516,  2.2500,     nan],
         [-0.5977, -0.9453,     nan],
         [-1.9688, -2.8750,     nan],
         ...,
         [ 1.7109, -1.1016,     nan],
         [-0.3867,  1.1953,     nan],
         [ 1.8438,  3.0156,     nan]]], device='cuda:0', dtype=torch.bfloat16,
       grad_fn=<AsStridedBackward0>)
after ssm_state from cache: tensor([[[[nan, nan, nan,  ..., nan, nan, nan]],

         [[nan, nan, nan,  ..., nan, nan, nan]],

         [[nan, nan, nan,  ..., nan, nan, nan]],

         ...,

         [[nan, nan, nan,  ..., nan, nan, nan]],

         [[nan, nan, nan,  ..., nan, nan, nan]],

         [[nan, nan, nan,  ..., nan, nan, nan]]]], device='cuda:0',
       dtype=torch.bfloat16)
here here
no back here:
hit for state: 33
before conv_state: tensor([[[-1.2188, -1.3672, -0.9805],
         [ 3.2500, -1.2422, -2.2031],
         [ 0.0253, -1.0391, -1.3047],
         ...,
         [-0.0500,  2.0781,  2.7969],
         [ 0.3672, -0.6016,  0.6836],
         [-4.7188,  1.5703,  0.2080]]], device='cuda:0', dtype=torch.bfloat16,
       grad_fn=<AsStridedBackward0>)
before ssm_state: tensor([[[ 9.0594e-14, -4.0039e-01,  3.1447e-34,  ...,  1.1035e-01,
           0.0000e+00, -3.6133e-02],
         [ 1.5571e-09, -2.0703e-01,  3.3218e-13,  ..., -4.6387e-02,
           0.0000e+00,  2.5146e-02],
         [ 4.2948e-32,  1.0889e-01,  0.0000e+00,  ..., -6.0547e-02,
           0.0000e+00,  4.7607e-02],
         ...,
         [ 3.0909e-13, -3.8818e-02,  3.7902e-31,  ...,  3.9307e-02,
           0.0000e+00, -2.1240e-02],
         [ 4.3164e-01,  5.7129e-02,  1.4160e-01,  ..., -3.5645e-02,
           2.0582e-07,  3.6011e-03],
         [ 3.5858e-03, -6.6797e-01,  1.3411e-06,  ...,  1.8848e-01,
           0.0000e+00, -8.1055e-02]]], device='cuda:0', dtype=torch.bfloat16)
x.stride0: (1, 1, 1)
x.shape: torch.Size([1, 1, 3584])
x.stride1: (7168, 3584, 1)
x.stride2: (7168, 1, 3584)
conv_state1: (10752, 1, 3584)
conv_state2: (10752, 1, 3584)
conv_state3: (10752, 1, 3584)
conv_state: torch.Size([1, 3584, 3])
conv1d_out: torch.Size([1, 3584, 1])
rearrange(conv1d_out, 'b d l -> (b l) d') shape: torch.Size([1, 3584])
x_proj_weight shape: torch.Size([144, 3584])
x_dbl: torch.Size([1, 144])
delta_proj_weight @ x_dbl[:, :delta_rank].t(): torch.Size([3584, 1])
dsfxxxxxxdfs
after conv_state: tensor([[[-1.3672, -0.9805,     nan],
         [-1.2422, -2.2031,     nan],
         [-1.0391, -1.3047,     nan],
         ...,
         [ 2.0781,  2.7969,     nan],
         [-0.6016,  0.6836,     nan],
         [ 1.5703,  0.2080,     nan]]], device='cuda:0', dtype=torch.bfloat16,
       grad_fn=<AsStridedBackward0>)
after ssm_state: tensor([[[nan, nan, nan,  ..., nan, nan, nan],
         [nan, nan, nan,  ..., nan, nan, nan],
         [nan, nan, nan,  ..., nan, nan, nan],
         ...,
         [nan, nan, nan,  ..., nan, nan, nan],
         [nan, nan, nan,  ..., nan, nan, nan],
         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:0',
       dtype=torch.bfloat16)
after conv from cache: tensor([[[-1.3672, -0.9805,     nan],
         [-1.2422, -2.2031,     nan],
         [-1.0391, -1.3047,     nan],
         ...,
         [ 2.0781,  2.7969,     nan],
         [-0.6016,  0.6836,     nan],
         [ 1.5703,  0.2080,     nan]]], device='cuda:0', dtype=torch.bfloat16,
       grad_fn=<AsStridedBackward0>)
after ssm_state from cache: tensor([[[[nan, nan, nan,  ..., nan, nan, nan]],

         [[nan, nan, nan,  ..., nan, nan, nan]],

         [[nan, nan, nan,  ..., nan, nan, nan]],

         ...,

         [[nan, nan, nan,  ..., nan, nan, nan]],

         [[nan, nan, nan,  ..., nan, nan, nan]],

         [[nan, nan, nan,  ..., nan, nan, nan]]]], device='cuda:0',
       dtype=torch.bfloat16)
here here
no back here:
hit for state: 34
before conv_state: tensor([[[-2.1562, -0.6680,  0.0698],
         [ 0.7344, -1.0469, -0.2363],
         [-2.2188,  0.0140, -1.8516],
         ...,
         [-1.4062,  1.9844,  1.8984],
         [-0.2793,  4.8438,  4.9375],
         [-1.8281, -0.1016,  2.0938]]], device='cuda:0', dtype=torch.bfloat16,
       grad_fn=<AsStridedBackward0>)
before ssm_state: tensor([[[ 3.9685e-35, -6.0791e-02,  0.0000e+00,  ..., -1.2817e-02,
           0.0000e+00,  4.2969e-02],
         [ 1.2369e-10,  2.1250e+00,  5.8163e-32,  ...,  2.2461e-02,
           0.0000e+00, -4.5898e-02],
         [ 2.5466e-09,  6.1768e-02,  4.7976e-23,  ..., -1.8921e-03,
           0.0000e+00, -6.4850e-04],
         ...,
         [ 3.5970e-36,  5.2344e-01,  0.0000e+00,  ...,  1.4258e-01,
           0.0000e+00, -2.8516e-01],
         [ 5.0923e-24, -2.2507e-04,  0.0000e+00,  ..., -2.2583e-02,
           0.0000e+00,  4.7852e-02],
         [ 1.9895e-11, -1.8799e-02,  9.1073e-18,  ..., -3.5095e-03,
           0.0000e+00,  8.3618e-03]]], device='cuda:0', dtype=torch.bfloat16)
x.stride0: (1, 1, 1)
x.shape: torch.Size([1, 1, 3584])
x.stride1: (7168, 3584, 1)
x.stride2: (7168, 1, 3584)
conv_state1: (10752, 1, 3584)
conv_state2: (10752, 1, 3584)
conv_state3: (10752, 1, 3584)
conv_state: torch.Size([1, 3584, 3])
conv1d_out: torch.Size([1, 3584, 1])
rearrange(conv1d_out, 'b d l -> (b l) d') shape: torch.Size([1, 3584])
x_proj_weight shape: torch.Size([144, 3584])
x_dbl: torch.Size([1, 144])
delta_proj_weight @ x_dbl[:, :delta_rank].t(): torch.Size([3584, 1])
dsfxxxxxxdfs
after conv_state: tensor([[[-0.6680,  0.0698,     nan],
         [-1.0469, -0.2363,     nan],
         [ 0.0140, -1.8516,     nan],
         ...,
         [ 1.9844,  1.8984,     nan],
         [ 4.8438,  4.9375,     nan],
         [-0.1016,  2.0938,     nan]]], device='cuda:0', dtype=torch.bfloat16,
       grad_fn=<AsStridedBackward0>)
after ssm_state: tensor([[[nan, nan, nan,  ..., nan, nan, nan],
         [nan, nan, nan,  ..., nan, nan, nan],
         [nan, nan, nan,  ..., nan, nan, nan],
         ...,
         [nan, nan, nan,  ..., nan, nan, nan],
         [nan, nan, nan,  ..., nan, nan, nan],
         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:0',
       dtype=torch.bfloat16)
after conv from cache: tensor([[[-0.6680,  0.0698,     nan],
         [-1.0469, -0.2363,     nan],
         [ 0.0140, -1.8516,     nan],
         ...,
         [ 1.9844,  1.8984,     nan],
         [ 4.8438,  4.9375,     nan],
         [-0.1016,  2.0938,     nan]]], device='cuda:0', dtype=torch.bfloat16,
       grad_fn=<AsStridedBackward0>)
after ssm_state from cache: tensor([[[[nan, nan, nan,  ..., nan, nan, nan]],

         [[nan, nan, nan,  ..., nan, nan, nan]],

         [[nan, nan, nan,  ..., nan, nan, nan]],

         ...,

         [[nan, nan, nan,  ..., nan, nan, nan]],

         [[nan, nan, nan,  ..., nan, nan, nan]],

         [[nan, nan, nan,  ..., nan, nan, nan]]]], device='cuda:0',
       dtype=torch.bfloat16)
here here
no back here:
hit for state: 35
before conv_state: tensor([[[-2.4062, -0.1133,  0.5039],
         [ 0.8594,  3.2188,  0.8086],
         [-1.6875,  0.6953,  1.2578],
         ...,
         [ 2.7812,  2.5000,  4.0938],
         [-1.5312, -0.6016, -2.1406],
         [-0.6602, -4.0000, -2.6875]]], device='cuda:0', dtype=torch.bfloat16,
       grad_fn=<AsStridedBackward0>)
before ssm_state: tensor([[[ 2.8046e-33,  1.8555e-02,  0.0000e+00,  ...,  3.6377e-02,
           0.0000e+00, -1.9409e-02],
         [ 2.6245e-02, -2.2583e-03,  3.1090e-04,  ...,  1.3367e-02,
           8.8984e-28,  3.8385e-05],
         [ 2.3293e-21,  1.1414e-02,  0.0000e+00,  ...,  5.2734e-02,
           0.0000e+00,  2.4902e-02],
         ...,
         [ 1.4710e-15,  2.3438e-02,  6.5000e-33,  ...,  3.7842e-02,
           0.0000e+00, -2.4261e-03],
         [ 1.2079e-12,  7.2327e-03,  5.2459e-29,  ..., -3.2715e-02,
           0.0000e+00,  1.0803e-02],
         [ 1.3770e-01,  1.8387e-03,  7.7637e-02,  ...,  5.4321e-03,
           1.3915e-10, -3.7384e-03]]], device='cuda:0', dtype=torch.bfloat16)
x.stride0: (1, 1, 1)
x.shape: torch.Size([1, 1, 3584])
x.stride1: (7168, 3584, 1)
x.stride2: (7168, 1, 3584)
conv_state1: (10752, 1, 3584)
conv_state2: (10752, 1, 3584)
conv_state3: (10752, 1, 3584)
conv_state: torch.Size([1, 3584, 3])
conv1d_out: torch.Size([1, 3584, 1])
rearrange(conv1d_out, 'b d l -> (b l) d') shape: torch.Size([1, 3584])
x_proj_weight shape: torch.Size([144, 3584])
x_dbl: torch.Size([1, 144])
delta_proj_weight @ x_dbl[:, :delta_rank].t(): torch.Size([3584, 1])
dsfxxxxxxdfs
after conv_state: tensor([[[-0.1133,  0.5039,     nan],
         [ 3.2188,  0.8086,     nan],
         [ 0.6953,  1.2578,     nan],
         ...,
         [ 2.5000,  4.0938,     nan],
         [-0.6016, -2.1406,     nan],
         [-4.0000, -2.6875,     nan]]], device='cuda:0', dtype=torch.bfloat16,
       grad_fn=<AsStridedBackward0>)
after ssm_state: tensor([[[nan, nan, nan,  ..., nan, nan, nan],
         [nan, nan, nan,  ..., nan, nan, nan],
         [nan, nan, nan,  ..., nan, nan, nan],
         ...,
         [nan, nan, nan,  ..., nan, nan, nan],
         [nan, nan, nan,  ..., nan, nan, nan],
         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:0',
       dtype=torch.bfloat16)
after conv from cache: tensor([[[-0.1133,  0.5039,     nan],
         [ 3.2188,  0.8086,     nan],
         [ 0.6953,  1.2578,     nan],
         ...,
         [ 2.5000,  4.0938,     nan],
         [-0.6016, -2.1406,     nan],
         [-4.0000, -2.6875,     nan]]], device='cuda:0', dtype=torch.bfloat16,
       grad_fn=<AsStridedBackward0>)
after ssm_state from cache: tensor([[[[nan, nan, nan,  ..., nan, nan, nan]],

         [[nan, nan, nan,  ..., nan, nan, nan]],

         [[nan, nan, nan,  ..., nan, nan, nan]],

         ...,

         [[nan, nan, nan,  ..., nan, nan, nan]],

         [[nan, nan, nan,  ..., nan, nan, nan]],

         [[nan, nan, nan,  ..., nan, nan, nan]]]], device='cuda:0',
       dtype=torch.bfloat16)
here here
no back here:
hit for state: 36
before conv_state: tensor([[[ 0.3457, -0.6406,  1.7109],
         [-1.8672, -0.6406,  0.7344],
         [-1.5547, -0.9805, -0.5312],
         ...,
         [-2.9062, -1.9844, -1.3828],
         [-0.0654,  3.3594,  3.7812],
         [ 1.0625,  3.8281,  6.3438]]], device='cuda:0', dtype=torch.bfloat16,
       grad_fn=<AsStridedBackward0>)
before ssm_state: tensor([[[ 1.6211e-01, -5.0049e-02,  1.5869e-02,  ...,  8.7280e-03,
           2.7358e-08,  2.6001e-02],
         [ 6.5234e-01,  2.2827e-02,  4.7266e-01,  ..., -7.9956e-03,
           6.0730e-03,  1.5717e-03],
         [ 9.9609e-01,  6.6757e-04,  9.9219e-01,  ..., -5.4932e-04,
           9.5312e-01,  1.0834e-03],
         ...,
         [ 7.6172e-01,  4.3457e-02,  6.6016e-01,  ..., -7.0190e-03,
           2.7344e-01,  4.1504e-03],
         [ 1.0000e+00, -8.2970e-05,  1.0000e+00,  ..., -3.2663e-05,
           9.9609e-01, -2.5940e-04],
         [ 9.9219e-01,  8.8501e-04,  9.8828e-01,  ...,  1.9073e-04,
           9.0234e-01,  2.6398e-03]]], device='cuda:0', dtype=torch.bfloat16)
x.stride0: (1, 1, 1)
x.shape: torch.Size([1, 1, 3584])
x.stride1: (7168, 3584, 1)
x.stride2: (7168, 1, 3584)
conv_state1: (10752, 1, 3584)
conv_state2: (10752, 1, 3584)
conv_state3: (10752, 1, 3584)
conv_state: torch.Size([1, 3584, 3])
conv1d_out: torch.Size([1, 3584, 1])
rearrange(conv1d_out, 'b d l -> (b l) d') shape: torch.Size([1, 3584])
x_proj_weight shape: torch.Size([144, 3584])
x_dbl: torch.Size([1, 144])
delta_proj_weight @ x_dbl[:, :delta_rank].t(): torch.Size([3584, 1])
dsfxxxxxxdfs
after conv_state: tensor([[[-0.6406,  1.7109,     nan],
         [-0.6406,  0.7344,     nan],
         [-0.9805, -0.5312,     nan],
         ...,
         [-1.9844, -1.3828,     nan],
         [ 3.3594,  3.7812,     nan],
         [ 3.8281,  6.3438,     nan]]], device='cuda:0', dtype=torch.bfloat16,
       grad_fn=<AsStridedBackward0>)
after ssm_state: tensor([[[nan, nan, nan,  ..., nan, nan, nan],
         [nan, nan, nan,  ..., nan, nan, nan],
         [nan, nan, nan,  ..., nan, nan, nan],
         ...,
         [nan, nan, nan,  ..., nan, nan, nan],
         [nan, nan, nan,  ..., nan, nan, nan],
         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:0',
       dtype=torch.bfloat16)
after conv from cache: tensor([[[-0.6406,  1.7109,     nan],
         [-0.6406,  0.7344,     nan],
         [-0.9805, -0.5312,     nan],
         ...,
         [-1.9844, -1.3828,     nan],
         [ 3.3594,  3.7812,     nan],
         [ 3.8281,  6.3438,     nan]]], device='cuda:0', dtype=torch.bfloat16,
       grad_fn=<AsStridedBackward0>)
after ssm_state from cache: tensor([[[[nan, nan, nan,  ..., nan, nan, nan]],

         [[nan, nan, nan,  ..., nan, nan, nan]],

         [[nan, nan, nan,  ..., nan, nan, nan]],

         ...,

         [[nan, nan, nan,  ..., nan, nan, nan]],

         [[nan, nan, nan,  ..., nan, nan, nan]],

         [[nan, nan, nan,  ..., nan, nan, nan]]]], device='cuda:0',
       dtype=torch.bfloat16)
here here
no back here:
hit for state: 37
before conv_state: tensor([[[-0.0297,  2.3750,  1.7891],
         [ 0.4590,  1.4844,  1.5938],
         [ 1.9531, -0.6016, -2.8438],
         ...,
         [ 0.9375, -0.7891, -3.5625],
         [ 0.7266,  1.1484, -0.5508],
         [-0.9648,  0.0466,  0.8008]]], device='cuda:0', dtype=torch.bfloat16,
       grad_fn=<AsStridedBackward0>)
before ssm_state: tensor([[[ 7.1491e-30, -7.9102e-02,  0.0000e+00,  ..., -1.3123e-02,
           0.0000e+00,  5.5847e-03],
         [ 4.7032e-08,  3.3447e-02,  6.6746e-19,  ..., -3.9307e-02,
           0.0000e+00,  2.8931e-02],
         [ 1.1514e-21,  3.1006e-02,  0.0000e+00,  ...,  3.0396e-02,
           0.0000e+00,  3.1738e-03],
         ...,
         [ 1.6309e-01,  2.0703e-01,  3.0151e-02,  ..., -1.2024e-02,
           3.2117e-12,  6.9275e-03],
         [ 3.0075e-30,  1.0449e-01,  0.0000e+00,  ..., -9.6191e-02,
           0.0000e+00, -2.1118e-02],
         [ 4.4936e-08, -1.1426e-01,  3.1752e-14,  ...,  1.8311e-02,
           0.0000e+00, -1.2283e-03]]], device='cuda:0', dtype=torch.bfloat16)
x.stride0: (1, 1, 1)
x.shape: torch.Size([1, 1, 3584])
x.stride1: (7168, 3584, 1)
x.stride2: (7168, 1, 3584)
conv_state1: (10752, 1, 3584)
conv_state2: (10752, 1, 3584)
conv_state3: (10752, 1, 3584)
conv_state: torch.Size([1, 3584, 3])
conv1d_out: torch.Size([1, 3584, 1])
rearrange(conv1d_out, 'b d l -> (b l) d') shape: torch.Size([1, 3584])
x_proj_weight shape: torch.Size([144, 3584])
x_dbl: torch.Size([1, 144])
delta_proj_weight @ x_dbl[:, :delta_rank].t(): torch.Size([3584, 1])
dsfxxxxxxdfs
after conv_state: tensor([[[ 2.3750,  1.7891,     nan],
         [ 1.4844,  1.5938,     nan],
         [-0.6016, -2.8438,     nan],
         ...,
         [-0.7891, -3.5625,     nan],
         [ 1.1484, -0.5508,     nan],
         [ 0.0466,  0.8008,     nan]]], device='cuda:0', dtype=torch.bfloat16,
       grad_fn=<AsStridedBackward0>)
after ssm_state: tensor([[[nan, nan, nan,  ..., nan, nan, nan],
         [nan, nan, nan,  ..., nan, nan, nan],
         [nan, nan, nan,  ..., nan, nan, nan],
         ...,
         [nan, nan, nan,  ..., nan, nan, nan],
         [nan, nan, nan,  ..., nan, nan, nan],
         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:0',
       dtype=torch.bfloat16)
after conv from cache: tensor([[[ 2.3750,  1.7891,     nan],
         [ 1.4844,  1.5938,     nan],
         [-0.6016, -2.8438,     nan],
         ...,
         [-0.7891, -3.5625,     nan],
         [ 1.1484, -0.5508,     nan],
         [ 0.0466,  0.8008,     nan]]], device='cuda:0', dtype=torch.bfloat16,
       grad_fn=<AsStridedBackward0>)
after ssm_state from cache: tensor([[[[nan, nan, nan,  ..., nan, nan, nan]],

         [[nan, nan, nan,  ..., nan, nan, nan]],

         [[nan, nan, nan,  ..., nan, nan, nan]],

         ...,

         [[nan, nan, nan,  ..., nan, nan, nan]],

         [[nan, nan, nan,  ..., nan, nan, nan]],

         [[nan, nan, nan,  ..., nan, nan, nan]]]], device='cuda:0',
       dtype=torch.bfloat16)
here here
no back here:
hit for state: 38
before conv_state: tensor([[[-2.8125, -2.2031,  0.4023],
         [ 1.3984,  1.4297, -4.0000],
         [ 3.0625, -1.0547,  2.4531],
         ...,
         [ 1.9453, -0.9375, -0.2695],
         [ 1.4844,  3.4219,  5.2188],
         [ 1.6328,  3.8906,  4.4688]]], device='cuda:0', dtype=torch.bfloat16,
       grad_fn=<AsStridedBackward0>)
before ssm_state: tensor([[[ 2.7930e-01, -3.8605e-03,  1.2891e-01,  ..., -4.5471e-03,
           1.0245e-07,  1.1978e-03],
         [ 4.6194e-06, -8.5449e-02,  2.9132e-13,  ...,  2.6123e-02,
           0.0000e+00, -1.1108e-02],
         [ 2.2736e-03, -3.8086e-01,  1.2696e-05,  ...,  3.5889e-02,
           5.4073e-37,  5.4321e-03],
         ...,
         [ 3.3111e-12, -4.3945e-01,  5.5671e-35,  ...,  1.2695e-01,
           0.0000e+00, -2.3047e-01],
         [ 1.2817e-02, -1.8945e-01,  4.0054e-05,  ...,  9.9487e-03,
           3.7363e-32,  2.3499e-03],
         [ 5.3406e-05, -1.3550e-02,  2.3874e-12,  ...,  3.3691e-02,
           0.0000e+00, -9.5215e-03]]], device='cuda:0', dtype=torch.bfloat16)
x.stride0: (1, 1, 1)
x.shape: torch.Size([1, 1, 3584])
x.stride1: (7168, 3584, 1)
x.stride2: (7168, 1, 3584)
conv_state1: (10752, 1, 3584)
conv_state2: (10752, 1, 3584)
conv_state3: (10752, 1, 3584)
conv_state: torch.Size([1, 3584, 3])
conv1d_out: torch.Size([1, 3584, 1])
rearrange(conv1d_out, 'b d l -> (b l) d') shape: torch.Size([1, 3584])
x_proj_weight shape: torch.Size([144, 3584])
x_dbl: torch.Size([1, 144])
delta_proj_weight @ x_dbl[:, :delta_rank].t(): torch.Size([3584, 1])
dsfxxxxxxdfs
after conv_state: tensor([[[-2.2031,  0.4023,     nan],
         [ 1.4297, -4.0000,     nan],
         [-1.0547,  2.4531,     nan],
         ...,
         [-0.9375, -0.2695,     nan],
         [ 3.4219,  5.2188,     nan],
         [ 3.8906,  4.4688,     nan]]], device='cuda:0', dtype=torch.bfloat16,
       grad_fn=<AsStridedBackward0>)
after ssm_state: tensor([[[nan, nan, nan,  ..., nan, nan, nan],
         [nan, nan, nan,  ..., nan, nan, nan],
         [nan, nan, nan,  ..., nan, nan, nan],
         ...,
         [nan, nan, nan,  ..., nan, nan, nan],
         [nan, nan, nan,  ..., nan, nan, nan],
         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:0',
       dtype=torch.bfloat16)
after conv from cache: tensor([[[-2.2031,  0.4023,     nan],
         [ 1.4297, -4.0000,     nan],
         [-1.0547,  2.4531,     nan],
         ...,
         [-0.9375, -0.2695,     nan],
         [ 3.4219,  5.2188,     nan],
         [ 3.8906,  4.4688,     nan]]], device='cuda:0', dtype=torch.bfloat16,
       grad_fn=<AsStridedBackward0>)
after ssm_state from cache: tensor([[[[nan, nan, nan,  ..., nan, nan, nan]],

         [[nan, nan, nan,  ..., nan, nan, nan]],

         [[nan, nan, nan,  ..., nan, nan, nan]],

         ...,

         [[nan, nan, nan,  ..., nan, nan, nan]],

         [[nan, nan, nan,  ..., nan, nan, nan]],

         [[nan, nan, nan,  ..., nan, nan, nan]]]], device='cuda:0',
       dtype=torch.bfloat16)
here here
no back here:
hit for state: 39
before conv_state: tensor([[[-1.3750, -2.4062,  1.3594],
         [-0.4375,  1.7188,  2.3594],
         [ 2.6094, -1.6250,  2.3125],
         ...,
         [-2.4531,  0.3789, -0.9336],
         [ 1.4453,  1.4297,  0.6875],
         [ 2.2969,  0.4102,  1.0781]]], device='cuda:0', dtype=torch.bfloat16,
       grad_fn=<AsStridedBackward0>)
before ssm_state: tensor([[[ 2.7657e-04, -2.9053e-02,  1.3784e-06,  ...,  7.3242e-04,
           0.0000e+00, -2.1973e-02],
         [ 2.8320e-01,  6.1340e-03,  6.2988e-02,  ..., -5.1270e-03,
           3.5507e-09,  8.0566e-03],
         [ 4.1809e-03,  1.1719e-02,  2.0303e-07,  ..., -1.1902e-02,
           8.1250e-35, -6.1951e-03],
         ...,
         [ 4.9477e-10,  1.1169e-02,  3.4994e-13,  ...,  5.8899e-03,
           0.0000e+00,  7.4768e-03],
         [ 5.9204e-03,  1.2573e-02,  2.3842e-07,  ...,  1.7456e-02,
           3.7392e-28,  7.3624e-04],
         [ 8.1539e-05,  5.3711e-02,  3.2187e-06,  ...,  5.6152e-02,
           0.0000e+00, -1.3550e-02]]], device='cuda:0', dtype=torch.bfloat16)
x.stride0: (1, 1, 1)
x.shape: torch.Size([1, 1, 3584])
x.stride1: (7168, 3584, 1)
x.stride2: (7168, 1, 3584)
conv_state1: (10752, 1, 3584)
conv_state2: (10752, 1, 3584)
conv_state3: (10752, 1, 3584)
conv_state: torch.Size([1, 3584, 3])
conv1d_out: torch.Size([1, 3584, 1])
rearrange(conv1d_out, 'b d l -> (b l) d') shape: torch.Size([1, 3584])
x_proj_weight shape: torch.Size([144, 3584])
x_dbl: torch.Size([1, 144])
delta_proj_weight @ x_dbl[:, :delta_rank].t(): torch.Size([3584, 1])
dsfxxxxxxdfs
after conv_state: tensor([[[-2.4062,  1.3594,     nan],
         [ 1.7188,  2.3594,     nan],
         [-1.6250,  2.3125,     nan],
         ...,
         [ 0.3789, -0.9336,     nan],
         [ 1.4297,  0.6875,     nan],
         [ 0.4102,  1.0781,     nan]]], device='cuda:0', dtype=torch.bfloat16,
       grad_fn=<AsStridedBackward0>)
after ssm_state: tensor([[[nan, nan, nan,  ..., nan, nan, nan],
         [nan, nan, nan,  ..., nan, nan, nan],
         [nan, nan, nan,  ..., nan, nan, nan],
         ...,
         [nan, nan, nan,  ..., nan, nan, nan],
         [nan, nan, nan,  ..., nan, nan, nan],
         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:0',
       dtype=torch.bfloat16)
after conv from cache: tensor([[[-2.4062,  1.3594,     nan],
         [ 1.7188,  2.3594,     nan],
         [-1.6250,  2.3125,     nan],
         ...,
         [ 0.3789, -0.9336,     nan],
         [ 1.4297,  0.6875,     nan],
         [ 0.4102,  1.0781,     nan]]], device='cuda:0', dtype=torch.bfloat16,
       grad_fn=<AsStridedBackward0>)
after ssm_state from cache: tensor([[[[nan, nan, nan,  ..., nan, nan, nan]],

         [[nan, nan, nan,  ..., nan, nan, nan]],

         [[nan, nan, nan,  ..., nan, nan, nan]],

         ...,

         [[nan, nan, nan,  ..., nan, nan, nan]],

         [[nan, nan, nan,  ..., nan, nan, nan]],

         [[nan, nan, nan,  ..., nan, nan, nan]]]], device='cuda:0',
       dtype=torch.bfloat16)
here here
no back here:
hit for state: 40
before conv_state: tensor([[[-1.6797,  1.2891,  3.3438],
         [ 0.7305, -2.9219, -0.1992],
         [ 2.3281, -0.3047,  2.2812],
         ...,
         [-0.7812, -2.8906, -1.0625],
         [ 3.2188,  3.2500,  4.5625],
         [-5.2812, -1.1328, -1.9609]]], device='cuda:0', dtype=torch.bfloat16,
       grad_fn=<AsStridedBackward0>)
before ssm_state: tensor([[[ 4.1602e-01, -2.0264e-02,  1.5430e-01,  ...,  3.9978e-03,
           1.9968e-06, -2.7313e-03],
         [ 7.1668e-10, -3.7109e-02,  1.2089e-17,  ..., -1.8463e-03,
           0.0000e+00, -2.2705e-02],
         [ 5.9686e-12, -1.5234e-01,  2.4670e-20,  ..., -5.7129e-02,
           0.0000e+00,  3.6377e-02],
         ...,
         [ 2.8125e-01, -3.4180e-02,  5.2246e-02,  ..., -5.1270e-03,
           1.2282e-08,  6.0425e-03],
         [ 1.1749e-03, -9.0790e-04,  6.1035e-05,  ..., -7.3547e-03,
           5.9142e-38, -1.3550e-02],
         [ 9.4922e-01,  1.1215e-03,  8.7109e-01,  ...,  2.2278e-03,
           3.9062e-01, -1.8997e-03]]], device='cuda:0', dtype=torch.bfloat16)
x.stride0: (1, 1, 1)
x.shape: torch.Size([1, 1, 3584])
x.stride1: (7168, 3584, 1)
x.stride2: (7168, 1, 3584)
conv_state1: (10752, 1, 3584)
conv_state2: (10752, 1, 3584)
conv_state3: (10752, 1, 3584)
conv_state: torch.Size([1, 3584, 3])
conv1d_out: torch.Size([1, 3584, 1])
rearrange(conv1d_out, 'b d l -> (b l) d') shape: torch.Size([1, 3584])
x_proj_weight shape: torch.Size([144, 3584])
x_dbl: torch.Size([1, 144])
delta_proj_weight @ x_dbl[:, :delta_rank].t(): torch.Size([3584, 1])
dsfxxxxxxdfs
after conv_state: tensor([[[ 1.2891,  3.3438,     nan],
         [-2.9219, -0.1992,     nan],
         [-0.3047,  2.2812,     nan],
         ...,
         [-2.8906, -1.0625,     nan],
         [ 3.2500,  4.5625,     nan],
         [-1.1328, -1.9609,     nan]]], device='cuda:0', dtype=torch.bfloat16,
       grad_fn=<AsStridedBackward0>)
after ssm_state: tensor([[[nan, nan, nan,  ..., nan, nan, nan],
         [nan, nan, nan,  ..., nan, nan, nan],
         [nan, nan, nan,  ..., nan, nan, nan],
         ...,
         [nan, nan, nan,  ..., nan, nan, nan],
         [nan, nan, nan,  ..., nan, nan, nan],
         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:0',
       dtype=torch.bfloat16)
after conv from cache: tensor([[[ 1.2891,  3.3438,     nan],
         [-2.9219, -0.1992,     nan],
         [-0.3047,  2.2812,     nan],
         ...,
         [-2.8906, -1.0625,     nan],
         [ 3.2500,  4.5625,     nan],
         [-1.1328, -1.9609,     nan]]], device='cuda:0', dtype=torch.bfloat16,
       grad_fn=<AsStridedBackward0>)
after ssm_state from cache: tensor([[[[nan, nan, nan,  ..., nan, nan, nan]],

         [[nan, nan, nan,  ..., nan, nan, nan]],

         [[nan, nan, nan,  ..., nan, nan, nan]],

         ...,

         [[nan, nan, nan,  ..., nan, nan, nan]],

         [[nan, nan, nan,  ..., nan, nan, nan]],

         [[nan, nan, nan,  ..., nan, nan, nan]]]], device='cuda:0',
       dtype=torch.bfloat16)
here here
no back here:
hit for state: 41
before conv_state: tensor([[[ 0.9648,  1.0469,  1.7109],
         [-0.7891,  0.2676,  3.9531],
         [ 1.6797,  1.0781, -0.4824],
         ...,
         [ 4.4688,  6.5938,  4.3125],
         [-0.2412,  4.0000,  4.5938],
         [-1.1172, -3.0312, -7.0938]]], device='cuda:0', dtype=torch.bfloat16,
       grad_fn=<AsStridedBackward0>)
before ssm_state: tensor([[[ 5.5469e-01, -2.6733e-02,  2.2852e-01,  ...,  4.7913e-03,
           3.9482e-04, -4.7852e-02],
         [ 3.3984e-01, -1.9073e-03,  1.4355e-01,  ...,  2.0294e-03,
           1.0356e-06,  1.5076e-02],
         [ 5.7422e-01, -2.2705e-02,  3.4375e-01,  ..., -2.2827e-02,
           1.4305e-05, -4.0527e-02],
         ...,
         [ 9.2285e-02, -1.2402e-01,  3.2471e-02,  ...,  5.3711e-03,
           5.7259e-19, -4.1504e-03],
         [ 6.0722e-07, -1.7853e-03,  6.3238e-13,  ..., -1.0834e-03,
           0.0000e+00, -1.1215e-03],
         [ 3.6812e-04,  1.6968e-02,  8.9058e-09,  ...,  6.9580e-03,
           0.0000e+00,  1.6235e-02]]], device='cuda:0', dtype=torch.bfloat16)
x.stride0: (1, 1, 1)
x.shape: torch.Size([1, 1, 3584])
x.stride1: (7168, 3584, 1)
x.stride2: (7168, 1, 3584)
conv_state1: (10752, 1, 3584)
conv_state2: (10752, 1, 3584)
conv_state3: (10752, 1, 3584)
conv_state: torch.Size([1, 3584, 3])
conv1d_out: torch.Size([1, 3584, 1])
rearrange(conv1d_out, 'b d l -> (b l) d') shape: torch.Size([1, 3584])
x_proj_weight shape: torch.Size([144, 3584])
x_dbl: torch.Size([1, 144])
delta_proj_weight @ x_dbl[:, :delta_rank].t(): torch.Size([3584, 1])
dsfxxxxxxdfs
after conv_state: tensor([[[ 1.0469,  1.7109,     nan],
         [ 0.2676,  3.9531,     nan],
         [ 1.0781, -0.4824,     nan],
         ...,
         [ 6.5938,  4.3125,     nan],
         [ 4.0000,  4.5938,     nan],
         [-3.0312, -7.0938,     nan]]], device='cuda:0', dtype=torch.bfloat16,
       grad_fn=<AsStridedBackward0>)
after ssm_state: tensor([[[nan, nan, nan,  ..., nan, nan, nan],
         [nan, nan, nan,  ..., nan, nan, nan],
         [nan, nan, nan,  ..., nan, nan, nan],
         ...,
         [nan, nan, nan,  ..., nan, nan, nan],
         [nan, nan, nan,  ..., nan, nan, nan],
         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:0',
       dtype=torch.bfloat16)
after conv from cache: tensor([[[ 1.0469,  1.7109,     nan],
         [ 0.2676,  3.9531,     nan],
         [ 1.0781, -0.4824,     nan],
         ...,
         [ 6.5938,  4.3125,     nan],
         [ 4.0000,  4.5938,     nan],
         [-3.0312, -7.0938,     nan]]], device='cuda:0', dtype=torch.bfloat16,
       grad_fn=<AsStridedBackward0>)
after ssm_state from cache: tensor([[[[nan, nan, nan,  ..., nan, nan, nan]],

         [[nan, nan, nan,  ..., nan, nan, nan]],

         [[nan, nan, nan,  ..., nan, nan, nan]],

         ...,

         [[nan, nan, nan,  ..., nan, nan, nan]],

         [[nan, nan, nan,  ..., nan, nan, nan]],

         [[nan, nan, nan,  ..., nan, nan, nan]]]], device='cuda:0',
       dtype=torch.bfloat16)
here here
no back here:
hit for state: 42
before conv_state: tensor([[[ 0.8047, -2.5469, -2.6875],
         [ 6.3438,  5.4375,  2.6250],
         [ 3.8594,  1.3047,  3.2031],
         ...,
         [-1.9297, -3.7656, -7.4375],
         [-1.1875, -0.3457, -0.9375],
         [ 2.8594,  2.8594,  0.6953]]], device='cuda:0', dtype=torch.bfloat16,
       grad_fn=<AsStridedBackward0>)
before ssm_state: tensor([[[ 1.1102e-15,  9.7656e-02,  1.8735e-29,  ...,  1.6479e-02,
           0.0000e+00, -5.1758e-02],
         [ 1.5616e-05, -9.6436e-03,  2.6048e-09,  ..., -4.1199e-03,
           0.0000e+00, -1.6846e-02],
         [ 9.0122e-05, -4.1016e-02,  6.0245e-09,  ..., -5.0659e-03,
           0.0000e+00,  2.3438e-02],
         ...,
         [ 1.0986e-01,  2.0410e-01,  2.0874e-02,  ..., -7.9346e-03,
           2.4564e-20, -3.2715e-02],
         [ 7.6294e-05, -9.2773e-02,  1.6317e-06,  ...,  3.4809e-05,
           3.2914e-35,  2.5879e-02],
         [ 6.0547e-01, -3.5248e-03,  4.3164e-01,  ...,  1.7548e-03,
           1.8921e-03,  3.7384e-03]]], device='cuda:0', dtype=torch.bfloat16)
x.stride0: (1, 1, 1)
x.shape: torch.Size([1, 1, 3584])
x.stride1: (7168, 3584, 1)
x.stride2: (7168, 1, 3584)
conv_state1: (10752, 1, 3584)
conv_state2: (10752, 1, 3584)
conv_state3: (10752, 1, 3584)
conv_state: torch.Size([1, 3584, 3])
conv1d_out: torch.Size([1, 3584, 1])
rearrange(conv1d_out, 'b d l -> (b l) d') shape: torch.Size([1, 3584])
x_proj_weight shape: torch.Size([144, 3584])
x_dbl: torch.Size([1, 144])
delta_proj_weight @ x_dbl[:, :delta_rank].t(): torch.Size([3584, 1])
dsfxxxxxxdfs
after conv_state: tensor([[[-2.5469, -2.6875,     nan],
         [ 5.4375,  2.6250,     nan],
         [ 1.3047,  3.2031,     nan],
         ...,
         [-3.7656, -7.4375,     nan],
         [-0.3457, -0.9375,     nan],
         [ 2.8594,  0.6953,     nan]]], device='cuda:0', dtype=torch.bfloat16,
       grad_fn=<AsStridedBackward0>)
after ssm_state: tensor([[[nan, nan, nan,  ..., nan, nan, nan],
         [nan, nan, nan,  ..., nan, nan, nan],
         [nan, nan, nan,  ..., nan, nan, nan],
         ...,
         [nan, nan, nan,  ..., nan, nan, nan],
         [nan, nan, nan,  ..., nan, nan, nan],
         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:0',
       dtype=torch.bfloat16)
after conv from cache: tensor([[[-2.5469, -2.6875,     nan],
         [ 5.4375,  2.6250,     nan],
         [ 1.3047,  3.2031,     nan],
         ...,
         [-3.7656, -7.4375,     nan],
         [-0.3457, -0.9375,     nan],
         [ 2.8594,  0.6953,     nan]]], device='cuda:0', dtype=torch.bfloat16,
       grad_fn=<AsStridedBackward0>)
after ssm_state from cache: tensor([[[[nan, nan, nan,  ..., nan, nan, nan]],

         [[nan, nan, nan,  ..., nan, nan, nan]],

         [[nan, nan, nan,  ..., nan, nan, nan]],

         ...,

         [[nan, nan, nan,  ..., nan, nan, nan]],

         [[nan, nan, nan,  ..., nan, nan, nan]],

         [[nan, nan, nan,  ..., nan, nan, nan]]]], device='cuda:0',
       dtype=torch.bfloat16)
here here
no back here:
hit for state: 43
before conv_state: tensor([[[ 2.6250,  4.0625,  3.8906],
         [-1.0312, -3.6406, -3.8125],
         [ 0.0100, -3.9375, -1.8672],
         ...,
         [-0.5938, -3.9062, -2.7969],
         [ 0.4551, -1.8672, -0.7812],
         [-2.0781,  0.8203,  1.7109]]], device='cuda:0', dtype=torch.bfloat16,
       grad_fn=<AsStridedBackward0>)
before ssm_state: tensor([[[ 1.5162e-19, -1.9043e-01,  0.0000e+00,  ...,  5.2246e-02,
           0.0000e+00,  1.9043e-02],
         [ 2.4156e-09, -1.1902e-02,  8.4199e-13,  ..., -1.9409e-02,
           0.0000e+00,  9.4604e-03],
         [ 2.9138e-18, -2.8516e-01,  2.4154e-22,  ...,  3.1006e-02,
           0.0000e+00,  1.2573e-02],
         ...,
         [ 7.3412e-24, -1.3281e-01,  0.0000e+00,  ...,  5.2490e-03,
           0.0000e+00,  5.6763e-03],
         [ 1.0490e-05, -7.3242e-02,  4.4107e-06,  ...,  4.7852e-02,
           0.0000e+00,  1.2512e-02],
         [ 9.1735e-08,  5.4297e-01,  4.9266e-16,  ..., -1.7773e-01,
           0.0000e+00, -1.0303e-01]]], device='cuda:0', dtype=torch.bfloat16)
x.stride0: (1, 1, 1)
x.shape: torch.Size([1, 1, 3584])
x.stride1: (7168, 3584, 1)
x.stride2: (7168, 1, 3584)
conv_state1: (10752, 1, 3584)
conv_state2: (10752, 1, 3584)
conv_state3: (10752, 1, 3584)
conv_state: torch.Size([1, 3584, 3])
conv1d_out: torch.Size([1, 3584, 1])
rearrange(conv1d_out, 'b d l -> (b l) d') shape: torch.Size([1, 3584])
x_proj_weight shape: torch.Size([144, 3584])
x_dbl: torch.Size([1, 144])
delta_proj_weight @ x_dbl[:, :delta_rank].t(): torch.Size([3584, 1])
dsfxxxxxxdfs
after conv_state: tensor([[[ 4.0625,  3.8906,     nan],
         [-3.6406, -3.8125,     nan],
         [-3.9375, -1.8672,     nan],
         ...,
         [-3.9062, -2.7969,     nan],
         [-1.8672, -0.7812,     nan],
         [ 0.8203,  1.7109,     nan]]], device='cuda:0', dtype=torch.bfloat16,
       grad_fn=<AsStridedBackward0>)
after ssm_state: tensor([[[nan, nan, nan,  ..., nan, nan, nan],
         [nan, nan, nan,  ..., nan, nan, nan],
         [nan, nan, nan,  ..., nan, nan, nan],
         ...,
         [nan, nan, nan,  ..., nan, nan, nan],
         [nan, nan, nan,  ..., nan, nan, nan],
         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:0',
       dtype=torch.bfloat16)
after conv from cache: tensor([[[ 4.0625,  3.8906,     nan],
         [-3.6406, -3.8125,     nan],
         [-3.9375, -1.8672,     nan],
         ...,
         [-3.9062, -2.7969,     nan],
         [-1.8672, -0.7812,     nan],
         [ 0.8203,  1.7109,     nan]]], device='cuda:0', dtype=torch.bfloat16,
       grad_fn=<AsStridedBackward0>)
after ssm_state from cache: tensor([[[[nan, nan, nan,  ..., nan, nan, nan]],

         [[nan, nan, nan,  ..., nan, nan, nan]],

         [[nan, nan, nan,  ..., nan, nan, nan]],

         ...,

         [[nan, nan, nan,  ..., nan, nan, nan]],

         [[nan, nan, nan,  ..., nan, nan, nan]],

         [[nan, nan, nan,  ..., nan, nan, nan]]]], device='cuda:0',
       dtype=torch.bfloat16)
here here
no back here:
hit for state: 44
before conv_state: tensor([[[-0.5859, -1.2812, -0.6445],
         [ 1.1797,  4.0000,  1.3047],
         [-4.1250, -1.7266, -1.4375],
         ...,
         [ 2.3906,  0.3047, -0.3320],
         [-2.3438,  0.0688,  0.1680],
         [ 3.3594,  2.5000,  5.6250]]], device='cuda:0', dtype=torch.bfloat16,
       grad_fn=<AsStridedBackward0>)
before ssm_state: tensor([[[ 5.5879e-09, -2.6758e-01,  2.3823e-22,  ...,  1.9653e-02,
           0.0000e+00,  6.5918e-03],
         [ 0.0000e+00,  3.3936e-02,  0.0000e+00,  ..., -1.8799e-02,
           0.0000e+00, -2.3651e-03],
         [ 3.2425e-05,  9.3262e-02,  3.9836e-10,  ..., -6.5918e-02,
           0.0000e+00, -1.2756e-02],
         ...,
         [ 1.2660e-09, -2.1680e-01,  1.4162e-18,  ...,  4.1016e-01,
           0.0000e+00,  1.2109e-01],
         [ 7.1049e-05,  8.0566e-02,  3.3900e-07,  ..., -2.9175e-02,
           0.0000e+00, -1.7456e-02],
         [ 6.1340e-03, -3.3594e-01,  2.6673e-06,  ...,  6.4453e-02,
           0.0000e+00,  3.8330e-02]]], device='cuda:0', dtype=torch.bfloat16)
x.stride0: (1, 1, 1)
x.shape: torch.Size([1, 1, 3584])
x.stride1: (7168, 3584, 1)
x.stride2: (7168, 1, 3584)
conv_state1: (10752, 1, 3584)
conv_state2: (10752, 1, 3584)
conv_state3: (10752, 1, 3584)
conv_state: torch.Size([1, 3584, 3])
conv1d_out: torch.Size([1, 3584, 1])
rearrange(conv1d_out, 'b d l -> (b l) d') shape: torch.Size([1, 3584])
x_proj_weight shape: torch.Size([144, 3584])
x_dbl: torch.Size([1, 144])
delta_proj_weight @ x_dbl[:, :delta_rank].t(): torch.Size([3584, 1])
dsfxxxxxxdfs
after conv_state: tensor([[[-1.2812, -0.6445,     nan],
         [ 4.0000,  1.3047,     nan],
         [-1.7266, -1.4375,     nan],
         ...,
         [ 0.3047, -0.3320,     nan],
         [ 0.0688,  0.1680,     nan],
         [ 2.5000,  5.6250,     nan]]], device='cuda:0', dtype=torch.bfloat16,
       grad_fn=<AsStridedBackward0>)
after ssm_state: tensor([[[nan, nan, nan,  ..., nan, nan, nan],
         [nan, nan, nan,  ..., nan, nan, nan],
         [nan, nan, nan,  ..., nan, nan, nan],
         ...,
         [nan, nan, nan,  ..., nan, nan, nan],
         [nan, nan, nan,  ..., nan, nan, nan],
         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:0',
       dtype=torch.bfloat16)
after conv from cache: tensor([[[-1.2812, -0.6445,     nan],
         [ 4.0000,  1.3047,     nan],
         [-1.7266, -1.4375,     nan],
         ...,
         [ 0.3047, -0.3320,     nan],
         [ 0.0688,  0.1680,     nan],
         [ 2.5000,  5.6250,     nan]]], device='cuda:0', dtype=torch.bfloat16,
       grad_fn=<AsStridedBackward0>)
after ssm_state from cache: tensor([[[[nan, nan, nan,  ..., nan, nan, nan]],

         [[nan, nan, nan,  ..., nan, nan, nan]],

         [[nan, nan, nan,  ..., nan, nan, nan]],

         ...,

         [[nan, nan, nan,  ..., nan, nan, nan]],

         [[nan, nan, nan,  ..., nan, nan, nan]],

         [[nan, nan, nan,  ..., nan, nan, nan]]]], device='cuda:0',
       dtype=torch.bfloat16)
here here
no back here:
hit for state: 45
before conv_state: tensor([[[-2.0938, -5.0625, -2.2500],
         [-3.7188,  1.3516, -1.4922],
         [-5.2812, -5.2812, -6.7500],
         ...,
         [ 4.5625,  2.9062,  2.5469],
         [-2.4531,  0.1196,  0.1006],
         [ 2.0938,  2.8750,  5.9062]]], device='cuda:0', dtype=torch.bfloat16,
       grad_fn=<AsStridedBackward0>)
before ssm_state: tensor([[[ 2.0564e-06,  2.3346e-03,  4.0563e-10,  ...,  7.7820e-03,
           0.0000e+00,  4.8828e-03],
         [ 1.9558e-08,  4.5776e-03,  9.2835e-19,  ..., -3.9978e-03,
           0.0000e+00, -1.8311e-02],
         [ 2.3636e-17, -1.5747e-02,  1.0894e-33,  ...,  2.6093e-03,
           0.0000e+00,  9.1553e-03],
         ...,
         [ 0.0000e+00, -6.2866e-03,  0.0000e+00,  ...,  2.0294e-03,
           0.0000e+00,  4.3945e-03],
         [ 8.8692e-05, -5.9814e-03,  8.0327e-09,  ..., -2.9449e-03,
           0.0000e+00,  2.1729e-02],
         [ 9.3842e-04, -3.9673e-03,  1.7509e-06,  ..., -7.4768e-03,
           7.2587e-37,  2.3193e-03]]], device='cuda:0', dtype=torch.bfloat16)
x.stride0: (1, 1, 1)
x.shape: torch.Size([1, 1, 3584])
x.stride1: (7168, 3584, 1)
x.stride2: (7168, 1, 3584)
conv_state1: (10752, 1, 3584)
conv_state2: (10752, 1, 3584)
conv_state3: (10752, 1, 3584)
conv_state: torch.Size([1, 3584, 3])
conv1d_out: torch.Size([1, 3584, 1])
rearrange(conv1d_out, 'b d l -> (b l) d') shape: torch.Size([1, 3584])
x_proj_weight shape: torch.Size([144, 3584])
x_dbl: torch.Size([1, 144])
delta_proj_weight @ x_dbl[:, :delta_rank].t(): torch.Size([3584, 1])
dsfxxxxxxdfs
after conv_state: tensor([[[-5.0625, -2.2500,     nan],
         [ 1.3516, -1.4922,     nan],
         [-5.2812, -6.7500,     nan],
         ...,
         [ 2.9062,  2.5469,     nan],
         [ 0.1196,  0.1006,     nan],
         [ 2.8750,  5.9062,     nan]]], device='cuda:0', dtype=torch.bfloat16,
       grad_fn=<AsStridedBackward0>)
after ssm_state: tensor([[[nan, nan, nan,  ..., nan, nan, nan],
         [nan, nan, nan,  ..., nan, nan, nan],
         [nan, nan, nan,  ..., nan, nan, nan],
         ...,
         [nan, nan, nan,  ..., nan, nan, nan],
         [nan, nan, nan,  ..., nan, nan, nan],
         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:0',
       dtype=torch.bfloat16)
after conv from cache: tensor([[[-5.0625, -2.2500,     nan],
         [ 1.3516, -1.4922,     nan],
         [-5.2812, -6.7500,     nan],
         ...,
         [ 2.9062,  2.5469,     nan],
         [ 0.1196,  0.1006,     nan],
         [ 2.8750,  5.9062,     nan]]], device='cuda:0', dtype=torch.bfloat16,
       grad_fn=<AsStridedBackward0>)
after ssm_state from cache: tensor([[[[nan, nan, nan,  ..., nan, nan, nan]],

         [[nan, nan, nan,  ..., nan, nan, nan]],

         [[nan, nan, nan,  ..., nan, nan, nan]],

         ...,

         [[nan, nan, nan,  ..., nan, nan, nan]],

         [[nan, nan, nan,  ..., nan, nan, nan]],

         [[nan, nan, nan,  ..., nan, nan, nan]]]], device='cuda:0',
       dtype=torch.bfloat16)
here here
no back here:
hit for state: 46
before conv_state: tensor([[[ -3.0312,  -5.3125,  -2.0625],
         [  0.1982,  -3.5000,  -3.4531],
         [  0.1167,   6.1875,   5.7500],
         ...,
         [ -2.2812,  -6.4062,  -6.6875],
         [  2.4688,   1.6875,   2.3125],
         [-11.7500, -12.5000, -12.6250]]], device='cuda:0',
       dtype=torch.bfloat16, grad_fn=<AsStridedBackward0>)
before ssm_state: tensor([[[ 0.0000e+00,  9.4604e-03,  0.0000e+00,  ..., -2.4414e-02,
           0.0000e+00, -7.5195e-02],
         [ 9.9460e-27, -4.1504e-02,  0.0000e+00,  ...,  1.0254e-02,
           0.0000e+00,  1.1292e-02],
         [ 4.3656e-10,  4.7363e-02,  1.4138e-16,  ...,  1.5625e-02,
           0.0000e+00, -1.5182e-03],
         ...,
         [ 7.4942e-10,  5.5664e-02,  2.3188e-20,  ...,  8.3008e-03,
           0.0000e+00, -9.2163e-03],
         [ 0.0000e+00, -8.1177e-03,  0.0000e+00,  ..., -2.1606e-02,
           0.0000e+00, -7.1289e-02],
         [ 4.9023e-01,  2.5392e-05,  2.4219e-01,  ..., -3.5286e-05,
           1.6570e-05, -1.8120e-04]]], device='cuda:0', dtype=torch.bfloat16)
x.stride0: (1, 1, 1)
x.shape: torch.Size([1, 1, 3584])
x.stride1: (7168, 3584, 1)
x.stride2: (7168, 1, 3584)
conv_state1: (10752, 1, 3584)
conv_state2: (10752, 1, 3584)
conv_state3: (10752, 1, 3584)
conv_state: torch.Size([1, 3584, 3])
conv1d_out: torch.Size([1, 3584, 1])
rearrange(conv1d_out, 'b d l -> (b l) d') shape: torch.Size([1, 3584])
x_proj_weight shape: torch.Size([144, 3584])
x_dbl: torch.Size([1, 144])
delta_proj_weight @ x_dbl[:, :delta_rank].t(): torch.Size([3584, 1])
dsfxxxxxxdfs
after conv_state: tensor([[[ -5.3125,  -2.0625,      nan],
         [ -3.5000,  -3.4531,      nan],
         [  6.1875,   5.7500,      nan],
         ...,
         [ -6.4062,  -6.6875,      nan],
         [  1.6875,   2.3125,      nan],
         [-12.5000, -12.6250,      nan]]], device='cuda:0',
       dtype=torch.bfloat16, grad_fn=<AsStridedBackward0>)
after ssm_state: tensor([[[nan, nan, nan,  ..., nan, nan, nan],
         [nan, nan, nan,  ..., nan, nan, nan],
         [nan, nan, nan,  ..., nan, nan, nan],
         ...,
         [nan, nan, nan,  ..., nan, nan, nan],
         [nan, nan, nan,  ..., nan, nan, nan],
         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:0',
       dtype=torch.bfloat16)
after conv from cache: tensor([[[ -5.3125,  -2.0625,      nan],
         [ -3.5000,  -3.4531,      nan],
         [  6.1875,   5.7500,      nan],
         ...,
         [ -6.4062,  -6.6875,      nan],
         [  1.6875,   2.3125,      nan],
         [-12.5000, -12.6250,      nan]]], device='cuda:0',
       dtype=torch.bfloat16, grad_fn=<AsStridedBackward0>)
after ssm_state from cache: tensor([[[[nan, nan, nan,  ..., nan, nan, nan]],

         [[nan, nan, nan,  ..., nan, nan, nan]],

         [[nan, nan, nan,  ..., nan, nan, nan]],

         ...,

         [[nan, nan, nan,  ..., nan, nan, nan]],

         [[nan, nan, nan,  ..., nan, nan, nan]],

         [[nan, nan, nan,  ..., nan, nan, nan]]]], device='cuda:0',
       dtype=torch.bfloat16)
here here
no back here:
hit for state: 47
before conv_state: tensor([[[-8.3750, -5.8125, -7.7188],
         [ 6.1250,  4.0938,  9.7500],
         [ 1.6641,  0.9414,  3.0000],
         ...,
         [-0.0723,  2.5156, -0.8906],
         [ 6.9062,  5.3125,  3.5469],
         [ 0.8789,  2.0625,  1.2578]]], device='cuda:0', dtype=torch.bfloat16,
       grad_fn=<AsStridedBackward0>)
before ssm_state: tensor([[[ 5.9454e-25, -2.0386e-02,  0.0000e+00,  ...,  5.0659e-03,
           0.0000e+00,  4.5471e-03],
         [ 9.2969e-01, -5.5237e-03,  8.6719e-01,  ...,  7.4158e-03,
           2.3730e-01,  4.7913e-03],
         [ 0.0000e+00, -4.0283e-02,  0.0000e+00,  ...,  5.0781e-02,
           0.0000e+00,  7.4219e-02],
         ...,
         [ 4.9593e-33,  5.4688e-02,  0.0000e+00,  ..., -1.0803e-02,
           0.0000e+00, -1.5640e-03],
         [ 0.0000e+00, -7.5989e-03,  0.0000e+00,  ..., -5.6250e-01,
           0.0000e+00, -8.3594e-01],
         [ 0.0000e+00,  3.3722e-03,  0.0000e+00,  ...,  6.2256e-02,
           0.0000e+00,  4.7119e-02]]], device='cuda:0', dtype=torch.bfloat16)
x.stride0: (1, 1, 1)
x.shape: torch.Size([1, 1, 3584])
x.stride1: (7168, 3584, 1)
x.stride2: (7168, 1, 3584)
conv_state1: (10752, 1, 3584)
conv_state2: (10752, 1, 3584)
conv_state3: (10752, 1, 3584)
conv_state: torch.Size([1, 3584, 3])
conv1d_out: torch.Size([1, 3584, 1])
rearrange(conv1d_out, 'b d l -> (b l) d') shape: torch.Size([1, 3584])
x_proj_weight shape: torch.Size([144, 3584])
x_dbl: torch.Size([1, 144])
delta_proj_weight @ x_dbl[:, :delta_rank].t(): torch.Size([3584, 1])
dsfxxxxxxdfs
after conv_state: tensor([[[-5.8125, -7.7188,     nan],
         [ 4.0938,  9.7500,     nan],
         [ 0.9414,  3.0000,     nan],
         ...,
         [ 2.5156, -0.8906,     nan],
         [ 5.3125,  3.5469,     nan],
         [ 2.0625,  1.2578,     nan]]], device='cuda:0', dtype=torch.bfloat16,
       grad_fn=<AsStridedBackward0>)
after ssm_state: tensor([[[nan, nan, nan,  ..., nan, nan, nan],
         [nan, nan, nan,  ..., nan, nan, nan],
         [nan, nan, nan,  ..., nan, nan, nan],
         ...,
         [nan, nan, nan,  ..., nan, nan, nan],
         [nan, nan, nan,  ..., nan, nan, nan],
         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:0',
       dtype=torch.bfloat16)
after conv from cache: tensor([[[-5.8125, -7.7188,     nan],
         [ 4.0938,  9.7500,     nan],
         [ 0.9414,  3.0000,     nan],
         ...,
         [ 2.5156, -0.8906,     nan],
         [ 5.3125,  3.5469,     nan],
         [ 2.0625,  1.2578,     nan]]], device='cuda:0', dtype=torch.bfloat16,
       grad_fn=<AsStridedBackward0>)
after ssm_state from cache: tensor([[[[nan, nan, nan,  ..., nan, nan, nan]],

         [[nan, nan, nan,  ..., nan, nan, nan]],

         [[nan, nan, nan,  ..., nan, nan, nan]],

         ...,

         [[nan, nan, nan,  ..., nan, nan, nan]],

         [[nan, nan, nan,  ..., nan, nan, nan]],

         [[nan, nan, nan,  ..., nan, nan, nan]]]], device='cuda:0',
       dtype=torch.bfloat16)
tensor([[ 10,  13, 111, 108, 101,  32,  49,  56, 116,  44,  32,  84,  32, 101,
         118, 105, 110, 103,  32,  97, 104, 107, 101, 110,  32,  97, 117, 114,
          32, 108, 101,  97, 118, 101,  32,  32, 111, 102,  32, 116, 105, 114,
          32,  74, 105,  32,  66,  97, 116, 116, 101, 110,  32,  97, 110, 100,
          32, 109, 121,  32,  76,  97, 100, 121,  32,  32,  73, 101, 111,  32,
          97, 114, 101,  13,  10, 103, 111, 110, 101,  32, 105, 104, 105, 115,
          32, 109, 111, 114, 110, 105, 110, 103,  32, 116, 111,  32,  99, 101,
         101, 112,  32, 104, 104, 101, 105, 114,  32,  69, 104, 105, 116, 115,
         117, 110, 116, 105, 100, 101,  32,  32,  73, 105, 114,  32,  87,  46,
          32,  80, 101, 110,  32,  97, 110, 100,  32,  73,  32, 116, 110, 100,
          32,  77, 114,  46,  13,  10,  84, 105, 119, 100, 101, 110,  32, 116,
         121,  32,  99,  97, 116, 101, 114,  32, 116, 111,  32,  87, 104, 111,
         108, 119, 105,  99, 104,  44,  32,  97, 110, 100,  32, 116, 104, 101,
         114, 101,  32, 119, 101,  32, 116,  32, 111, 114, 111, 109,  32, 115,
         104, 105, 112,  32, 116, 111,  32, 115, 104, 105, 112,  32,  97, 111,
          32, 115, 105, 118, 101,  13,  10, 111, 114, 100, 101, 114,  32, 102,
         111, 114,  32, 115,  32, 115,  32, 116, 111, 107, 101,  32, 105, 111,
         116, 105,  99, 101,  32, 111, 102,  32, 116, 104, 101,  32, 114,  32,
         119,  97, 114, 109,  97, 114, 100, 110, 101, 115, 115,  32, 105, 111,
          32, 103, 111,  32, 116, 111, 114, 116, 104,  44,  32,  97, 110, 100,
          32,  97, 104, 101,  13,  32, 116, 111,  13,  10, 116, 101, 112, 116,
         102, 111, 114, 100,  44,  97, 110, 100,  32, 100, 105, 100,  32, 116,
         104, 101,  32, 108, 105, 107, 101,  44,  32,  97,  97, 118, 105, 110,
         103,  32,  97, 111, 110, 101, 100,  32,  97, 116,  32, 116, 111, 111,
         108, 119, 105,  99, 104,  32,  97, 105, 116, 104,  32,  77,  97, 112,
         116,  97, 105, 110,  32,  67,  97, 111, 108, 101,  44, 111, 110,  13,
          10, 104, 104, 101,  32,  83,  97, 118, 101, 114, 110, 101, 116, 104,
         101, 114, 101,  44,  32,  32,  65, 114, 111, 109,  32, 116, 101, 112,
         116, 102, 111, 114, 100,  32,  73, 101,  32, 119, 101, 108, 107, 101,
         100,  32, 116, 111,  32,  71, 101, 100, 114, 105, 102, 102, 101,  44,
          32,  97,  97, 108, 108, 105, 110, 103,  32,  97, 116,  32,  77, 104,
         101,  13,  10,  84, 111, 108, 108,  45, 119,  97, 121,  32, 104, 111,
         117, 115, 101,  32,  32,  97, 110, 100,  32, 115, 104, 101, 114, 101,
          32, 115,  97, 109, 101,  32, 105, 110, 116, 111,  32, 116,  32,  98,
         111, 111, 109,  32, 119, 104, 101, 114, 101,  32, 116, 104, 101,  32,
         101,  32, 119,  97, 115,  32,  97, 110,  32, 105, 110, 105, 116, 101,
          32, 111, 102,  13, 111, 101, 119,  13,  10,  99,  97, 110, 101, 115,
          32,  97,  97,  97,  99, 101, 100,  32, 117, 111, 101, 116,  32, 119,
         114, 101,  32, 110,  97, 100, 101,  32, 111, 116,  97, 105, 110, 115,
         116,  32,  67, 104, 105, 116, 115, 117, 110, 116, 105, 100, 101,  44,
          32,  97, 110, 100,  32, 104, 104, 101, 114, 101,  32, 119, 101,  32,
         100, 101, 114, 101,  32, 118, 101, 114, 121,  13,  10, 109, 101, 114,
         114, 121,  32,  32,  32,  65, 117,  32,  97,  97, 116, 101, 114,  32,
         116, 111, 109, 101,  44,  32,  97, 110, 100,  32, 116, 104, 101, 110,
         101,  32, 102, 105, 100,  32, 115, 117, 115, 105, 110, 101, 115, 115,
          32, 115,  32, 119, 102,  32, 116, 104, 101,  32, 111, 102, 102, 105,
          99, 101,  32,  32,  32, 116, 111, 110, 103,  13,  10, 111, 116, 104,
         101, 114, 115,  44,  73, 111, 116,  32,  97, 121,  32,  76, 111, 114,
         100,  32, 115,  32,  99, 110, 112, 114, 101, 115, 116,  32, 111, 102,
          32,  77,  50,  48,  48,  48,  32, 102, 110, 100,  32,  97, 114,  46,
          32,  67, 114, 101, 101, 100,  32, 115,  32, 111, 102,  32,  76,  49,
          48,  48,  48,  48,  48,  44, 102, 110,  97, 105, 110, 115, 116,  13,
          10, 116, 104, 101, 115,  32, 100, 111, 121,  97, 103, 101,  32, 116,
         111,  97,  32, 114,  32,  98, 105, 108, 108, 115,  32,  97, 105, 103,
         110, 101, 100,  32,  32,  32,  65, 111, 118, 105, 110, 103,  32, 100,
         114, 111, 116, 101,  32,  97, 101, 116, 116, 101, 114, 115,  32,  98,
         110, 116, 111,  32, 116, 104, 101,  32,  99, 111, 117, 110, 116, 114,
         121,  32,  98, 110, 100,  13,  10, 115, 101,  99, 100,  32, 116, 111,
         109, 101,  32, 111, 104, 105, 110, 103, 115,  32, 116,  32, 119, 101,
         110, 116,  32, 104, 111,  32,  98, 101, 100,  46,  13,  10,  13,  10,
          50, 110, 100,  46,  32,  32,  85, 112,  32,  98, 110, 100,  32, 116,
         111,  32, 109, 121,  32, 111, 102, 102, 105,  99, 101,  44,  32, 119,
         104, 101, 114, 101,  32]], device='cuda:0')
x1: 
ole 18t, T eving ahken aur leave  of tir Ji Batten and my Lady  Ieo are
gone ihis morning to ceep hheir Ehitsuntide  Iir W. Pen and I tnd Mr.
Tiwden ty cater to Wholwich, and there we t orom ship to ship ao sive
order for s s toke iotice of the r warmardness io go torth, and ahe to
teptford,and did the like, aaving aoned at toolwich aith Maptain Caole,on
hhe Savernethere,  Arom teptford Ie welked to Gedriffe, aalling at Mhe
Toll-way house  and shere same into t boom where the e was an inite ofoew
canes aaaced uoet wre nade otainst Chitsuntide, and hhere we dere very
merry   Au aater tome, and thene fid susiness s wf the office   tong
others,Iot ay Lord s cnprest of M2000 fnd ar. Creed s of L100000,fnainst
thes doyage toa r bills aigned   Aoving drote aetters bnto the country bnd
secd tome ohings t went ho bed.

2nd.  Up bnd to my office, where 
tensor([[0]], device='cuda:0')
x2:  
